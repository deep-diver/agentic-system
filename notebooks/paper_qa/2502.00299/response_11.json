{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\nJia, F., Zhang, Y., and Ginsburg, B. Ruler: What\u2019s the\nreal context size of your long-context language models?\nArXiv preprint, abs/2404.06654, 2024. URL https:\n//arxiv.org/abs/2404.06654.\n\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adapta-\ntion of large language models. In The Tenth Interna-\ntional Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\n2022. URL https://openreview.net/forum?\nid=nZeVKeeFYf9.\n\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang,\nL. Efficient attentions for long document summariza-\ntion. In Toutanova, K., Rumshisky, A., Zettlemoyer,\nL., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\nR., Chakraborty, T., and Zhou, Y. (eds.), Proceedings\nof the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pp. 1419\u20131436, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.naacl-main.112. URL https://\naclanthology.org/2021.naacl-main.112.\n\nJacobs, S. A. et al. DeepSpeed Ulysses: System optimiza-\ntions for enabling training of extreme long sequence\nTransformer models. ArXiv preprint, abs/2309.14509,\n2023. URL https://arxiv.org/abs/2309.\n14509.\n\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\nT., and Sayed, W. E. Mistral 7b, 2023a. URL https:\n//arxiv.org/abs/2310.06825.\n\nJiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.\nLLMLingua: Compressing prompts for accelerated in-\nference of large language models. In Bouamor, H.,\nPino, J., and Bali, K. (eds.), Proceedings of the 2023\nConference on Empirical Methods in Natural Language\nProcessing, pp. 13358\u201313376, Singapore, December\n2023b. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.825. URL https://\naclanthology.org/2023.emnlp-main.825.\n\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\nQiu, L. LongLLMLingua: Accelerating and enhancing\nLLMs in long context scenarios via prompt compression.\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\nceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\n\npp. 1658\u20131677, Bangkok, Thailand, August 2024. As-\nsociation for Computational Linguistics. URL https:\n//aclanthology.org/2024.acl-long.91.\n\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\naQA: A large scale distantly supervised challenge dataset\nfor reading comprehension. In Barzilay, R. and Kan,\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1601\u20131611, Vancouver,\nCanada, 2017. Association for Computational Linguis-\ntics. doi: 10.18653/v1/P17-1147. URL https://\naclanthology.org/P17-1147.\n\nKamradt, G. Needle In A Haystack - pres-\nsure testing LLMs. Github, 2023. URL\nhttps://github.com/gkamradt/LLMTest_\nNeedleInAHaystack/tree/main.\n\nKleijn and der Vaart, V. The bernstein-von-mises the-\norem under misspecification. Electronic Journal of\nStatistics, 6:354\u2013381, 2012. URL https://api.\nsemanticscholar.org/CorpusID:85548207.\n\nKo\u02c7cisk\u00fd, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\nreading comprehension challenge. Transactions of the\nAssociation for Computational Linguistics, 6:317\u2013328,\n2018. doi: 10.1162/tacl_a_00023. URL https://\naclanthology.org/Q18-1023.\n\nLi, D., Shao, R., et al. How long can open-source LLMs\ntruly promise on context length?, 2023. URL https:\n//lmsys.org/blog/2023-06-29-longchat.\n\nLi, X. and Roth, D. Learning question classifiers. In\nCOLING 2002: The 19th International Conference on\nComputational Linguistics, 2002. URL https://\naclanthology.org/C02-1150.\n\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\nLlm knows what you are looking for before genera-\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\nhttps://arxiv.org/abs/2404.14469.\n\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\nMinicache: Kv cache compression in depth dimension for\nlarge language models. arXiv preprint arXiv:2405.14366,\n2024a.\n\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\non million-length video and language with ringattention.\nArXiv preprint, abs/2402.08268, 2024b. URL https:\n//arxiv.org/abs/2402.08268.\n\n11",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1721,
          "y": 0.0577
        },
        {
          "x": 0.8039,
          "y": 0.0577
        },
        {
          "x": 0.8039,
          "y": 0.072
        },
        {
          "x": 0.1721,
          "y": 0.072
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\nJia, F., Zhang, Y., and Ginsburg, B. Ruler: What\u2019s the\nreal context size of your long-context language models?\nArXiv preprint, abs/2404.06654, 2024. URL https:\n//arxiv.org/abs/2404.06654.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0874,
          "y": 0.086
        },
        {
          "x": 0.4786,
          "y": 0.086
        },
        {
          "x": 0.4786,
          "y": 0.1614
        },
        {
          "x": 0.0874,
          "y": 0.1614
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\nS., Wang, L., and Chen, W. Lora: Low-rank adapta-\ntion of large language models. In The Tenth Interna-\ntional Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\n2022. URL https://openreview.net/forum?\nid=nZeVKeeFYf9.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0866,
          "y": 0.1749
        },
        {
          "x": 0.4777,
          "y": 0.1749
        },
        {
          "x": 0.4777,
          "y": 0.2802
        },
        {
          "x": 0.0866,
          "y": 0.2802
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Huang, L., Cao, S., Parulian, N., Ji, H., and Wang,\nL. Efficient attentions for long document summariza-\ntion. In Toutanova, K., Rumshisky, A., Zettlemoyer,\nL., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\nR., Chakraborty, T., and Zhou, Y. (eds.), Proceedings\nof the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pp. 1419\u20131436, Online,\n2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.naacl-main.112. URL https://\naclanthology.org/2021.naacl-main.112.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0886,
          "y": 0.2936
        },
        {
          "x": 0.4791,
          "y": 0.2936
        },
        {
          "x": 0.4791,
          "y": 0.4609
        },
        {
          "x": 0.0886,
          "y": 0.4609
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Jacobs, S. A. et al. DeepSpeed Ulysses: System optimiza-\ntions for enabling training of extreme long sequence\nTransformer models. ArXiv preprint, abs/2309.14509,\n2023. URL https://arxiv.org/abs/2309.\n14509.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0862,
          "y": 0.4735
        },
        {
          "x": 0.4791,
          "y": 0.4735
        },
        {
          "x": 0.4791,
          "y": 0.5485
        },
        {
          "x": 0.0862,
          "y": 0.5485
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\nT., and Sayed, W. E. Mistral 7b, 2023a. URL https:\n//arxiv.org/abs/2310.06825.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0858,
          "y": 0.562
        },
        {
          "x": 0.4796,
          "y": 0.562
        },
        {
          "x": 0.4796,
          "y": 0.6548
        },
        {
          "x": 0.0858,
          "y": 0.6548
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.\nLLMLingua: Compressing prompts for accelerated in-\nference of large language models. In Bouamor, H.,\nPino, J., and Bali, K. (eds.), Proceedings of the 2023\nConference on Empirical Methods in Natural Language\nProcessing, pp. 13358\u201313376, Singapore, December\n2023b. Association for Computational Linguistics. doi:\n10.18653/v1/2023.emnlp-main.825. URL https://\naclanthology.org/2023.emnlp-main.825.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0848,
          "y": 0.6665
        },
        {
          "x": 0.4793,
          "y": 0.6665
        },
        {
          "x": 0.4793,
          "y": 0.8028
        },
        {
          "x": 0.0848,
          "y": 0.8028
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Jiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\nQiu, L. LongLLMLingua: Accelerating and enhancing\nLLMs in long context scenarios via prompt compression.\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\nceedings of the 62nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0861,
          "y": 0.8157
        },
        {
          "x": 0.4773,
          "y": 0.8157
        },
        {
          "x": 0.4773,
          "y": 0.9085
        },
        {
          "x": 0.0861,
          "y": 0.9085
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "pp. 1658\u20131677, Bangkok, Thailand, August 2024. As-\nsociation for Computational Linguistics. URL https:\n//aclanthology.org/2024.acl-long.91.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5155,
          "y": 0.0859
        },
        {
          "x": 0.8911,
          "y": 0.0859
        },
        {
          "x": 0.8911,
          "y": 0.1307
        },
        {
          "x": 0.5155,
          "y": 0.1307
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\naQA: A large scale distantly supervised challenge dataset\nfor reading comprehension. In Barzilay, R. and Kan,\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pp. 1601\u20131611, Vancouver,\nCanada, 2017. Association for Computational Linguis-\ntics. doi: 10.18653/v1/P17-1147. URL https://\naclanthology.org/P17-1147.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5009,
          "y": 0.1455
        },
        {
          "x": 0.8908,
          "y": 0.1455
        },
        {
          "x": 0.8908,
          "y": 0.2808
        },
        {
          "x": 0.5009,
          "y": 0.2808
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Kamradt, G. Needle In A Haystack - pres-\nsure testing LLMs. Github, 2023. URL\nhttps://github.com/gkamradt/LLMTest_\nNeedleInAHaystack/tree/main.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4979,
          "y": 0.2955
        },
        {
          "x": 0.8909,
          "y": 0.2955
        },
        {
          "x": 0.8909,
          "y": 0.3558
        },
        {
          "x": 0.4979,
          "y": 0.3558
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Kleijn and der Vaart, V. The bernstein-von-mises the-\norem under misspecification. Electronic Journal of\nStatistics, 6:354\u2013381, 2012. URL https://api.\nsemanticscholar.org/CorpusID:85548207.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4978,
          "y": 0.3692
        },
        {
          "x": 0.8931,
          "y": 0.3692
        },
        {
          "x": 0.8931,
          "y": 0.431
        },
        {
          "x": 0.4978,
          "y": 0.431
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Ko\u02c7cisk\u00fd, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\nreading comprehension challenge. Transactions of the\nAssociation for Computational Linguistics, 6:317\u2013328,\n2018. doi: 10.1162/tacl_a_00023. URL https://\naclanthology.org/Q18-1023.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5009,
          "y": 0.4435
        },
        {
          "x": 0.8904,
          "y": 0.4435
        },
        {
          "x": 0.8904,
          "y": 0.5352
        },
        {
          "x": 0.5009,
          "y": 0.5352
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Li, D., Shao, R., et al. How long can open-source LLMs\ntruly promise on context length?, 2023. URL https:\n//lmsys.org/blog/2023-06-29-longchat.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4987,
          "y": 0.5488
        },
        {
          "x": 0.8908,
          "y": 0.5488
        },
        {
          "x": 0.8908,
          "y": 0.5949
        },
        {
          "x": 0.4987,
          "y": 0.5949
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Li, X. and Roth, D. Learning question classifiers. In\nCOLING 2002: The 19th International Conference on\nComputational Linguistics, 2002. URL https://\naclanthology.org/C02-1150.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.497,
          "y": 0.6084
        },
        {
          "x": 0.8891,
          "y": 0.6084
        },
        {
          "x": 0.8891,
          "y": 0.6688
        },
        {
          "x": 0.497,
          "y": 0.6688
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\nLlm knows what you are looking for before genera-\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\nhttps://arxiv.org/abs/2404.14469.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4972,
          "y": 0.6824
        },
        {
          "x": 0.8913,
          "y": 0.6824
        },
        {
          "x": 0.8913,
          "y": 0.7578
        },
        {
          "x": 0.4972,
          "y": 0.7578
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Liu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\nMinicache: Kv cache compression in depth dimension for\nlarge language models. arXiv preprint arXiv:2405.14366,\n2024a.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4987,
          "y": 0.7716
        },
        {
          "x": 0.8904,
          "y": 0.7716
        },
        {
          "x": 0.8904,
          "y": 0.8319
        },
        {
          "x": 0.4987,
          "y": 0.8319
        }
      ],
      "id": 16,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Liu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\non million-length video and language with ringattention.\nArXiv preprint, abs/2402.08268, 2024b. URL https:\n//arxiv.org/abs/2402.08268.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4984,
          "y": 0.8466
        },
        {
          "x": 0.8923,
          "y": 0.8466
        },
        {
          "x": 0.8923,
          "y": 0.9077
        },
        {
          "x": 0.4984,
          "y": 0.9077
        }
      ],
      "id": 17,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "11",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4767,
          "y": 0.9237
        },
        {
          "x": 0.4961,
          "y": 0.9237
        },
        {
          "x": 0.4961,
          "y": 0.9376
        },
        {
          "x": 0.4767,
          "y": 0.9376
        }
      ],
      "id": 18,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}