{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import PyPDF2\n",
    "from mistralai import Mistral\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments\n",
    "\n",
    "def execute_tool_call(tool_call_str, tools, agent_name):\n",
    "    name = tool_call_str[0][\"name\"]\n",
    "    args = tool_call_str[0][\"arguments\"]\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"mistral-large-latest\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent(paper_title: str):\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
    ")\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in supervisor_agent.tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ToolCall(function=FunctionCall(name='to_paper_search_agent', arguments='{\"paper_title\": \"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}'), id='JAUrxeRpg', type=None, index=0)]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    \n",
    "\n",
    "# Initial trial\n",
    "response = client.chat.complete(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas,\n",
    "    tool_choice=\"any\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"name\": \"to_download_and_parse_paper_agent\", \"arguments\": {\"paper_url\": \"https://arxiv.org/abs/2502.00299\"}}]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Second)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"name\": response.choices[0].message.tool_calls[0].function.name,\n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n",
    "        \"content\": \"Paper URL: https://arxiv.org/abs/2502.00299\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_fn_call = json.loads(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'to_download_and_parse_paper_agent',\n",
       "  'arguments': {'paper_url': 'https://arxiv.org/abs/2502.00299'}}]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_fn_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_download_and_parse_paper_agent\n",
      "{'paper_url': 'https://arxiv.org/abs/2502.00299'}\n"
     ]
    }
   ],
   "source": [
    "# def execute_tool_call(tool_call, tools, agent_name):\n",
    "name = alt_fn_call[0][\"name\"]\n",
    "print(name)\n",
    "args = alt_fn_call[0][\"arguments\"]\n",
    "print(args)\n",
    "\n",
    "tools[name](**args)\n",
    "\n",
    "# print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "# return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper titled 'ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' introduces a novel approach to reduce memory costs in long-context inference with Large Language Models (LLMs). The authors identify that previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in real-world language characteristics. To address this, they propose ChunkKV, which groups tokens in chunks as a basic compressing unit and retains the most informative semantic chunks while discarding the less important ones. Additionally, ChunkKV exhibits higher similarity in the preserved indices across different layers, leading to the proposal of layer-wise index reuse to further reduce computational overhead. The evaluation of ChunkKV on long-context benchmarks, including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark, demonstrates up to a 10% performance improvement under aggressive compression ratios compared to existing methods.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Third)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"name\": response.choices[0].message.tool_calls[0].function.name,\n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id, \n",
    "        \"content\": \"Retrieved Paper Content\\n\"\n",
    "        \"--------------------------------\\n\"\n",
    "        \"Title: ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\"\n",
    "        \"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.complete(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
