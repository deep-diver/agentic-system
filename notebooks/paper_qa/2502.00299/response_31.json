{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\nE\u2300 resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting\npio) and Piprompt\ncorrespond to the concept 0 and 0*.\n\nCondition 1 (distinguishability (Fang & Xie, 2022)). The 0* is distinguishable if for all 0 E \uc694, 0 \u2260 0*,\n\n$$\\sum_{i=1}^{k}K L_{i}(\\theta^{\\star}||\\theta)>\\epsilon_{\\theta},$$\n\n(3)\n\n# [KL(piprompt |lp\ufffd)].\nwhere the KLi(0*||0) := EO[1:i-1]~pprompt\n\nNoises from KV Cache Compression. Naturally, because of the sparsified KV cache, some history tokens in 01:t-1 at\ndifferent layers lost its attention score calculation with respect to the next word prediction Ot. We can regard this as the noise\nadded onto the 01:t-1. Thus, distincting 0* from 0 requires larger KL divergence. Following (Zhou et al., 2024), we provide\nthe following second condition about the distinguishability with the KV cache sparsity.\n\nCondition 2 (distinguishability under sparsified KV cache). With the noise introduced by the sparsified KV cache of the\nsparse ratio T, the distribution mismatch between the prompt and the pretraining distribution that is approximated by LLM\nis enlarged, resulting in a varied requirement with error term 50(r) for 0* being distinguishable iffor all 0 E \u65e5, 0 \u2260 0*,\n\n$$\\sum_{i=1}^{k}K L_{i}(\\theta^{*}||\\theta)>\\epsilon_{\\theta}+\\xi_{\\theta}(r),\\quad\\mathrm{where}\\quad\\xi_{\\theta}(r)\\propto r.$$\n\n(4)\n\nLemma 1 (noisy-relaxed bound in (Fang & Xie, 2022; Zhou et al., 2024)). let B denotes the set of 0 which does not\nsatisfy Condition 1. We assume that KL(Pprompt (yoear|xrest)]|P(yhear|xnest, 0) is boundedfor all 0 and that 0* minimizes the\nmulti-class logistic risk as,\n\n$${\\cal L}_{C E}(\\theta)=-\\mathrm{i}\\mathrm{E}_{x_{t e t}\\sim p_{p r o m p t}}[p_{p r o m p t}(y_{t e s t}|x_{t e s t})\\cdot\\log p(y_{t e s t}|x_{t e s t},\\theta)].$$\n\n(5)\n\n# If\n\n$$\\mathbb{E}_{x_{t r e r}\\sim p_{u r o m p t}}[K L(p_{p r o m p t}(y_{t e s t}|x_{t e t})]]p(y_{t e s t}|x_{t e s t},\\theta))]\\leq(\\epsilon_{\\theta}+\\xi_{\\theta}(r)),\\quad\\forall\\quad\\theta\\in B,$$\n\n(6)\n\n# then\n\n$$\\operatorname*{lim}_{n\\rightarrow\\infty}L_{0-1}(f_{n})\\le\\mathrm{inf}\\,L_{0-1}(f)+g^{-1}\\left(\\mathrm{sup}(\\epsilon\\theta)\\right),$$\n\n(7)\n\nwhere g(v) = \ube14((1-v)log(1-v)+(1+v) log(1 +v)) is the calibrationfunction (Steinwart, 2007; Pires & Szepesv\ufffdri,\n2016)for the multiclass logistic loss for VE [0,1].\n\nFollowing (Kleijn & der Vaart, 2012; Fang & Xie, 2022), KL divergence is assumed to haver the 2nd-order Taylor expansion\nwith the concept 0. Then, we have the following theorem and proof.\n\nTheorem 1. (Fang & Xie, 2022; Zhou et al., 2024) Let the set of0 which does not satisfy Equation 3 in Condition 1 to be B.\nAssume that KL divergences have a 2nd-order Taylor expansion around 0*:\n\n$$\\forall j>1,\\;\\;K L_{i}(\\theta^{\\star}||\\theta)=\\frac{1}{2}(\\theta-\\theta^{\\star})^{\\top}I_{j,\\theta^{\\star}}(\\theta-\\theta^{\\star})+O(|\\theta-\\theta^{\\star}||^{3})$$\n\n(8)\n\nmaxj \ufffdmax(Ij.0* )\nwhere Ij,0* is the Fisher information matrix of the j-th token distribution with respect to 0*. Let Y\u2300* =\nmin j\ufffdmin (Ij,0* )\nwhere \u5165max, \u5165min return the largest and smallest eigenvalues. Thenfor k > 2 and as n \u2192 8, the 0-1 risk of the in-context\nlearning predictor fn is bounded as\n\n$$\\operatorname*{lim}_{n\\to\\infty}L_{o.l}(f_{n})\\leq\\operatorname*{inf}_{f}L_{o.l}(f)+g^{-1}\\left(O\\left({\\frac{\\gamma_{o^{*}}\\operatorname{sup}_{\\theta\\in{\\mathrm{B}}}(\\epsilon_{\\theta}+\\xi_{\\theta}(r)}{k-1}}\\right)\\right)$$\n\n(9)\n\n31",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1728,
          "y": 0.0573
        },
        {
          "x": 0.8033,
          "y": 0.0573
        },
        {
          "x": 0.8033,
          "y": 0.0723
        },
        {
          "x": 0.1728,
          "y": 0.0723
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "E\u2300 resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting\npio) and Piprompt\ncorrespond to the concept 0 and 0*.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0868,
          "y": 0.0853
        },
        {
          "x": 0.8896,
          "y": 0.0853
        },
        {
          "x": 0.8896,
          "y": 0.1174
        },
        {
          "x": 0.0868,
          "y": 0.1174
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Condition 1 (distinguishability (Fang & Xie, 2022)). The 0* is distinguishable if for all 0 E \uc694, 0 \u2260 0*,",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0867,
          "y": 0.122
        },
        {
          "x": 0.7693,
          "y": 0.122
        },
        {
          "x": 0.7693,
          "y": 0.1383
        },
        {
          "x": 0.0867,
          "y": 0.1383
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\sum_{i=1}^{k}K L_{i}(\\theta^{\\star}||\\theta)>\\epsilon_{\\theta},$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4149,
          "y": 0.1523
        },
        {
          "x": 0.561,
          "y": 0.1523
        },
        {
          "x": 0.561,
          "y": 0.1938
        },
        {
          "x": 0.4149,
          "y": 0.1938
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(3)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.864,
          "y": 0.1637
        },
        {
          "x": 0.8883,
          "y": 0.1637
        },
        {
          "x": 0.8883,
          "y": 0.1801
        },
        {
          "x": 0.864,
          "y": 0.1801
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# [KL(piprompt |lp\ufffd)].\nwhere the KLi(0*||0) := EO[1:i-1]~pprompt",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0879,
          "y": 0.2046
        },
        {
          "x": 0.4958,
          "y": 0.2046
        },
        {
          "x": 0.4958,
          "y": 0.2253
        },
        {
          "x": 0.0879,
          "y": 0.2253
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Noises from KV Cache Compression. Naturally, because of the sparsified KV cache, some history tokens in 01:t-1 at\ndifferent layers lost its attention score calculation with respect to the next word prediction Ot. We can regard this as the noise\nadded onto the 01:t-1. Thus, distincting 0* from 0 requires larger KL divergence. Following (Zhou et al., 2024), we provide\nthe following second condition about the distinguishability with the KV cache sparsity.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0864,
          "y": 0.2346
        },
        {
          "x": 0.891,
          "y": 0.2346
        },
        {
          "x": 0.891,
          "y": 0.2962
        },
        {
          "x": 0.0864,
          "y": 0.2962
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Condition 2 (distinguishability under sparsified KV cache). With the noise introduced by the sparsified KV cache of the\nsparse ratio T, the distribution mismatch between the prompt and the pretraining distribution that is approximated by LLM\nis enlarged, resulting in a varied requirement with error term 50(r) for 0* being distinguishable iffor all 0 E \u65e5, 0 \u2260 0*,",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.087,
          "y": 0.3009
        },
        {
          "x": 0.8909,
          "y": 0.3009
        },
        {
          "x": 0.8909,
          "y": 0.3494
        },
        {
          "x": 0.087,
          "y": 0.3494
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\sum_{i=1}^{k}K L_{i}(\\theta^{*}||\\theta)>\\epsilon_{\\theta}+\\xi_{\\theta}(r),\\quad\\mathrm{where}\\quad\\xi_{\\theta}(r)\\propto r.$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3088,
          "y": 0.3614
        },
        {
          "x": 0.6623,
          "y": 0.3614
        },
        {
          "x": 0.6623,
          "y": 0.4033
        },
        {
          "x": 0.3088,
          "y": 0.4033
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(4)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8643,
          "y": 0.3738
        },
        {
          "x": 0.8882,
          "y": 0.3738
        },
        {
          "x": 0.8882,
          "y": 0.3894
        },
        {
          "x": 0.8643,
          "y": 0.3894
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Lemma 1 (noisy-relaxed bound in (Fang & Xie, 2022; Zhou et al., 2024)). let B denotes the set of 0 which does not\nsatisfy Condition 1. We assume that KL(Pprompt (yoear|xrest)]|P(yhear|xnest, 0) is boundedfor all 0 and that 0* minimizes the\nmulti-class logistic risk as,",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0871,
          "y": 0.4246
        },
        {
          "x": 0.8891,
          "y": 0.4246
        },
        {
          "x": 0.8891,
          "y": 0.4732
        },
        {
          "x": 0.0871,
          "y": 0.4732
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$${\\cal L}_{C E}(\\theta)=-\\mathrm{i}\\mathrm{E}_{x_{t e t}\\sim p_{p r o m p t}}[p_{p r o m p t}(y_{t e s t}|x_{t e s t})\\cdot\\log p(y_{t e s t}|x_{t e s t},\\theta)].$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2683,
          "y": 0.4843
        },
        {
          "x": 0.7035,
          "y": 0.4843
        },
        {
          "x": 0.7035,
          "y": 0.506
        },
        {
          "x": 0.2683,
          "y": 0.506
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(5)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8641,
          "y": 0.4859
        },
        {
          "x": 0.8884,
          "y": 0.4859
        },
        {
          "x": 0.8884,
          "y": 0.502
        },
        {
          "x": 0.8641,
          "y": 0.502
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# If",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.087,
          "y": 0.5159
        },
        {
          "x": 0.1052,
          "y": 0.5159
        },
        {
          "x": 0.1052,
          "y": 0.5318
        },
        {
          "x": 0.087,
          "y": 0.5318
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\mathbb{E}_{x_{t r e r}\\sim p_{u r o m p t}}[K L(p_{p r o m p t}(y_{t e s t}|x_{t e t})]]p(y_{t e s t}|x_{t e s t},\\theta))]\\leq(\\epsilon_{\\theta}+\\xi_{\\theta}(r)),\\quad\\forall\\quad\\theta\\in B,$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2098,
          "y": 0.5441
        },
        {
          "x": 0.7624,
          "y": 0.5441
        },
        {
          "x": 0.7624,
          "y": 0.5639
        },
        {
          "x": 0.2098,
          "y": 0.5639
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(6)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8644,
          "y": 0.5455
        },
        {
          "x": 0.8883,
          "y": 0.5455
        },
        {
          "x": 0.8883,
          "y": 0.5611
        },
        {
          "x": 0.8644,
          "y": 0.5611
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# then",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0878,
          "y": 0.5767
        },
        {
          "x": 0.1207,
          "y": 0.5767
        },
        {
          "x": 0.1207,
          "y": 0.59
        },
        {
          "x": 0.0878,
          "y": 0.59
        }
      ],
      "id": 16,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\operatorname*{lim}_{n\\rightarrow\\infty}L_{0-1}(f_{n})\\le\\mathrm{inf}\\,L_{0-1}(f)+g^{-1}\\left(\\mathrm{sup}(\\epsilon\\theta)\\right),$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3186,
          "y": 0.6009
        },
        {
          "x": 0.6557,
          "y": 0.6009
        },
        {
          "x": 0.6557,
          "y": 0.6358
        },
        {
          "x": 0.3186,
          "y": 0.6358
        }
      ],
      "id": 17,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(7)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8642,
          "y": 0.6095
        },
        {
          "x": 0.8884,
          "y": 0.6095
        },
        {
          "x": 0.8884,
          "y": 0.6265
        },
        {
          "x": 0.8642,
          "y": 0.6265
        }
      ],
      "id": 18,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "where g(v) = \ube14((1-v)log(1-v)+(1+v) log(1 +v)) is the calibrationfunction (Steinwart, 2007; Pires & Szepesv\ufffdri,\n2016)for the multiclass logistic loss for VE [0,1].",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0864,
          "y": 0.6471
        },
        {
          "x": 0.8898,
          "y": 0.6471
        },
        {
          "x": 0.8898,
          "y": 0.6806
        },
        {
          "x": 0.0864,
          "y": 0.6806
        }
      ],
      "id": 19,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Following (Kleijn & der Vaart, 2012; Fang & Xie, 2022), KL divergence is assumed to haver the 2nd-order Taylor expansion\nwith the concept 0. Then, we have the following theorem and proof.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0872,
          "y": 0.692
        },
        {
          "x": 0.8875,
          "y": 0.692
        },
        {
          "x": 0.8875,
          "y": 0.7231
        },
        {
          "x": 0.0872,
          "y": 0.7231
        }
      ],
      "id": 20,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Theorem 1. (Fang & Xie, 2022; Zhou et al., 2024) Let the set of0 which does not satisfy Equation 3 in Condition 1 to be B.\nAssume that KL divergences have a 2nd-order Taylor expansion around 0*:",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0866,
          "y": 0.7285
        },
        {
          "x": 0.8902,
          "y": 0.7285
        },
        {
          "x": 0.8902,
          "y": 0.7597
        },
        {
          "x": 0.0866,
          "y": 0.7597
        }
      ],
      "id": 21,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\forall j>1,\\;\\;K L_{i}(\\theta^{\\star}||\\theta)=\\frac{1}{2}(\\theta-\\theta^{\\star})^{\\top}I_{j,\\theta^{\\star}}(\\theta-\\theta^{\\star})+O(|\\theta-\\theta^{\\star}||^{3})$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2656,
          "y": 0.7721
        },
        {
          "x": 0.711,
          "y": 0.7721
        },
        {
          "x": 0.711,
          "y": 0.8009
        },
        {
          "x": 0.2656,
          "y": 0.8009
        }
      ],
      "id": 22,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(8)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8648,
          "y": 0.779
        },
        {
          "x": 0.8883,
          "y": 0.779
        },
        {
          "x": 0.8883,
          "y": 0.795
        },
        {
          "x": 0.8648,
          "y": 0.795
        }
      ],
      "id": 23,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "maxj \ufffdmax(Ij.0* )\nwhere Ij,0* is the Fisher information matrix of the j-th token distribution with respect to 0*. Let Y\u2300* =\nmin j\ufffdmin (Ij,0* )\nwhere \u5165max, \u5165min return the largest and smallest eigenvalues. Thenfor k > 2 and as n \u2192 8, the 0-1 risk of the in-context\nlearning predictor fn is bounded as",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0863,
          "y": 0.8158
        },
        {
          "x": 0.8874,
          "y": 0.8158
        },
        {
          "x": 0.8874,
          "y": 0.865
        },
        {
          "x": 0.0863,
          "y": 0.865
        }
      ],
      "id": 24,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\operatorname*{lim}_{n\\to\\infty}L_{o.l}(f_{n})\\leq\\operatorname*{inf}_{f}L_{o.l}(f)+g^{-1}\\left(O\\left({\\frac{\\gamma_{o^{*}}\\operatorname{sup}_{\\theta\\in{\\mathrm{B}}}(\\epsilon_{\\theta}+\\xi_{\\theta}(r)}{k-1}}\\right)\\right)$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2535,
          "y": 0.8738
        },
        {
          "x": 0.7186,
          "y": 0.8738
        },
        {
          "x": 0.7186,
          "y": 0.9136
        },
        {
          "x": 0.2535,
          "y": 0.9136
        }
      ],
      "id": 25,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(9)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8643,
          "y": 0.8859
        },
        {
          "x": 0.8884,
          "y": 0.8859
        },
        {
          "x": 0.8884,
          "y": 0.9015
        },
        {
          "x": 0.8643,
          "y": 0.9015
        }
      ],
      "id": 26,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "31",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4758,
          "y": 0.9237
        },
        {
          "x": 0.4963,
          "y": 0.9237
        },
        {
          "x": 0.4963,
          "y": 0.9378
        },
        {
          "x": 0.4758,
          "y": 0.9378
        }
      ],
      "id": 27,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": true,
  "usage": {
    "pages": 1
  }
}