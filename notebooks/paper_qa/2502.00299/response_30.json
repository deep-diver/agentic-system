{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\nTable 17: Performance comparison of Chinese subtask on LongBench for Qwen2-7B-Instruct.\n\n| Method | Single-Document QA | Multi-Document QA | Summarization | Few-shot Learning | Synthetic | Avg. \u2191 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Method | MF-zh | DuReader | VCSum | LSHT | PR-zh | Avg. \u2191 |\n| Avg len | 6,701 | 15,768 | 15,380 | 22,337 | 6,745 | Avg. \u2191 |\n| Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full |\n| FullKV | 39.17 | 23.63 | 16.21 | 43.50 | 70.50 | 38.60 |\n| Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% |\n| StreamingLLM | 38.05 | 23.24 | 15.92 | 40.50 | 44.50 | 32.44 |\n| H2O | 37.99 | 19.58 | 16.16 | 41.67 | 67.35 | 36.55 |\n| SnapKV | 44.25 | 20.27 | 16.24 | 44.50 | 68.10 | 38.67 |\n| PyramidKV | 36.57 | 20.56 | 16.15 | 43.50 | 66.50 | 36.55 |\n| ChunkKV | 45.92 | 20.15 | 16.37 | 43.75 | 71.10 | 39.45 |\n\n\n# C. Theoretical Understanding\n\nIn this section, we provide the theoretical interpretation from the perspective from the In-context learning (ICL) to further\nunderstand how ChunkKV outperforms token-level KV cache compression.\n\nPretraining Data Distribution. Given a set of concepts \u2463 and a concept 0 E \u65e5, we define the pretraining data is sampled\nfrom p(01, \u00b7 .. , oT) = SAEO p(01,... , oT|0)p(0)d0 (Fang & Xie, 2022). Each token 0 is sampled from a vocabulary 0. For\nsimplicity, we write 01:t = 01 ... Ot.\n\nLanguage Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Fang & Xie, 2022) usually utilize the next\nword prediction as the language modelling, which predicts the next token Ot given the previous tokens 01 \u00b7 \u00b7 0t-1 for all\nt = 1, \u00b7 \u00b7 , T. Formally, a language modelling can be writen as the distribution f(0t|01:t-1). And it is pretrained on a huge\ncorpus sampled from the pretraining distribution p(01,... , 0t+1) (Fang & Xie, 2022). Considering the large scale of the\nmodel size and dataset size, it can be assumed that the f(01 \u00b7 \u00b7 \u00b7 ot+1) has been aligned with the p(01 \u00b7 \u00b7 \u00b7 Ot+1) (Fang & Xie,\n2022).\n\nPrompt Distribution. Following (Fang & Xie, 2022), a prompt is composed of an input token sequence x followed by an\noutput token y. Then, the i-th training example 1 that can appear in any place in the whole prompt 01:T is defined as Oi\nconsisting of an input Xi = Oi [1 : k - 1] (the first k - 1 tokens) followed by the output Yi = Oi [k] at the end, where the\nlength k is fixed for simplicity.\n\nThe i-th training example is independently generated as follows: 1) Generate a start hidden state hstart from a prompt\nstart distribution Pprompt; 2) Given hstart, generate the example sequence Oi = [xi, yi] from p(Oi/hstart, 0*). The test input\nXtest = Xn+1 is sampled similarly. Then, the prompt consists of a sequence of training examples (Sn) followed by the\nexample Xtest:\n\n$$\\left[\\S_{n},\\Pi_{\\mathrm{test}}\\right]=\\left[\\imath_{1},y_{1},\\bar{\\nu}_{2},\\sqrt{\\jmath}2,\\cdot\\cdot\\cdot\\cdot\\cdot\\imath\\bar{\\imath}_{n},\\langle\\imath,\\bar{\\imath}_{\\mathrm{test}}\\right]\\sim\\bar{\\cal P}_{\\mathrm{prompt}}\\varepsilon_{\\mathrm{tot}}\\right]$$\n\n(2)\n\nIn-context learning setups and Assumptions. We follow other settings and assumptions in (Fang & Xie, 2022). With the\ngreedy decoding (Fu et al., 2024b), sampling the next token from the language modeling f(0t|01:t-1) becomes the predictor\nas y = arg max\u2300t f(0t|01:t-1).\n\nThus, for [Sn, Xtest], the in-context learning predictor can be written as fn (xtest) := arg maxy p(y|Sn, Xtest), which outputs\nthe most likely prediction over the pretraining distribution conditioned on the prompt distribution. Its expected 0-1 error\n[1[fn(xtest) \u2260 ytest]].\nwith n examples is L0-1 (fn) = Extest;Ytest~Pprompt\n\nWe define pa(o) := p(O[i] = \uc7740[1 : i - 1], 0) of the i-th token with previous tokens and the analogous distribution\nPiprompt := Pprompt (\u25cb[2] = \uc7740[1 : i - 1]) under the prompt distribution. Following (Fang & Xie, 2022), there is a\ndistinguishability condition formalizes when in-context learning occurs giving the concept 0.\n\nThe distinguishability condition is dependent on a KL divergence between the previous two distributions and the error terms\n\n1Here, training example in prompts means happens during the prompt learning.\n\n30",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1723,
          "y": 0.0577
        },
        {
          "x": 0.8029,
          "y": 0.0577
        },
        {
          "x": 0.8029,
          "y": 0.0719
        },
        {
          "x": 0.1723,
          "y": 0.0719
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 17: Performance comparison of Chinese subtask on LongBench for Qwen2-7B-Instruct.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1755,
          "y": 0.0835
        },
        {
          "x": 0.7988,
          "y": 0.0835
        },
        {
          "x": 0.7988,
          "y": 0.0997
        },
        {
          "x": 0.1755,
          "y": 0.0997
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| Method | Single-Document QA | Multi-Document QA | Summarization | Few-shot Learning | Synthetic | Avg. \u2191 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Method | MF-zh | DuReader | VCSum | LSHT | PR-zh | Avg. \u2191 |\n| Avg len | 6,701 | 15,768 | 15,380 | 22,337 | 6,745 | Avg. \u2191 |\n| Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full |\n| FullKV | 39.17 | 23.63 | 16.21 | 43.50 | 70.50 | 38.60 |\n| Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% |\n| StreamingLLM | 38.05 | 23.24 | 15.92 | 40.50 | 44.50 | 32.44 |\n| H2O | 37.99 | 19.58 | 16.16 | 41.67 | 67.35 | 36.55 |\n| SnapKV | 44.25 | 20.27 | 16.24 | 44.50 | 68.10 | 38.67 |\n| PyramidKV | 36.57 | 20.56 | 16.15 | 43.50 | 66.50 | 36.55 |\n| ChunkKV | 45.92 | 20.15 | 16.37 | 43.75 | 71.10 | 39.45 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1338,
          "y": 0.1112
        },
        {
          "x": 0.8412,
          "y": 0.1112
        },
        {
          "x": 0.8412,
          "y": 0.2896
        },
        {
          "x": 0.1338,
          "y": 0.2896
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# C. Theoretical Understanding",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0879,
          "y": 0.3021
        },
        {
          "x": 0.3424,
          "y": 0.3021
        },
        {
          "x": 0.3424,
          "y": 0.32
        },
        {
          "x": 0.0879,
          "y": 0.32
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "In this section, we provide the theoretical interpretation from the perspective from the In-context learning (ICL) to further\nunderstand how ChunkKV outperforms token-level KV cache compression.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0878,
          "y": 0.3279
        },
        {
          "x": 0.8883,
          "y": 0.3279
        },
        {
          "x": 0.8883,
          "y": 0.3587
        },
        {
          "x": 0.0878,
          "y": 0.3587
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Pretraining Data Distribution. Given a set of concepts \u2463 and a concept 0 E \u65e5, we define the pretraining data is sampled\nfrom p(01, \u00b7 .. , oT) = SAEO p(01,... , oT|0)p(0)d0 (Fang & Xie, 2022). Each token 0 is sampled from a vocabulary 0. For\nsimplicity, we write 01:t = 01 ... Ot.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0878,
          "y": 0.3648
        },
        {
          "x": 0.8893,
          "y": 0.3648
        },
        {
          "x": 0.8893,
          "y": 0.4118
        },
        {
          "x": 0.0878,
          "y": 0.4118
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Language Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Fang & Xie, 2022) usually utilize the next\nword prediction as the language modelling, which predicts the next token Ot given the previous tokens 01 \u00b7 \u00b7 0t-1 for all\nt = 1, \u00b7 \u00b7 , T. Formally, a language modelling can be writen as the distribution f(0t|01:t-1). And it is pretrained on a huge\ncorpus sampled from the pretraining distribution p(01,... , 0t+1) (Fang & Xie, 2022). Considering the large scale of the\nmodel size and dataset size, it can be assumed that the f(01 \u00b7 \u00b7 \u00b7 ot+1) has been aligned with the p(01 \u00b7 \u00b7 \u00b7 Ot+1) (Fang & Xie,\n2022).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0868,
          "y": 0.4184
        },
        {
          "x": 0.8905,
          "y": 0.4184
        },
        {
          "x": 0.8905,
          "y": 0.5079
        },
        {
          "x": 0.0868,
          "y": 0.5079
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Prompt Distribution. Following (Fang & Xie, 2022), a prompt is composed of an input token sequence x followed by an\noutput token y. Then, the i-th training example 1 that can appear in any place in the whole prompt 01:T is defined as Oi\nconsisting of an input Xi = Oi [1 : k - 1] (the first k - 1 tokens) followed by the output Yi = Oi [k] at the end, where the\nlength k is fixed for simplicity.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0875,
          "y": 0.5161
        },
        {
          "x": 0.8889,
          "y": 0.5161
        },
        {
          "x": 0.8889,
          "y": 0.5777
        },
        {
          "x": 0.0875,
          "y": 0.5777
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The i-th training example is independently generated as follows: 1) Generate a start hidden state hstart from a prompt\nstart distribution Pprompt; 2) Given hstart, generate the example sequence Oi = [xi, yi] from p(Oi/hstart, 0*). The test input\nXtest = Xn+1 is sampled similarly. Then, the prompt consists of a sequence of training examples (Sn) followed by the\nexample Xtest:",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0878,
          "y": 0.5844
        },
        {
          "x": 0.89,
          "y": 0.5844
        },
        {
          "x": 0.89,
          "y": 0.6464
        },
        {
          "x": 0.0878,
          "y": 0.6464
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\left[\\S_{n},\\Pi_{\\mathrm{test}}\\right]=\\left[\\imath_{1},y_{1},\\bar{\\nu}_{2},\\sqrt{\\jmath}2,\\cdot\\cdot\\cdot\\cdot\\cdot\\imath\\bar{\\imath}_{n},\\langle\\imath,\\bar{\\imath}_{\\mathrm{test}}\\right]\\sim\\bar{\\cal P}_{\\mathrm{prompt}}\\varepsilon_{\\mathrm{tot}}\\right]$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3057,
          "y": 0.676
        },
        {
          "x": 0.6692,
          "y": 0.676
        },
        {
          "x": 0.6692,
          "y": 0.6957
        },
        {
          "x": 0.3057,
          "y": 0.6957
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(2)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8647,
          "y": 0.6776
        },
        {
          "x": 0.8883,
          "y": 0.6776
        },
        {
          "x": 0.8883,
          "y": 0.693
        },
        {
          "x": 0.8647,
          "y": 0.693
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "In-context learning setups and Assumptions. We follow other settings and assumptions in (Fang & Xie, 2022). With the\ngreedy decoding (Fu et al., 2024b), sampling the next token from the language modeling f(0t|01:t-1) becomes the predictor\nas y = arg max\u2300t f(0t|01:t-1).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.088,
          "y": 0.7087
        },
        {
          "x": 0.8893,
          "y": 0.7087
        },
        {
          "x": 0.8893,
          "y": 0.7563
        },
        {
          "x": 0.088,
          "y": 0.7563
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Thus, for [Sn, Xtest], the in-context learning predictor can be written as fn (xtest) := arg maxy p(y|Sn, Xtest), which outputs\nthe most likely prediction over the pretraining distribution conditioned on the prompt distribution. Its expected 0-1 error\n[1[fn(xtest) \u2260 ytest]].\nwith n examples is L0-1 (fn) = Extest;Ytest~Pprompt",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0872,
          "y": 0.7615
        },
        {
          "x": 0.8883,
          "y": 0.7615
        },
        {
          "x": 0.8883,
          "y": 0.8095
        },
        {
          "x": 0.0872,
          "y": 0.8095
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "We define pa(o) := p(O[i] = \uc7740[1 : i - 1], 0) of the i-th token with previous tokens and the analogous distribution\nPiprompt := Pprompt (\u25cb[2] = \uc7740[1 : i - 1]) under the prompt distribution. Following (Fang & Xie, 2022), there is a\ndistinguishability condition formalizes when in-context learning occurs giving the concept 0.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0874,
          "y": 0.8147
        },
        {
          "x": 0.8891,
          "y": 0.8147
        },
        {
          "x": 0.8891,
          "y": 0.8609
        },
        {
          "x": 0.0874,
          "y": 0.8609
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The distinguishability condition is dependent on a KL divergence between the previous two distributions and the error terms",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0872,
          "y": 0.8674
        },
        {
          "x": 0.8892,
          "y": 0.8674
        },
        {
          "x": 0.8892,
          "y": 0.8838
        },
        {
          "x": 0.0872,
          "y": 0.8838
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "footnote",
      "content": {
        "html": "",
        "markdown": "1Here, training example in prompts means happens during the prompt learning.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1081,
          "y": 0.8919
        },
        {
          "x": 0.5766,
          "y": 0.8919
        },
        {
          "x": 0.5766,
          "y": 0.9087
        },
        {
          "x": 0.1081,
          "y": 0.9087
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "30",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.477,
          "y": 0.9238
        },
        {
          "x": 0.4986,
          "y": 0.9238
        },
        {
          "x": 0.4986,
          "y": 0.9373
        },
        {
          "x": 0.477,
          "y": 0.9373
        }
      ],
      "id": 16,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": true,
  "usage": {
    "pages": 1
  }
}