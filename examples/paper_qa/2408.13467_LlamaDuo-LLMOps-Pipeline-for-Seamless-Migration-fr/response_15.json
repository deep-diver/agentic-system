{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "![image](/image/placeholder)\n- Chart Title: Clude3 Sonnet\n- X-Axis: Synthetic Dataset Volume (k)\n- Y-Axis: Cverage (%) #50\n- Chart Type: line\n|  | 1 k | 4 k | 16 k | 64 k | 256 k |\n| --- | --- | --- | --- | --- | --- |\n| item_01 | 0% | 10% | 20% | 60% | 80% |\n\n\n![image](/image/placeholder)\nSimilarity on GPT4o Response Similarity on Claude3 Sonnet Response Similarity on Gemini 1.5 Flash Response\n\n90 2B fine-tuned on varied volumes of synthetic dataset producted by various\nFigure 8: Performance of Gemma\nservice LLMs including GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash. The first to third columns represent the\n80\nperformance of the model evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges, respectively. The\n70\nfirst row show mean scores, while the second and third rows show the coverage percentage with 50 and 70 scores,\nrespectively. 60\n\nScore\n\n50\n\nMean\n\n# cated computational budget.\n\n40\n\n30\nC More Experimental Results\n\n20\n\nThe performance of Gemma 2B fine-tuned on var-\n10\nied volumes of synthetic dataset produced by var-\n1 2 4\nious service LLMs including GPT4o, Claude 3\nSonnet, and Gemini 1.5 Flash is shown in Figure 8.\n\n# D Case Study\n\nThis section delves into detailed case studies show-\ncasing the enhanced capabilities of the aligned\nsmall-scale local LLMs. We use Gemma 2B and\n7B models as examples to illustrate.\n\nGPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash, of-\nfering a comprehensive assessment of the precision\nand similarity of the models\u2019 responses.\n\nThe cases (Figure 9-17) illustrate the perfor-\nmances of the aligned models across summariza-\ntion, classification, coding, and closed QA tasks.\nSpecifically, these models are tuned on distinct\n128K datasets generated by GPT4o for each corre-\nsponding task. Each case provides evaluations by\n\n8\n\nSynthetic\n\nTo expand the scope of our analysis, we include\ntwo additional cases (Figure 11 and 12) to explore\n16 32 64 128 256\nthe summarization capabilities of the Gemma 2B\nDataset Volume (k)\nand 7B models tuned with 256K synthetic datasets.\nThese datasets are generated by GPT4o, Claude 3\nSonnet, and Gemini 1.5 Flash respectively, provid-\ning valuable insights into the models\u2019 adaptability\nto different training data sources.\n\nThe cases presented above demonstrate the capa-\nbility of the aligned Gemma 2B and 7B models to\nproduce high-quality responses. Additionally, the\ncases offer insight into how different service LLMs\nevaluate text. Through this comparative lens, we\nreveal discernible variances in judgment and as-\nsessment criteria, enriching our understanding of\nthe models\u2019 operational dynamics.",
    "text": ""
  },
  "elements": [
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: Clude3 Sonnet\n- X-Axis: Synthetic Dataset Volume (k)\n- Y-Axis: Cverage (%) #50\n- Chart Type: line\n|  | 1 k | 4 k | 16 k | 64 k | 256 k |\n| --- | --- | --- | --- | --- | --- |\n| item_01 | 0% | 10% | 20% | 60% | 80% |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2301,
          "y": 0.0764
        },
        {
          "x": 0.7863,
          "y": 0.0764
        },
        {
          "x": 0.7863,
          "y": 0.4899
        },
        {
          "x": 0.2301,
          "y": 0.4899
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "figure",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\nSimilarity on GPT4o Response Similarity on Claude3 Sonnet Response Similarity on Gemini 1.5 Flash Response",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.235,
          "y": 0.4674
        },
        {
          "x": 0.7671,
          "y": 0.4674
        },
        {
          "x": 0.7671,
          "y": 0.5012
        },
        {
          "x": 0.235,
          "y": 0.5012
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "90 2B fine-tuned on varied volumes of synthetic dataset producted by various\nFigure 8: Performance of Gemma\nservice LLMs including GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash. The first to third columns represent the\n80\nperformance of the model evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges, respectively. The\n70\nfirst row show mean scores, while the second and third rows show the coverage percentage with 50 and 70 scores,\nrespectively. 60",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1147,
          "y": 0.5133
        },
        {
          "x": 0.8845,
          "y": 0.5133
        },
        {
          "x": 0.8845,
          "y": 0.585
        },
        {
          "x": 0.1147,
          "y": 0.585
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Score",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3229,
          "y": 0.5777
        },
        {
          "x": 0.3288,
          "y": 0.5777
        },
        {
          "x": 0.3288,
          "y": 0.5956
        },
        {
          "x": 0.3229,
          "y": 0.5956
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "50",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.33,
          "y": 0.5941
        },
        {
          "x": 0.3413,
          "y": 0.5941
        },
        {
          "x": 0.3413,
          "y": 0.6006
        },
        {
          "x": 0.33,
          "y": 0.6006
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Mean",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3219,
          "y": 0.5945
        },
        {
          "x": 0.3298,
          "y": 0.5945
        },
        {
          "x": 0.3298,
          "y": 0.6116
        },
        {
          "x": 0.3219,
          "y": 0.6116
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# cated computational budget.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.115,
          "y": 0.6103
        },
        {
          "x": 0.3283,
          "y": 0.6103
        },
        {
          "x": 0.3283,
          "y": 0.626
        },
        {
          "x": 0.115,
          "y": 0.626
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "40",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.33,
          "y": 0.6144
        },
        {
          "x": 0.3413,
          "y": 0.6144
        },
        {
          "x": 0.3413,
          "y": 0.6209
        },
        {
          "x": 0.33,
          "y": 0.6209
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "30\nC More Experimental Results",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1153,
          "y": 0.6371
        },
        {
          "x": 0.3928,
          "y": 0.6371
        },
        {
          "x": 0.3928,
          "y": 0.6536
        },
        {
          "x": 0.1153,
          "y": 0.6536
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "20",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.33,
          "y": 0.6549
        },
        {
          "x": 0.3413,
          "y": 0.6549
        },
        {
          "x": 0.3413,
          "y": 0.6614
        },
        {
          "x": 0.33,
          "y": 0.6614
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The performance of Gemma 2B fine-tuned on var-\n10\nied volumes of synthetic dataset produced by var-\n1 2 4\nious service LLMs including GPT4o, Claude 3\nSonnet, and Gemini 1.5 Flash is shown in Figure 8.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1166,
          "y": 0.6622
        },
        {
          "x": 0.4896,
          "y": 0.6622
        },
        {
          "x": 0.4896,
          "y": 0.7256
        },
        {
          "x": 0.1166,
          "y": 0.7256
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# D Case Study",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.115,
          "y": 0.737
        },
        {
          "x": 0.253,
          "y": 0.737
        },
        {
          "x": 0.253,
          "y": 0.7533
        },
        {
          "x": 0.115,
          "y": 0.7533
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "This section delves into detailed case studies show-\ncasing the enhanced capabilities of the aligned\nsmall-scale local LLMs. We use Gemma 2B and\n7B models as examples to illustrate.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1163,
          "y": 0.7617
        },
        {
          "x": 0.4891,
          "y": 0.7617
        },
        {
          "x": 0.4891,
          "y": 0.8254
        },
        {
          "x": 0.1163,
          "y": 0.8254
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash, of-\nfering a comprehensive assessment of the precision\nand similarity of the models\u2019 responses.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5106,
          "y": 0.6104
        },
        {
          "x": 0.8843,
          "y": 0.6104
        },
        {
          "x": 0.8843,
          "y": 0.658
        },
        {
          "x": 0.5106,
          "y": 0.658
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The cases (Figure 9-17) illustrate the perfor-\nmances of the aligned models across summariza-\ntion, classification, coding, and closed QA tasks.\nSpecifically, these models are tuned on distinct\n128K datasets generated by GPT4o for each corre-\nsponding task. Each case provides evaluations by",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1164,
          "y": 0.8261
        },
        {
          "x": 0.4902,
          "y": 0.8261
        },
        {
          "x": 0.4902,
          "y": 0.9236
        },
        {
          "x": 0.1164,
          "y": 0.9236
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "8",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4918,
          "y": 0.6898
        },
        {
          "x": 0.4977,
          "y": 0.6898
        },
        {
          "x": 0.4977,
          "y": 0.6959
        },
        {
          "x": 0.4918,
          "y": 0.6959
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Synthetic",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4814,
          "y": 0.697
        },
        {
          "x": 0.5192,
          "y": 0.697
        },
        {
          "x": 0.5192,
          "y": 0.7035
        },
        {
          "x": 0.4814,
          "y": 0.7035
        }
      ],
      "id": 16,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "To expand the scope of our analysis, we include\ntwo additional cases (Figure 11 and 12) to explore\n16 32 64 128 256\nthe summarization capabilities of the Gemma 2B\nDataset Volume (k)\nand 7B models tuned with 256K synthetic datasets.\nThese datasets are generated by GPT4o, Claude 3\nSonnet, and Gemini 1.5 Flash respectively, provid-\ning valuable insights into the models\u2019 adaptability\nto different training data sources.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5115,
          "y": 0.6589
        },
        {
          "x": 0.8842,
          "y": 0.6589
        },
        {
          "x": 0.8842,
          "y": 0.7864
        },
        {
          "x": 0.5115,
          "y": 0.7864
        }
      ],
      "id": 17,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The cases presented above demonstrate the capa-\nbility of the aligned Gemma 2B and 7B models to\nproduce high-quality responses. Additionally, the\ncases offer insight into how different service LLMs\nevaluate text. Through this comparative lens, we\nreveal discernible variances in judgment and as-\nsessment criteria, enriching our understanding of\nthe models\u2019 operational dynamics.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5109,
          "y": 0.7884
        },
        {
          "x": 0.8856,
          "y": 0.7884
        },
        {
          "x": 0.8856,
          "y": 0.9159
        },
        {
          "x": 0.5109,
          "y": 0.9159
        }
      ],
      "id": 18,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250404",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}