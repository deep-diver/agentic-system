{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "designed to automatically facilitate the seamless\nmigration from service-oriented LLMs to smaller,\nlocally manageable models without the need for hu-\nman intervention. Our pipeline begins with utiliz-\ning a task-specific initial dataset, referred to as the\ncoverage dataset, to fine-tune a smaller open-source\nLLM. The performance of fine-tuned local LLMs\nis evaluated using a service LLMs-as-a-Judge strat-\negy (Zheng et al., 2024). If the performance of\nthe fine-tuned model falls short of expectations, we\nimprove it by iteratively fine-tuning on additional\nsynthetic data generated by the service LLM. Lla-\nmaDuo ensures that the smaller model is capable of\neventually matching or even surpassing the service\nLLM\u2019s performance in specific downstream tasks,\noffering superior long-term economic advantages.\nTherefore, it presents a practical and scalable solu-\ntion for managing AI deployments in environments\nwhere resources are limited. We conduct extensive\nexperiments and analyses across a range of typical\ntasks, using popular service LLMs such as GPT4o,\nClaude 3 Sonnet, and Gemini 1.5 Flash, as well as\nlocal LLMs, including Gemma 2B and 7B, Mistral\n7B, and LLaMA3 8B, to demonstrate that our Lla-\nmaDuo guarantees the smaller local LLMs possess\nthe potential to eventually match or even exceed\nthe performance of service LLMs in specific down-\nstream tasks. To summarize, our contributions are\nas follows:\n\n- \u2022 We introduce LlamaDuo, an efficient and af-\n- fordable LLMOps pipeline designed to facili-\n- tate seamless migration from service-oriented\n- LLMs to smaller, locally manageable models\n- without human intervention, ensuring service\n- continuity in constrained environments.\n\n\n- \u2022 We employ a multi-turn approach using task-\n- specific synthetic data generated by service\n- LLMs to ensure that LlamaDuo empowers the\n- smaller model to eventually match or even\n- exceed the performance of the service LLM\n- in specific downstream tasks.\n\n\n- \u2022 We substantiate the pipeline\u2019s robust perfor-\n- mance and adaptability in real-world context\n- through comprehensive experiments across a\n- range of typical tasks, employing popular ser-\n- vice LLMs as synthetic data generators and\n- judges for well-known small local LLMs.\n\n\n- \u2022 We emphasize the significant economic advan-\n- tages of LlamaDuo for investing in smaller,\n\n\nlocally manageable LLMs and their deploy-\nment for sustained use, as opposed to the tran-\nsient benefits derived from the token-based\nAPI usage of service LLMs.\n\n# 2 Related Work\n\n# 2.1 Alignment with Instruction Tuning\n\nLLMs pretrained on massive corpora demonstrate\nremarkable capabilities across a wide range of tasks\n(Zhao et al., 2023; Cai et al., 2024; Yoo et al.,\n2024; Wang et al., 2024a). Despite their capa-\nbilities, a notable challenge with LLMs is their\nmisalignment with user instructions, which limits\ntheir practical applications in real-world scenarios\n(Xu et al., 2023; Wang et al., 2023b). The misalign-\nment stems from the initial pretraining objective\nof LLMs, which focuses on minimizing genera-\ntion errors rather than adhering to human instruc-\ntions (Ouyang et al., 2022; Chung et al., 2024). To\nsolve the mismatch, instruction tuning is proposed,\nwhich enables LLMs to complete diverse tasks\nfrom instructions without significant computational\nresources or alterations to the model\u2019s architec-\nture(Longpre et al., 2023; Muennighoff et al., 2023;\nTaori et al., 2023b). Specifically, instruction tun-\ning involves supplementary training of pretrained\nLLMs with datasets structured as instruction-output\npairs (Zhang et al., 2023). The efficacy of instruc-\ntion tuning is largely contingent upon the quality\nand diversity of the instruction datasets employed\n(Wang et al., 2024b). However, the process of cu-\nrating high-quality, diversified data is fraught with\nchallenges, including the extensive time required\nfor creation, privacy concerns, high costs, and the\nneed for substantial human labor (Xu et al., 2023).\nIn response to these challenges, recent studies have\nexplored innovative methods for constructing in-\nstruction datasets, notably the utilization of LLMs\nfor data synthesis (Liu et al., 2024).\n\n2.2 LLM-synthetic Instruction Data\n\nLLMs have demonstrated an unprecedented ability\nto comprehend and execute natural language in-\nstructions (Ouyang et al., 2022; Chung et al., 2024;\nTouvron et al., 2023). This ability is attributed to\nthe process of training LLMs using substantial in-\nstruction datasets (Wang et al., 2023b). However,\nacquiring massive instruction datasets is challeng-\ning due to data scarcity, privacy issues, low data\nquality, and prohibitive costs associated with man-\nual data curation (Abay et al., 2019; Xu et al., 2023;",
    "text": ""
  },
  "elements": [
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "designed to automatically facilitate the seamless\nmigration from service-oriented LLMs to smaller,\nlocally manageable models without the need for hu-\nman intervention. Our pipeline begins with utiliz-\ning a task-specific initial dataset, referred to as the\ncoverage dataset, to fine-tune a smaller open-source\nLLM. The performance of fine-tuned local LLMs\nis evaluated using a service LLMs-as-a-Judge strat-\negy (Zheng et al., 2024). If the performance of\nthe fine-tuned model falls short of expectations, we\nimprove it by iteratively fine-tuning on additional\nsynthetic data generated by the service LLM. Lla-\nmaDuo ensures that the smaller model is capable of\neventually matching or even surpassing the service\nLLM\u2019s performance in specific downstream tasks,\noffering superior long-term economic advantages.\nTherefore, it presents a practical and scalable solu-\ntion for managing AI deployments in environments\nwhere resources are limited. We conduct extensive\nexperiments and analyses across a range of typical\ntasks, using popular service LLMs such as GPT4o,\nClaude 3 Sonnet, and Gemini 1.5 Flash, as well as\nlocal LLMs, including Gemma 2B and 7B, Mistral\n7B, and LLaMA3 8B, to demonstrate that our Lla-\nmaDuo guarantees the smaller local LLMs possess\nthe potential to eventually match or even exceed\nthe performance of service LLMs in specific down-\nstream tasks. To summarize, our contributions are\nas follows:",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1157,
          "y": 0.0848
        },
        {
          "x": 0.4906,
          "y": 0.0848
        },
        {
          "x": 0.4906,
          "y": 0.5521
        },
        {
          "x": 0.1157,
          "y": 0.5521
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "list",
      "content": {
        "html": "",
        "markdown": "- \u2022 We introduce LlamaDuo, an efficient and af-\n- fordable LLMOps pipeline designed to facili-\n- tate seamless migration from service-oriented\n- LLMs to smaller, locally manageable models\n- without human intervention, ensuring service\n- continuity in constrained environments.\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1391,
          "y": 0.5635
        },
        {
          "x": 0.4902,
          "y": 0.5635
        },
        {
          "x": 0.4902,
          "y": 0.6602
        },
        {
          "x": 0.1391,
          "y": 0.6602
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "list",
      "content": {
        "html": "",
        "markdown": "- \u2022 We employ a multi-turn approach using task-\n- specific synthetic data generated by service\n- LLMs to ensure that LlamaDuo empowers the\n- smaller model to eventually match or even\n- exceed the performance of the service LLM\n- in specific downstream tasks.\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1376,
          "y": 0.6738
        },
        {
          "x": 0.4899,
          "y": 0.6738
        },
        {
          "x": 0.4899,
          "y": 0.7688
        },
        {
          "x": 0.1376,
          "y": 0.7688
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "list",
      "content": {
        "html": "",
        "markdown": "- \u2022 We substantiate the pipeline\u2019s robust perfor-\n- mance and adaptability in real-world context\n- through comprehensive experiments across a\n- range of typical tasks, employing popular ser-\n- vice LLMs as synthetic data generators and\n- judges for well-known small local LLMs.\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.139,
          "y": 0.7816
        },
        {
          "x": 0.4904,
          "y": 0.7816
        },
        {
          "x": 0.4904,
          "y": 0.8781
        },
        {
          "x": 0.139,
          "y": 0.8781
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "list",
      "content": {
        "html": "",
        "markdown": "- \u2022 We emphasize the significant economic advan-\n- tages of LlamaDuo for investing in smaller,\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.137,
          "y": 0.8912
        },
        {
          "x": 0.4907,
          "y": 0.8912
        },
        {
          "x": 0.4907,
          "y": 0.9228
        },
        {
          "x": 0.137,
          "y": 0.9228
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "locally manageable LLMs and their deploy-\nment for sustained use, as opposed to the tran-\nsient benefits derived from the token-based\nAPI usage of service LLMs.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5468,
          "y": 0.0854
        },
        {
          "x": 0.8862,
          "y": 0.0854
        },
        {
          "x": 0.8862,
          "y": 0.15
        },
        {
          "x": 0.5468,
          "y": 0.15
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# 2 Related Work",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5098,
          "y": 0.1631
        },
        {
          "x": 0.6658,
          "y": 0.1631
        },
        {
          "x": 0.6658,
          "y": 0.1791
        },
        {
          "x": 0.5098,
          "y": 0.1791
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# 2.1 Alignment with Instruction Tuning",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.511,
          "y": 0.1909
        },
        {
          "x": 0.8322,
          "y": 0.1909
        },
        {
          "x": 0.8322,
          "y": 0.2065
        },
        {
          "x": 0.511,
          "y": 0.2065
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "LLMs pretrained on massive corpora demonstrate\nremarkable capabilities across a wide range of tasks\n(Zhao et al., 2023; Cai et al., 2024; Yoo et al.,\n2024; Wang et al., 2024a). Despite their capa-\nbilities, a notable challenge with LLMs is their\nmisalignment with user instructions, which limits\ntheir practical applications in real-world scenarios\n(Xu et al., 2023; Wang et al., 2023b). The misalign-\nment stems from the initial pretraining objective\nof LLMs, which focuses on minimizing genera-\ntion errors rather than adhering to human instruc-\ntions (Ouyang et al., 2022; Chung et al., 2024). To\nsolve the mismatch, instruction tuning is proposed,\nwhich enables LLMs to complete diverse tasks\nfrom instructions without significant computational\nresources or alterations to the model\u2019s architec-\nture(Longpre et al., 2023; Muennighoff et al., 2023;\nTaori et al., 2023b). Specifically, instruction tun-\ning involves supplementary training of pretrained\nLLMs with datasets structured as instruction-output\npairs (Zhang et al., 2023). The efficacy of instruc-\ntion tuning is largely contingent upon the quality\nand diversity of the instruction datasets employed\n(Wang et al., 2024b). However, the process of cu-\nrating high-quality, diversified data is fraught with\nchallenges, including the extensive time required\nfor creation, privacy concerns, high costs, and the\nneed for substantial human labor (Xu et al., 2023).\nIn response to these challenges, recent studies have\nexplored innovative methods for constructing in-\nstruction datasets, notably the utilization of LLMs\nfor data synthesis (Liu et al., 2024).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5112,
          "y": 0.2142
        },
        {
          "x": 0.8861,
          "y": 0.2142
        },
        {
          "x": 0.8861,
          "y": 0.7261
        },
        {
          "x": 0.5112,
          "y": 0.7261
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "2.2 LLM-synthetic Instruction Data",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5116,
          "y": 0.7399
        },
        {
          "x": 0.8102,
          "y": 0.7399
        },
        {
          "x": 0.8102,
          "y": 0.7553
        },
        {
          "x": 0.5116,
          "y": 0.7553
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "LLMs have demonstrated an unprecedented ability\nto comprehend and execute natural language in-\nstructions (Ouyang et al., 2022; Chung et al., 2024;\nTouvron et al., 2023). This ability is attributed to\nthe process of training LLMs using substantial in-\nstruction datasets (Wang et al., 2023b). However,\nacquiring massive instruction datasets is challeng-\ning due to data scarcity, privacy issues, low data\nquality, and prohibitive costs associated with man-\nual data curation (Abay et al., 2019; Xu et al., 2023;",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5116,
          "y": 0.7622
        },
        {
          "x": 0.8842,
          "y": 0.7622
        },
        {
          "x": 0.8842,
          "y": 0.9218
        },
        {
          "x": 0.5116,
          "y": 0.9218
        }
      ],
      "id": 10,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250404",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}