{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Parse as a Tool w/ LLMs and Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you are using macos\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin/:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API Keys\n",
    "\n",
    "Once `load_dotenv()` is called successfully, you will see `True` is returned and printed out. At this point, all the variables from `.env` file is loaded up as environment variable. Hence, you can access them with `os.getenv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we are going to need the following three API keys:\n",
    "- `SERPER_API_KEY`: API key for Google Search API service from [Serper.dev](https://serper.dev/)\n",
    "- `UPSTAGE_API_KEY`: API key for accessing [Upstage](https://www.upstage.ai/)'s Solar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions\n",
    "\n",
    "Here, we are defining two helper functions necessary for tool(function) calling:\n",
    "- `function_to_schema()`: Converts a Python function into a JSON schema format that can be used for function calling with LLMs. This can be thought as just structured string that is going to be injected into LLM as context so that LLM can understand what kind of functions are available to call\n",
    "- `execute_tool_call()`: Executes the function call based on the LLM's response and returns the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a Python function into a JSON schema format for LLM function calling.\n",
    "    \n",
    "    Args:\n",
    "        func: The Python function to convert to schema\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON schema describing the function's interface\n",
    "        \n",
    "    The schema includes:\n",
    "    - Function name\n",
    "    - Description from docstring\n",
    "    - Parameters with their types\n",
    "    - Required parameters list\n",
    "    \"\"\"\n",
    "    # Map Python types to JSON schema types\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\", \n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get function signature using inspect\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    # Build parameters dictionary\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            # Get JSON type for parameter, default to string if type not found\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    # Get list of required parameters (those without default values)\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    # Return complete schema\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    \"\"\"\n",
    "    Executes a function call based on the LLM's response.\n",
    "    \n",
    "    Args:\n",
    "        tool_call: Object containing function call details from LLM\n",
    "        tools: Dictionary mapping function names to actual functions\n",
    "        agent_name: Name of the agent making the call, for logging\n",
    "        \n",
    "    Returns:\n",
    "        The result of executing the specified function with given arguments\n",
    "        \n",
    "    This function:\n",
    "    1. Extracts function name and arguments from tool_call\n",
    "    2. Logs the function call\n",
    "    3. Executes the function with provided arguments\n",
    "    \"\"\"\n",
    "    # Extract function name and parse arguments from JSON\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Log the function call\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    # Execute the function with unpacked arguments\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tokens_if_needed(tokenizer, messages, content, max_token_limit=32000):\n",
    "    \"\"\"\n",
    "    Truncate the markdown content if the total tokens exceed the maximum limit.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding/decoding\n",
    "        messages: List of message dictionaries for the conversation\n",
    "        content: The markdown content to potentially truncate\n",
    "        max_token_limit: Maximum token limit (default: 32000)\n",
    "        \n",
    "    Returns:\n",
    "        truncated_markdown: The potentially truncated markdown\n",
    "        base_token_numbers: Number of tokens in the base conversation\n",
    "        paper_token_numbers: Number of tokens in the paper after potential truncation\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": supervisor_agent.instructions}\n",
    "        ] + messages\n",
    "    )\n",
    "    base_token_numbers = len(inputs)\n",
    "    encoded_content = tokenizer.encode(content)\n",
    "    paper_token_numbers = len(encoded_content)\n",
    "\n",
    "    print(f\"Base token numbers: {base_token_numbers}\")\n",
    "    print(f\"Paper token numbers: {paper_token_numbers}\")\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "\n",
    "    total_token_numbers = base_token_numbers + paper_token_numbers\n",
    "\n",
    "    if total_token_numbers > max_token_limit:\n",
    "        # Calculate how many tokens we need to truncate\n",
    "        tokens_to_keep = max_token_limit - base_token_numbers\n",
    "        # Truncate the encoded markdown\n",
    "        encoded_content = encoded_content[:tokens_to_keep]\n",
    "        # Update the paper token count\n",
    "        paper_token_numbers = len(encoded_content)\n",
    "        # Update the markdown string by decoding the truncated tokens\n",
    "        truncated_content = tokenizer.decode(encoded_content, skip_special_tokens=True)\n",
    "        print(f\"Truncated paper tokens to: {paper_token_numbers}\")\n",
    "    else:\n",
    "        print(\"No truncation needed\")\n",
    "        truncated_content = content\n",
    "\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "    return truncated_content, base_token_numbers, paper_token_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filesystem based Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
    "message_template = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper below, \\n\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def to_paper_search_agent(paper_title: str):\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "\n",
    "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
    "    headers = {\n",
    "        'X-API-KEY': SERPER_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    search_results = response.json()['organic']\n",
    "    \n",
    "    if len(search_results) == 0:\n",
    "        return \"Count not find the URL to download the paper\"\n",
    "    \n",
    "    first_result = search_results[0]\n",
    "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
    "        return \"Could not find the URL to download the paper\"\n",
    "    \n",
    "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
    "\n",
    "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
    "    # Open the PDF\n",
    "    pdf = PdfReader(input_pdf_path)\n",
    "    total_pages = len(pdf.pages)\n",
    "    \n",
    "    # Calculate number of output PDFs needed\n",
    "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
    "    \n",
    "    output_paths = []\n",
    "    \n",
    "    # Split into multiple PDFs\n",
    "    for i in range(num_pdfs):\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Calculate start and end pages for this split\n",
    "        start_page = i * pages_per_pdf\n",
    "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
    "        \n",
    "        # Add pages to writer\n",
    "        for page_num in range(start_page, end_page):\n",
    "            writer.add_page(pdf.pages[page_num])\n",
    "            \n",
    "        # Save the split PDF\n",
    "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
    "        with open(output_path, \"wb\") as output_file:\n",
    "            writer.write(output_file)\n",
    "        output_paths.append(output_path)\n",
    "        \n",
    "    return output_paths\n",
    "\n",
    "def get_document_parse_response(filename, api_key):\n",
    "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    files = {\"document\": open(filename, \"rb\")}\n",
    "    data = {\"output_formats\": \"['markdown']\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    upstage_response = json.loads(response.text)\n",
    "    return upstage_response\n",
    "\n",
    "def get_md_with_document_parse(root_path, paper_url):\n",
    "    response = requests.get(paper_url)\n",
    "    # Save the PDF to a temporary file\n",
    "    \n",
    "    pdf_path = f\"{root_path}/paper.pdf\"\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "            \n",
    "    split_factor = 1\n",
    "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
    "\n",
    "    markdown = \"\"\n",
    "    total_responses = []\n",
    "    for i, split_pdf in enumerate(split_pdfs):\n",
    "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
    "        \n",
    "        # Append the response to the total_responses list\n",
    "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})        \n",
    "        # Also write the response to a JSON file for persistence\n",
    "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
    "        with open(json_output_path, \"w\") as json_file:\n",
    "            json.dump(upstage_response, json_file, indent=2)\n",
    "\n",
    "        try:\n",
    "            markdown += upstage_response['content']['markdown']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return markdown\n",
    "\n",
    "def get_md_from_fs(root_path):\n",
    "    markdown = \"\"\n",
    "    for file in os.listdir(root_path):\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(root_path, file), \"r\") as f:\n",
    "                upstage_response = json.load(f)\n",
    "                markdown += upstage_response['content']['markdown']\n",
    "    return markdown\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    paper_id = paper_url.split(\"/\")[-1]\n",
    "    root_path = paper_id\n",
    "\n",
    "    if os.path.exists(root_path):\n",
    "        print(f\"Found cached markdown for {paper_id}\")\n",
    "        markdown = get_md_from_fs(root_path)\n",
    "    else:\n",
    "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
    "        os.makedirs(root_path, exist_ok=True)\n",
    "        markdown = get_md_with_document_parse(root_path, paper_url)\n",
    "\n",
    "    markdown = \"Retrieved Paper Content\\n-----------------------------------\\n\" + markdown\n",
    "    markdown, _, _ = truncate_tokens_if_needed(tokenizer, message_template, markdown)\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"solar-pro\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(client, messages, supervisor_agent):\n",
    "    # Loop through the conversation steps\n",
    "    while True:\n",
    "        # Prepare tools for the current step\n",
    "        tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in supervisor_agent.tools}\n",
    "        \n",
    "        # Get model response\n",
    "        response = client.chat.completions.create(\n",
    "            model=supervisor_agent.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            print(response.choices[0].message.tool_calls)\n",
    "        else:\n",
    "            print(\"--------------------------------\")\n",
    "            print(response.choices[0].message.content)\n",
    "            print(\"--------------------------------\")\n",
    "            break # escape the loop when there is no need for tool(function) call anymore\n",
    "        \n",
    "        # Add model response to messages\n",
    "        messages.append(response.choices[0].message)\n",
    "        \n",
    "        # Add tool response to messages\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_response = execute_tool_call(tool_call, tools, supervisor_agent.name)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\", \n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"content\": tool_response\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='10c7b285-2f29-41a1-9714-b7ab7f9ca486', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='1cc34b5e-0822-4e7d-8991-cf0fba0aa44e', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "No cached markdown found for 2502.00299, parsing from URL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (57243 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base token numbers: 115\n",
      "Paper token numbers: 57243\n",
      "Total token numbers: 57358\n",
      "Truncated paper tokens to: 31885\n",
      "Total token numbers: 32000\n",
      "--------------------------------\n",
      "The assistant should now reach the point in the conversation where it can provide a final answer based on the generated output of the function call.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    \n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='3f6464ec-120f-41a0-8822-1a6db6c7ae82', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='a17098c5-3cbb-4393-88f0-a7908540bbb6', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "Found cached markdown for 2502.00299\n",
      "Base token numbers: 115\n",
      "Paper token numbers: 57243\n",
      "Total token numbers: 57358\n",
      "Truncated paper tokens to: 31885\n",
      "Total token numbers: 32000\n",
      "--------------------------------\n",
      "ChunkKV is a simple yet effective KV cache compression method that retains essential information through selective chunk retention. It achieves better performance than existing KV cache compression methods by preserving semantic information and efficiently reusing indices. In experiments across various long-context benchmarks, including LongBench and Needle-In-A-HayStack, as well as GSM8K and JailbreakV in-context learning benchmarks, ChunkKV consistently outperforms other methods. This establishes ChunkKV as a promising approach for maintaining crucial information in the KV cache.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    \n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv. \"},\n",
       " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='3f6464ec-120f-41a0-8822-1a6db6c7ae82', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': '3f6464ec-120f-41a0-8822-1a6db6c7ae82',\n",
       "  'content': \"URL to download 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference': https://arxiv.org/pdf/2502.00299\"},\n",
       " ChatCompletionMessage(content=None, refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='a17098c5-3cbb-4393-88f0-a7908540bbb6', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'a17098c5-3cbb-4393-88f0-a7908540bbb6',\n",
       "  'content': 'Retrieved Paper Content\\n-----------------------------------\\nChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTable 11: Detailed comparison of KV cache metrics across different task categories in LongBench.\\n\\n| Method | Single-Document QA | Multi-Document QA | Summarization | Few-shot Learning | Synthetic & Code |\\n| --- | --- | --- | --- | --- | --- |\\n| KV Cache L1 Loss ↓ | KV Cache L1 Loss ↓ | KV Cache L1 Loss ↓ | KV Cache L1 Loss ↓ | KV Cache L1 Loss ↓ | KV Cache L1 Loss ↓ |\\n| ChunkKV | 0.8741 | 0.8748 | 0.8770 | 0.8861 | 0.8726 |\\n| SnapKV | 0.8921 | 0.8933 | 0.8930 | 0.8917 | 0.8938 |\\n| H2O | 0.8905 | 0.8917 | 0.8913 | 0.8906 | 0.8915 |\\n| Attention Score Cosine Similarity ↑ | Attention Score Cosine Similarity ↑ | Attention Score Cosine Similarity ↑ | Attention Score Cosine Similarity ↑ | Attention Score Cosine Similarity ↑ | Attention Score Cosine Similarity ↑ |\\n| ChunkKV | 0.3567 | 0.3651 | 0.3841 | 0.4330 | 0.3805 |\\n| SnapKV | 0.3513 | 0.3594 | 0.3771 | 0.4305 | 0.3759 |\\n| H2O | 0.3491 | 0.3572 | 0.3750 | 0.4284 | 0.3740 |\\n\\n\\nA.2. Hypothetical Scenario\\n\\nTo provide a deeper understanding of ChunkKV’s effectiveness compared to discrete token-based methods, we present a\\ndetailed analysis using a hypothetical scenario. This analysis aims to illustrate the fundamental differences between these\\napproaches and explain why ChunkKV is more effective at preserving semantic information in long contexts.\\n\\nConsider a comprehensive document that contains detailed information on various animals, including their habitats, diets,\\nand behaviors. A user asks the question \"What do pandas eat in the wild?\"\\n\\nBoth ChunkKV and discrete token-based methods would use this question to calculate observation scores for the document.\\nHowever, their approaches to selecting and retaining information differ significantly.\\n\\nA.2.1. DISCRETE TOKEN-BASED METHOD\\n\\nA discrete token-based method might identify and retain individual tokens with high relevance scores, such as:\\n\\n• “pandas\",“eat\", “bamboo\", “wild\", “diet\", “food\"\\n\\nAlthough these tokens are relevant, they lack context and coherence. The method might discard other essential tokens that\\nprovide crucial context or complete the information.\\n\\nA.2.2. CHUNKKV METHOD\\n\\nIn contrast, ChunkKV would identify and retain semantically meaningful chunks, such as:\\n\\n- • “In the wild, pandas primarily eat bamboo shoots and leaves\"\\n- • “Their diet consists of 99% bamboo, but they occasionally consume other vegetation\"\\n- • “Wild pandas may also eat small rodents or birds when available\"\\n\\n\\nBy preserving these chunks, ChunkKV maintains not only the relevant keywords but also their contextual relationships and\\nadditional pertinent information.\\n\\nA.3. Comparative Analysis\\n\\nThe advantages of ChunkKV become evident when we consider how these retained pieces of information would be used in\\nsubsequent processing:\\n\\n1. Contextual Understanding: Discrete tokens require the model to reconstruct meaning from isolated words, which\\ncould lead to ambiguity. ChunkKV provides complete phrases or sentences, allowing for immediate and accurate\\ncomprehension.\\n\\n17ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTable 4: Many-Shot GSM8K Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 71.2% ↑ |\\n| 10% | 63.2% | 54.2% | 54.1% | 59.2% | 68.2% |\\n| LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ | LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ | LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ | LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ | LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ | LlaMa-3.1-8B-Instruct FullKV: 82.4% ↑ |\\n| 10% | 74.3% | 51.2% | 68.2% | 70.3% | 79.3% |\\n\\n\\nFor more details on the prompt settings, please refer to the\\nAPPENDIX G.\\n\\nTable 3 presents the performance comparison. The results\\ndemonstrate that ChunkKV outperforms other KV cache\\ncompression methods on different models and compres-\\nsion ratios. Table 4 presents the performance comparison\\nof many-shot GSM8K, also ChunkKV outperforms other\\nKV cache compression methods. The consistent superior\\nperformance of ChunkKV in both models underscores its\\neffectiveness in maintaining crucial contextual information\\nfor complex arithmetic reasoning tasks.\\n\\nJailbreak In this section, we evaluate the performance of\\nChunkKV on the JailbreakV benchmark (Luo et al., 2024).\\nThe prompt settings are the same as those used by Luo et al.\\n(2024).\\n\\nTable 5 presents the performance comparison. The results\\ndemonstrate that ChunkKV outperforms other KV cache\\ncompression methods on different models and compression\\nratios. Which shows the effectiveness of ChunkKV in main-\\ntaining crucial contextual information for safety benchmark.\\n\\nTable 5: JailbreakV Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ | LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ | LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ | LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ | LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ | LlaMa-3.1-8B-Instruct FullKV: 88.9% ↑ |\\n| 20% | 65.0% | 71.7% | 88.0% | 87.5% | 89.0% |\\n| 10% | 53.1% | 65.4% | 84.3% | 85.5% | 87.9% |\\n\\n\\n# 4.2. Long-Context Benchmark\\n\\nLongBench and NIAH are two widely used benchmarks for\\nKV cache compression methods. Both benchmarks have a\\ncontext length that exceeds 10K. NIAH requires retrieval\\ncapability, while LongBench is a meticulously designed\\nbenchmark suite that tests the capabilities of language mod-\\nels in handling extended documents and complex informa-\\ntion sequences.\\n\\nLongBench We use LongBench (Bai et al., 2024) to as-\\nsess the performance of ChunkKV on tasks involving long-\\ncontext inputs. For more details on LongBench, please\\nrefer to the APPENDIX F. We evaluated multiple KV cache\\neviction methods using the LongBench benchmark with\\nLLaMA-3-8B-Instruct (Meta, 2024), Mistral-7B-Instruct-\\n\\nTable 6: KV cache compression methods on the LongBench\\nbenchmark. Results show performance gap compared to Ful-\\nlKV baseline (negative values indicate worse performance).\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| LlaMa-3-8B-Instruct FullKV: 41.46 ↑ | LlaMa-3-8B-Instruct FullKV: 41.46 ↑ | LlaMa-3-8B-Instruct FullKV: 41.46 ↑ | LlaMa-3-8B-Instruct FullKV: 41.46 ↑ | LlaMa-3-8B-Instruct FullKV: 41.46 ↑ | LlaMa-3-8B-Instruct FullKV: 41.46 ↑ |\\n| 10% | -13.80% | -10.61% | -3.16% | -3.33% | -2.29% |\\n| 20% | -6.42% | -8.85% | -2.24% | -2.00% | -1.74% |\\n| 30% | -2.36% | -5.38% | -0.07% | -0.22% | +0.31% |\\n| Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ | Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ | Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ | Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ | Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ | Mistral-7B-Instruct-v0.3 FullKV: 48.08 ↑ |\\n| 10% | -16.58% | -9.30% | -3.54% | -3.52% | -2.85% |\\n| Qwen2-7B-Instruct FullKV: 40.71 ↑ | Qwen2-7B-Instruct FullKV: 40.71 ↑ | Qwen2-7B-Instruct FullKV: 40.71 ↑ | Qwen2-7B-Instruct FullKV: 40.71 ↑ | Qwen2-7B-Instruct FullKV: 40.71 ↑ | Qwen2-7B-Instruct FullKV: 40.71 ↑ |\\n| 10% | -5.28% | -0.64% | -0.39% | -0.98% | +0.42% |\\n\\n\\nv0.3 (Jiang et al., 2023a), and Qwen2-7B-Instruct (Yang\\net al., 2024a), with a KV cache compression ratio of 10%.\\nThe LongBench provides the Chinese subtask, and Qwen2-\\n7B-Instruct also supports Chinese, so we tested Qwen2-7B-\\nInstruct with different KV cache compression methods on\\nthe Chinese subtasks.\\n\\nTables 6 show that ChunkKV is capable of achieving on-par\\nperformance or even better than the full KV cache with less\\nGPU memory consumption. This table presents the perfor-\\nmance gap (in percentage) between each method and the Ful-\\nlKV baseline, where negative values indicate performance\\ndegradation compared to FullKV. The table is evaluated in\\nthe LongBench English subtask, where ChunkKV outper-\\nforms other compression methods overall. This suggests\\nthat ChunkKV’s approach of retaining semantic chunks is\\nmore effective in preserving important information com-\\npared to other discrete token-based compression methods.\\nFor detailed results and Chinese subtask results, please refer\\nto Appendix B.2 and B.5.\\n\\nNeedle-In-A-HayStack We use Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023) to evaluate LLMs’ long-context\\nretrieval capability. NIAH assesses how well LLM extract\\nhidden tricked information from extensive documents, and\\nfollow LLM-as-a-Judge (Zheng et al., 2023) we apply GPT-\\n4o-mini (OpenAI, 2023) to assess the accuracy of the re-\\ntrieved information. We evaluated multiple KV cache evic-\\ntion methods using NIAH with LLaMA-3-8B-Instruct and\\nMistral-7B-Instruct-v0.2, setting benchmark context lengths\\nto 8k and 32k tokens.\\n\\nTable 7 provides statistical results for different compression\\nmethods. These findings clearly indicate the effectiveness\\nof ChunkKV in managing varying token lengths and depth\\npercentages, making it a robust choice for KV cache man-\\nagement in LLMs. Figure 3 presents the NIAH benchmark\\nresults for LLaMA-3-8B-Instruct. The vertical axis repre-\\nsents the depth percentage, while the horizontal axis repre-\\nsents the token length, with shorter lengths on the left and\\nlonger lengths on the right. A cell highlighted in green indi-\\n\\n6ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LLaMA-3-8B-Instruct with ChunkKV\\n- X-Axis: Layer\\n- Y-Axis: Layer\\n- Chart Type: line\\n|  | top_graph | second_graph | bottom_graph |\\n| --- | --- | --- | --- |\\n| item_01 | 0.32No units explicitly visible | 0.32No units explicitly visible | 0.32No units explicitly visible |\\n\\n\\nFigure 9: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on LLaMA-3-8B-Instruct\\n\\n![image](/image/placeholder)\\n- Chart Title: Mistral-7B-Instruct with H2O\\n- Y-Axis: LEVERT\\n- Chart Type: line\\n|  | top_left | top_right | middle_left | middle_right | bottom_left | bottom_right |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 0.8 | 0.6 | 0.4 | 0.2 | 0.0 | 0.0 |\\n\\n\\nFigure 10: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on Mistral-7B-Instruct\\n\\n21ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LLaMA-3-8B-Instruct with H2o\\n- Chart Type: bar\\n|  | 1.00 | 1.01 | 1.02 | 1.03 | 1.04 | 1.05 | 1.06 | 1.07 | 1.08 | 1.09 | 1.10 | 1.11 | 1.12 | 1.13 | 1.14 | 1.15 | 1.16 | 1.17 | 1.18 | 1.19 | 1.20 | 1.21 | 1.22 | 1.23 | 1.24 | 1.25 | 1.26 | 1.27 | 1.28 | 1.29 | 1.30 | 1.31 | 1.32 | 1.33 | 1.34 | 1.35 | 1.36 | 1.37 | 1.38 | 1.39 | 1.40 | 1.41 | 1.42 | 1.43 | 1.44 | 1.45 | 1.46 | 1.47 | 1.48 | 1.49 | 1.50 | 1.51 | 1.52 | 1.53 | 1.54 | 1.55 | 1.56 | 1.57 | 1.58 | 1.59 | 1.60 | 1.61 | 1.62 | 1.63 | 1.64 | 1.65 | 1.66 | 1.67 | 1.68 | 1.69 | 1.70 | 1.71 | 1.72 | 1.73 | 1.74 | 1.75 | 1.76 | 1.77 | 1.78 | 1.79 | 1.80 | 1.81 | 1.82 | 1.83 | 1.84 | 1.85 | 1.86 | 1.87 | 1.88 | 1.89 | 1.90 | 1.91 | 1.92 | 1.93 | 1.94 | 1.95 | 2.96 | 2.97 | 2.98 | 2.99 | 2.00 | 2.101 | 2.12 | 2.13 | 2.14 | 2.15 | 2.16 | 2.17 | 2.18 | 2.19 | 2.20 | 2.21 | 2.22 | 2.23 | 2.24 | 2.25 | 2.26 | 2.27 | 2.28 | 2.29 | 2.30 | 3.31 | 3.32 | 3.3 | 3.43 | 3.5 | 3.56 | 3.57 | 3.58 | 3.69 | 3.71 | 3.72 | 3.73 | 3.74 | 3.75 | 3.76 | 3.77 | 3.78 | 3.79 | 3.8 | 3.81 | 3.82 | 3.83 | 3.84 | 3.85 | 3.86 | 3.87 | 3.88 | 3.89 | 3.90 | 3.91 | 3.92 | 3.93 | 3.94 | 3.95 | 3.96 | 3.97 | 3.98 | 3.99 | 3.00 | 3.101 | 3.12 |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n\\n\\nFigure 7: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on LLaMA-3-8B-Instruct\\n\\n![image](/image/placeholder)\\n- Chart Title: LLaMA-3-8B-Instruct with SnapKV\\n- X-Axis: Layer\\n- Y-Axis: Layer\\n- Chart Type: line\\n|  | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17 | 18 | 19 | 20 | 21 | 22 | 23 | 24 | 25 | 26 | 27 | 28 | 29 | 30 | 31 |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 0.0No units specified | 0.12No units specified | 0.12No units specified | 0.0No units specified | 0.0No units specified | 0.2No units specified | 0.22No units specified | 0.22No units specified | 0.23No units specified | 0.24No units specified | 0.25No units specified | 0.26No units specified | 0.26No units specified | 0.27No units specified | 0.28No units specified | 0.29No units specified | 0.3No units specified | 0.3No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified | 0.32No units specified |\\n\\n\\nFigure 8: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on LLaMA-3-8B-Instruct\\n\\n20ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTable 7: NIAH Performance Comparison.\\n\\n| KV cache Size | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ | LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ | LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ | LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ | LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ | LlaMa-3.1-8B-Instruct FullKV: 74.6% ↑ |\\n| 512 | 32.0% | 68.6% | 71.2 % | 72.6% | 74.5% |\\n| 256 | 28.0% | 61.7% | 68.8% | 69.5% | 74.1% |\\n| 128 | 23.7% | 47.9% | 58.9% | 65.1% | 73.8% |\\n| 96 | 21.5% | 41.0% | 56.2% | 63.2% | 70.3% |\\n| Mistral-7B-Instruct FullKV: 99.8% ↑ | Mistral-7B-Instruct FullKV: 99.8% ↑ | Mistral-7B-Instruct FullKV: 99.8% ↑ | Mistral-7B-Instruct FullKV: 99.8% ↑ | Mistral-7B-Instruct FullKV: 99.8% ↑ | Mistral-7B-Instruct FullKV: 99.8% ↑ |\\n| 128 | 44.3% | 88.2% | 91.6% | 99.3% | 99.8% |\\n\\n\\ncates that the method can retrieve the needle at that length\\nand depth percentage. The detail visualization of the NIAH\\nbenchmark can be found in Appendix B.3. The visualization\\nresults demonstrate that ChunkKV outperforms other KV\\ncache compression methods.\\n\\n![image](/image/placeholder)\\n(a) ChunkKV, accuracy 73.8%\\n(b) PyramidKV, accuracy 65.1%\\n(c) SnapKV, accuracy 58.9%\\n\\n(d) StreamingLLM, accuracy 23.7%\\n\\nFigure 3: NIAH benchmark for LLaMA3-8B-Instruct with\\nKV cache size=128 under 8k context length.\\n\\n4.3. Index Reuse\\n\\nThis section will evaluate the performance of the layer-wise\\nindex reuse approach with ChunkKV from the two aspects\\nof efficiency and performance.\\n\\nMeasuring Efficiency. We evaluated the latency and\\nthroughput of ChunkKV compared to FullKV using\\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\\nwere conducted with reuse layer is 2, batch size set to 1\\nand inference was performed using Flash Attention 2, each\\nexperiment was repeated 10 times and the average latency\\nand throughput were reported.\\n\\nTable 8: Latency and throughput comparison between\\nChunkKV and FullKV under different input-output configu-\\nrations. Percentages in parentheses indicate improvements\\nover FullKV baseline.\\n\\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\\n| --- | --- | --- | --- | --- |\\n| Method | Input | Output | Latency(s) ↓ | Throughput(T/S) ↑ |\\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\\n\\n\\nThe results in Table 8 shows that the layer-wise index reuse\\nstrategy (ChunkKV_reuse) further boosts performance,\\nachieving up to a 20.7% reduction in latency, and through-\\nput improvements are particularly notable for longer input\\nsequences, with ChunkKV_reuse delivering up to a 26.5%\\nimprovement over FullKV.\\n\\nIndex Reuse Performance on LongBench\\n\\n![image](/image/placeholder)\\n- Chart Title: Index Reuse Performance on LongBench\\n- X-Axis: Number of Index Reuse Layers\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\\n\\n\\nFigure 4: Comparison with different index reuse layers on\\nLongBench.\\n\\nMeasuring Task Performance. This experiment evaluates\\nthe performance of the layer-wise index reuse approach by\\nmeasuring the performance of the LongBench (Bai et al.,\\n2024), the experiment settings are the same as LongBench\\nin 4.2. And the number of index reuse layers is set from 1\\nto the number of layers in the model, where an index reuse\\nlayer of 1 corresponds to the normal ChunkKV without\\nindex reuse, and our method set reuse layer to 2.\\n\\nFigure 4 illustrates the performance of ChunkKV with vary-\\ning index reuse layers on the LongBench benchmark. Gen-\\nerally, reuse layer set to 2 can achieve the minimal perfor-\\nmance degradation across all models. For more experiments\\n\\n7ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# A. In-depth Analysis of ChunkKV vs. Discrete Token Methods\\n\\nA.1. Quantitative Analysis\\n\\nTo rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic\\nexperiments using a LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of\\nLongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine\\nsimilarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground\\ntruth. 2. Applied ChunkKV, SnapKV, and H2O compression methods with a fixed 10% compression ratio, and the parameters\\nof the three methods are set the same as in Table 14. 3. Measured the differences between compressed and uncompressed\\nversions.\\n\\n![image](/image/placeholder)\\n- Chart Title: Layer-wise KV cache L Loss\\n- X-Axis: Layer\\n- Y-Axis: L Loss\\n- Chart Type: line\\n|  | Layer 0 | Layer 1 | Layer 2 | Layer 3 | Layer 4 | Layer 5 | Layer 6 | Layer 7 | Layer 8 | Layer 9 | Layer 10 | Layer 11 | Layer 12 | Layer 13 | Layer 14 | Layer 15 | Layer 16 | Layer 17 | Layer 18 | Layer 19 | Layer 20 | Layer 21 | Layer 22 | Layer 23 | Layer 24 | Layer 25 | Layer 26 | Layer 27 | Layer 28 | Layer 29 | Layer 30 |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 0.6Not explicitly visible | 0.8Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.8Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible | 0.9Not explicitly visible |\\n\\n\\nFigure 6: Layer-wise comparison of L1 loss and attention cosine similarity between ChunkKV and discrete token-based\\nmethods in Single-Document QA sub-category of LongBench.\\n\\nResults Analysis As shown in Figure 6, ChunkKV demonstrates superior performance across both metrics:\\n\\n- • KV Cache L1 Loss: ChunkKV achieves consistently lower L1 loss compared to SnapKV and H2O, particularly in the\\n- early and middle layers (layers 5-25). This indicates better preservation of the original KV cache information through\\n- the semantic chunk-based approach.\\n\\n\\n- • Attention Cosine Similarity: ChunkKV exhibits higher similarity scores across most layers, with notably strong\\n- performance in layers 0-5 and 20-30. This suggests better preservation of attention relationships between tokens, which\\n- is crucial for maintaining semantic understanding.\\n\\n\\nTo quantify these improvements, we calculated average metrics across all layers, as shown in Table 11. ChunkKV achieves\\nboth the lowest L1 loss and highest attention cosine similarity, outperforming both baseline methods.\\n\\nSignificance of Results While the improvements may appear modest in absolute terms (approximately 2% in L1 loss and\\n1.5% in cosine similarity), their practical significance is substantial. These metrics reflect the model’s ability to maintain\\ncrucial semantic relationships and attention patterns, which are essential for complex reasoning tasks. The consistent\\nimprovements across different sequences demonstrate that preserving semantic chunks leads to better information retention\\nthan selecting individual tokens.\\n\\nThe enhanced performance is particularly evident in the middle layers of the model, which are typically responsible for\\nhigher-level semantic processing. This provides concrete evidence for why ChunkKV achieves superior performance on\\ndownstream tasks compared to discrete token-based methods.\\n\\n16ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Type: bar\\n|  | China | Chinese | Tamil | United States | Latino | d\\'I | Human Resources | Adhesives | Solvents | Other |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 73.8% | 65.0% | 58.9% | 47.9% | 23.7% | 74.9% | 10.0% | 12.0% | 0.5% | 0.5% |\\n\\n\\nFigure 17: NIAH benchmark for LLaMA-3-8B-Instruct with KV cache size=128 under 8k context length\\n\\n27ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nE⌀ resulting from the distribution mismatch between the prompt and the pertaining distributions for each example. Letting\\npio) and Piprompt\\ncorrespond to the concept 0 and 0*.\\n\\nCondition 1 (distinguishability (Fang & Xie, 2022)). The 0* is distinguishable if for all 0 E 요, 0 ≠ 0*,\\n\\n$$\\\\sum_{i=1}^{k}K L_{i}(\\\\theta^{\\\\star}||\\\\theta)>\\\\epsilon_{\\\\theta},$$\\n\\n(3)\\n\\n# [KL(piprompt |lp�)].\\nwhere the KLi(0*||0) := EO[1:i-1]~pprompt\\n\\nNoises from KV Cache Compression. Naturally, because of the sparsified KV cache, some history tokens in 01:t-1 at\\ndifferent layers lost its attention score calculation with respect to the next word prediction Ot. We can regard this as the noise\\nadded onto the 01:t-1. Thus, distincting 0* from 0 requires larger KL divergence. Following (Zhou et al., 2024), we provide\\nthe following second condition about the distinguishability with the KV cache sparsity.\\n\\nCondition 2 (distinguishability under sparsified KV cache). With the noise introduced by the sparsified KV cache of the\\nsparse ratio T, the distribution mismatch between the prompt and the pretraining distribution that is approximated by LLM\\nis enlarged, resulting in a varied requirement with error term 50(r) for 0* being distinguishable iffor all 0 E 日, 0 ≠ 0*,\\n\\n$$\\\\sum_{i=1}^{k}K L_{i}(\\\\theta^{*}||\\\\theta)>\\\\epsilon_{\\\\theta}+\\\\xi_{\\\\theta}(r),\\\\quad\\\\mathrm{where}\\\\quad\\\\xi_{\\\\theta}(r)\\\\propto r.$$\\n\\n(4)\\n\\nLemma 1 (noisy-relaxed bound in (Fang & Xie, 2022; Zhou et al., 2024)). let B denotes the set of 0 which does not\\nsatisfy Condition 1. We assume that KL(Pprompt (yoear|xrest)]|P(yhear|xnest, 0) is boundedfor all 0 and that 0* minimizes the\\nmulti-class logistic risk as,\\n\\n$${\\\\cal L}_{C E}(\\\\theta)=-\\\\mathrm{i}\\\\mathrm{E}_{x_{t e t}\\\\sim p_{p r o m p t}}[p_{p r o m p t}(y_{t e s t}|x_{t e s t})\\\\cdot\\\\log p(y_{t e s t}|x_{t e s t},\\\\theta)].$$\\n\\n(5)\\n\\n# If\\n\\n$$\\\\mathbb{E}_{x_{t r e r}\\\\sim p_{u r o m p t}}[K L(p_{p r o m p t}(y_{t e s t}|x_{t e t})]]p(y_{t e s t}|x_{t e s t},\\\\theta))]\\\\leq(\\\\epsilon_{\\\\theta}+\\\\xi_{\\\\theta}(r)),\\\\quad\\\\forall\\\\quad\\\\theta\\\\in B,$$\\n\\n(6)\\n\\n# then\\n\\n$$\\\\operatorname*{lim}_{n\\\\rightarrow\\\\infty}L_{0-1}(f_{n})\\\\le\\\\mathrm{inf}\\\\,L_{0-1}(f)+g^{-1}\\\\left(\\\\mathrm{sup}(\\\\epsilon\\\\theta)\\\\right),$$\\n\\n(7)\\n\\nwhere g(v) = 블((1-v)log(1-v)+(1+v) log(1 +v)) is the calibrationfunction (Steinwart, 2007; Pires & Szepesv�ri,\\n2016)for the multiclass logistic loss for VE [0,1].\\n\\nFollowing (Kleijn & der Vaart, 2012; Fang & Xie, 2022), KL divergence is assumed to haver the 2nd-order Taylor expansion\\nwith the concept 0. Then, we have the following theorem and proof.\\n\\nTheorem 1. (Fang & Xie, 2022; Zhou et al., 2024) Let the set of0 which does not satisfy Equation 3 in Condition 1 to be B.\\nAssume that KL divergences have a 2nd-order Taylor expansion around 0*:\\n\\n$$\\\\forall j>1,\\\\;\\\\;K L_{i}(\\\\theta^{\\\\star}||\\\\theta)=\\\\frac{1}{2}(\\\\theta-\\\\theta^{\\\\star})^{\\\\top}I_{j,\\\\theta^{\\\\star}}(\\\\theta-\\\\theta^{\\\\star})+O(|\\\\theta-\\\\theta^{\\\\star}||^{3})$$\\n\\n(8)\\n\\nmaxj �max(Ij.0* )\\nwhere Ij,0* is the Fisher information matrix of the j-th token distribution with respect to 0*. Let Y⌀* =\\nmin j�min (Ij,0* )\\nwhere 入max, 入min return the largest and smallest eigenvalues. Thenfor k > 2 and as n → 8, the 0-1 risk of the in-context\\nlearning predictor fn is bounded as\\n\\n$$\\\\operatorname*{lim}_{n\\\\to\\\\infty}L_{o.l}(f_{n})\\\\leq\\\\operatorname*{inf}_{f}L_{o.l}(f)+g^{-1}\\\\left(O\\\\left({\\\\frac{\\\\gamma_{o^{*}}\\\\operatorname{sup}_{\\\\theta\\\\in{\\\\mathrm{B}}}(\\\\epsilon_{\\\\theta}+\\\\xi_{\\\\theta}(r)}{k-1}}\\\\right)\\\\right)$$\\n\\n(9)\\n\\n31ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\\nJia, F., Zhang, Y., and Ginsburg, B. Ruler: What’s the\\nreal context size of your long-context language models?\\nArXiv preprint, abs/2404.06654, 2024. URL https:\\n//arxiv.org/abs/2404.06654.\\n\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adapta-\\ntion of large language models. In The Tenth Interna-\\ntional Conference on Learning Representations, ICLR\\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\\n2022. URL https://openreview.net/forum?\\nid=nZeVKeeFYf9.\\n\\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang,\\nL. Efficient attentions for long document summariza-\\ntion. In Toutanova, K., Rumshisky, A., Zettlemoyer,\\nL., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\\nR., Chakraborty, T., and Zhou, Y. (eds.), Proceedings\\nof the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies, pp. 1419–1436, Online,\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.naacl-main.112. URL https://\\naclanthology.org/2021.naacl-main.112.\\n\\nJacobs, S. A. et al. DeepSpeed Ulysses: System optimiza-\\ntions for enabling training of extreme long sequence\\nTransformer models. ArXiv preprint, abs/2309.14509,\\n2023. URL https://arxiv.org/abs/2309.\\n14509.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\\nT., and Sayed, W. E. Mistral 7b, 2023a. URL https:\\n//arxiv.org/abs/2310.06825.\\n\\nJiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.\\nLLMLingua: Compressing prompts for accelerated in-\\nference of large language models. In Bouamor, H.,\\nPino, J., and Bali, K. (eds.), Proceedings of the 2023\\nConference on Empirical Methods in Natural Language\\nProcessing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n365. URL https://aclanthology.org/2021.\\nnaacl-main.365.\\n\\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\\nT. Active prompting with chain-of-thought for large lan-\\nguage models. In Ku, L.-W., Martins, A., and Srikumar,\\nV . (eds.), Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Volume\\n1: Long Papers), pp. 1330–1350, Bangkok, Thailand,\\nAugust 2024. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2024.acl-long.73. URL https:\\n//aclanthology.org/2024.acl-long.73.\\n\\nFabbri, A., Li, I., She, T., Li, S., and Radev, D. Multi-news:\\nA large-scale multi-document summarization dataset and\\nabstractive hierarchical model. In Korhonen, A., Traum,\\nD., and Màrquez, L. (eds.), Proceedings of the 57th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics, pp. 1074–1084, Florence, Italy, 2019. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/\\nP19-1102. URL https://aclanthology.org/\\nP19-1102.\\n\\nFang, H. and Xie, P. An end-to-end contrastive self-\\nsupervised learning framework for language understand-\\ning. Transactions of the Association for Computational\\nLinguistics, 10:1324–1340, 2022. doi: 10.1162/tacl_\\na_00521. URL https://aclanthology.org/\\n2022.tacl-1.76/.\\n\\nFei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng,\\nL., and Han, W. Extending context window of large\\nlanguage models via semantic compression. In Ku,\\nL.-W., Martins, A., and Srikumar, V. (eds.), Find-\\nings of the Association for Computational Linguistics\\nACL 2024, pp. 5169–5181, Bangkok, Thailand and vir-\\ntual meeting, August 2024. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2024.findings-acl.\\n306. URL https://aclanthology.org/2024.\\nfindings-acl.306.\\n\\nFu, Q., Cho, M., Merth, T., Mehta, S., Rastegari, M., and\\nNajibi, M. LazyLLM: Dynamic token pruning for ef-\\nficient long context LLM inference. In Workshop on\\nEfficient Systems for Foundation Models II @ ICML2024,\\n2024a. URL https://openreview.net/forum?\\nid=gGZD1dsJqZ.\\n\\nFu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the se-\\nquential dependency of llm inference using lookahead\\ndecoding. arXiv preprint arXiv:2402.02057, 2024b.\\n\\nGe, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.\\nModel tells you what to discard: Adaptive kv cache com-\\npression for llms. ArXiv preprint, abs/2310.01801, 2023.\\nURL https://arxiv.org/abs/2310.01801.\\n\\nGliwa, B., Mochol, I., Biesek, M., and Wawer, A. SAM-\\nSum corpus: A human-annotated dialogue dataset for\\nabstractive summarization. In Wang, L., Cheung, J. C. K.,\\nCarenini, G., and Liu, F. (eds.), Proceedings of the 2nd\\nWorkshop on New Frontiers in Summarization, pp. 70–\\n79, Hong Kong, China, 2019. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/D19-5409. URL\\nhttps://aclanthology.org/D19-5409.\\n\\nGuo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. J.\\nLongcoder: A long-range pre-trained language model for\\ncode completion. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Inter-\\nnational Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\\nProceedings of Machine Learning Research, pp. 12098–\\n12107. PMLR, 2023. URL https://proceedings.\\nmlr.press/v202/guo23j.html.\\n\\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\\ncentivizing reasoning capability in llms via reinforcement\\nlearning. arXiv preprint arXiv:2501.12948, 2025.\\n\\nHan, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H.,\\nand Wang, S. LM-infinite: Zero-shot extreme length gen-\\neralization for large language models. In Duh, K., Gomez,\\nH., and Bethard, S. (eds.), Proceedings of the 2024 Con-\\nference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pp. 3991–4008, Mex-\\nico City, Mexico, 2024. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\n2024.naacl-long.222.\\n\\nHe, W., Liu, K., Liu, J., Lyu, Y., Zhao, S., Xiao, X., Liu, Y.,\\nWang, Y., Wu, H., She, Q., Liu, X., Wu, T., and Wang, H.\\nDuReader: a Chinese machine reading comprehension\\ndataset from real-world applications. In Choi, E., Seo, M.,\\nChen, D., Jia, R., and Berant, J. (eds.), Proceedings of the\\nWorkshop on Machine Reading for Question Answering,\\npp. 37–46, Melbourne, Australia, 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-2605.\\nURL https://aclanthology.org/W18-2605.\\n\\nHo, X., Duong Nguyen, A.-K., Sugawara, S., and\\nAizawa, A. Constructing a multi-hop QA dataset\\nfor comprehensive evaluation of reasoning steps. In\\nScott, D., Bel, N., and Zong, C. (eds.), Proceed-\\nings of the 28th International Conference on Compu-\\ntational Linguistics, pp. 6609–6625, Barcelona, Spain\\n(Online), 2020. International Committee on Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.coling-main.\\n580. URL https://aclanthology.org/2020.\\ncoling-main.580.\\n\\n10ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTable 17: Performance comparison of Chinese subtask on LongBench for Qwen2-7B-Instruct.\\n\\n| Method | Single-Document QA | Multi-Document QA | Summarization | Few-shot Learning | Synthetic | Avg. ↑ |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Method | MF-zh | DuReader | VCSum | LSHT | PR-zh | Avg. ↑ |\\n| Avg len | 6,701 | 15,768 | 15,380 | 22,337 | 6,745 | Avg. ↑ |\\n| Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full | Qwen2-7B-Instruct, KV Size = Full |\\n| FullKV | 39.17 | 23.63 | 16.21 | 43.50 | 70.50 | 38.60 |\\n| Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | Qwen2-7B-Instruct, KV Size Compression Ratio = 10% |\\n| StreamingLLM | 38.05 | 23.24 | 15.92 | 40.50 | 44.50 | 32.44 |\\n| H2O | 37.99 | 19.58 | 16.16 | 41.67 | 67.35 | 36.55 |\\n| SnapKV | 44.25 | 20.27 | 16.24 | 44.50 | 68.10 | 38.67 |\\n| PyramidKV | 36.57 | 20.56 | 16.15 | 43.50 | 66.50 | 36.55 |\\n| ChunkKV | 45.92 | 20.15 | 16.37 | 43.75 | 71.10 | 39.45 |\\n\\n\\n# C. Theoretical Understanding\\n\\nIn this section, we provide the theoretical interpretation from the perspective from the In-context learning (ICL) to further\\nunderstand how ChunkKV outperforms token-level KV cache compression.\\n\\nPretraining Data Distribution. Given a set of concepts ④ and a concept 0 E 日, we define the pretraining data is sampled\\nfrom p(01, · .. , oT) = SAEO p(01,... , oT|0)p(0)d0 (Fang & Xie, 2022). Each token 0 is sampled from a vocabulary 0. For\\nsimplicity, we write 01:t = 01 ... Ot.\\n\\nLanguage Modeling. Current LLMs (Brown et al., 2020; Touvron et al., 2023; Fang & Xie, 2022) usually utilize the next\\nword prediction as the language modelling, which predicts the next token Ot given the previous tokens 01 · · 0t-1 for all\\nt = 1, · · , T. Formally, a language modelling can be writen as the distribution f(0t|01:t-1). And it is pretrained on a huge\\ncorpus sampled from the pretraining distribution p(01,... , 0t+1) (Fang & Xie, 2022). Considering the large scale of the\\nmodel size and dataset size, it can be assumed that the f(01 · · · ot+1) has been aligned with the p(01 · · · Ot+1) (Fang & Xie,\\n2022).\\n\\nPrompt Distribution. Following (Fang & Xie, 2022), a prompt is composed of an input token sequence x followed by an\\noutput token y. Then, the i-th training example 1 that can appear in any place in the whole prompt 01:T is defined as Oi\\nconsisting of an input Xi = Oi [1 : k - 1] (the first k - 1 tokens) followed by the output Yi = Oi [k] at the end, where the\\nlength k is fixed for simplicity.\\n\\nThe i-th training example is independently generated as follows: 1) Generate a start hidden state hstart from a prompt\\nstart distribution Pprompt; 2) Given hstart, generate the example sequence Oi = [xi, yi] from p(Oi/hstart, 0*). The test input\\nXtest = Xn+1 is sampled similarly. Then, the prompt consists of a sequence of training examples (Sn) followed by the\\nexample Xtest:\\n\\n$$\\\\left[\\\\S_{n},\\\\Pi_{\\\\mathrm{test}}\\\\right]=\\\\left[\\\\imath_{1},y_{1},\\\\bar{\\\\nu}_{2},\\\\sqrt{\\\\jmath}2,\\\\cdot\\\\cdot\\\\cdot\\\\cdot\\\\cdot\\\\imath\\\\bar{\\\\imath}_{n},\\\\langle\\\\imath,\\\\bar{\\\\imath}_{\\\\mathrm{test}}\\\\right]\\\\sim\\\\bar{\\\\cal P}_{\\\\mathrm{prompt}}\\\\varepsilon_{\\\\mathrm{tot}}\\\\right]$$\\n\\n(2)\\n\\nIn-context learning setups and Assumptions. We follow other settings and assumptions in (Fang & Xie, 2022). With the\\ngreedy decoding (Fu et al., 2024b), sampling the next token from the language modeling f(0t|01:t-1) becomes the predictor\\nas y = arg max⌀t f(0t|01:t-1).\\n\\nThus, for [Sn, Xtest], the in-context learning predictor can be written as fn (xtest) := arg maxy p(y|Sn, Xtest), which outputs\\nthe most likely prediction over the pretraining distribution conditioned on the prompt distribution. Its expected 0-1 error\\n[1[fn(xtest) ≠ ytest]].\\nwith n examples is L0-1 (fn) = Extest;Ytest~Pprompt\\n\\nWe define pa(o) := p(O[i] = 이0[1 : i - 1], 0) of the i-th token with previous tokens and the analogous distribution\\nPiprompt := Pprompt (○[2] = 이0[1 : i - 1]) under the prompt distribution. Following (Fang & Xie, 2022), there is a\\ndistinguishability condition formalizes when in-context learning occurs giving the concept 0.\\n\\nThe distinguishability condition is dependent on a KL divergence between the previous two distributions and the error terms\\n\\n1Here, training example in prompts means happens during the prompt learning.\\n\\n30ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# B.3. Needle-In-A-Haystack\\n\\nFigure 17 and 18 visualizes the performance of ChunkKV on the NIAH benchmark for LLaMA-3-8B-Instruct and Mistral-\\n7B-Instruct with a KV cache size of 128 under 8k and 32k context length. The performance of ChunkKV is consistently\\nbetter as the context length increases.\\n\\n26# ChunkKV: Semantic-Preserving KV Cache Compression for\\nEfficient Long-Context LLM Inference\\n\\nXiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1\\n\\n# Abstract\\n\\nTo reduce memory costs in long-context inference\\nwith Large Language Models (LLMs), many re-\\ncent works focus on compressing the key-value\\n(KV) cache of different tokens. However, we\\nidentify that the previous KV cache compression\\nmethods measure token importance individually,\\nneglecting the dependency between different to-\\nkens in the real-world language characterics. In\\nlight of this, we introduce ChunkKV, grouping the\\ntokens in a chunk as a basic compressing unit, and\\nretaining the most informative semantic chunks\\nwhile discarding the less important ones. Further-\\nmore, observing that ChunkKV exhibits higher\\nsimilarity in the preserved indices across differ-\\nent layers, we propose layer-wise index reuse\\nto further reduce computational overhead. We\\nevaluated ChunkKV on cutting-edge long-context\\nbenchmarks including LongBench and Needle-\\nIn-A-HayStack, as well as the GSM8K and Jail-\\nbreakV in-context learning benchmark. Our ex-\\nperiments with instruction tuning and multi-step\\nreasoning (O1 and R1) LLMs, achieve up to 10%\\nperformance improvement under aggressive com-\\npression ratios compared to existing methods.\\n\\n# 1. Introduction\\n\\n2025\\nFeb\\n1\\n[cs.CL]\\narXiv:2502.00299v1\\n\\nLarge Language Models (LLMs) have become essential for\\naddressing various downstream tasks of natural language\\nprocessing (NLP), including summarization and question\\nanswering, which require the interpretation of a long con-\\ntext from sources such as books, reports, and documents,\\noften encompassing tens of thousands of tokens (Brown\\net al., 2020; Tay et al., 2022; Touvron et al., 2023). Re-\\ncent advances in long-context technology within the field of\\n\\n1The Hong Kong University of Science and Technol-\\nogy(Guangzhou), Guangzhou, China 2The Hong Kong University\\nof Science and Technology, Hong Kong, China. Correspondence\\nto: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu\\n<xwchu@hkust-gz.edu.cn>.\\n\\nmachine learning (ML) systems (Dao, 2024; Jacobs et al.,\\n2023; Xiao et al., 2024) have significantly enhanced com-\\nputational throughputs and reduced latency of LLMs to\\nprocess increasingly large input context lengths (Liu et al.,\\n2024b; Young et al., 2024) with saving historical KV cache\\n(key value attentions). However, the memory requirement\\nof the KV cache in serving super-long contexts becomes a\\nnew bottlneck (Zhang et al., 2023; Reid et al., 2024). For\\ninstance, the KV cache for a single token in a 7B-parameter\\nmodel requires approximately 0.5 MB of GPU memory, re-\\nsulting in a 10,000-token prompt consuming around 5 GB\\nof GPU memory.\\n\\nTo address the substantial GPU memory consumption\\ncaused by KV caching, recent studies consider compressing\\nthe KV cache by pruning non-important discrete parts from\\nthe prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge\\net al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al.,\\n2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang\\net al., 2023) and SnapKV (Li et al., 2024) have shown that\\nretaining less than 50% of the discrete KV cache can signif-\\nicantly reduce GPU memory usage with minimal impact on\\nperformance. However, we identify that the previous KV\\ncache compression methods (Zhang et al., 2023; Cai et al.,\\n2024) measure token importance isolatedly, neglecting the\\ndependency between different tokens in the real-world lan-\\nguage characterics. For example, as shown in Figure 1,\\nfocusing on token-level importance might excessively fo-\\ncus on words about subjects “turaco” in the question while\\nomitting crucial information about the objects (foods) in the\\ndocuments, resulting the loss of essential semantic informa-\\ntion. This motivates us to rethink the following question:\\n\\nHow to avoid isolated token importance measurement and\\npreserve the semantic information in KV cache?\\n\\nIn light of this, we observe that the complete semantic in-\\nformation usually appear in a continuous sequence (Fang &\\nXie, 2022). Thus, we introduce a straightforward yet effec-\\ntive ChunkKV, grouping the tokens in a chunk as a basic\\ncompressing unit, which should be preserved or discarded\\nas a whole. Thus, it retains the most informative semantic\\nchunks from the original KV cache. As shown in Figure 1,\\npreserving a chunk helps to catch the subject, predicate,\\nand object. Furthermore, we investigate that the preserved\\n\\n1ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# D. Additional Related Work\\n\\nKV cache sharing Recent work has explored various strategies for sharing KV caches across transformer layers. Layer-\\nCondensed KV Cache (LCKV) (Wu & Tu, 2024) computes KVs only for the top layer and pairs them with queries from all\\nlayers, while optionally retaining standard attention for a few top and bottom layers to mitigate performance degradation.\\nSimilarly, You Only Cache Once (YOCO) (Sun et al., 2024) computes KVs exclusively for the top layer but pairs them with\\nqueries from only the top half of layers, employing efficient attention in the bottom layers to maintain a constant cache\\nsize. In contrast, Cross-Layer Attention (CLA) (Brandon et al., 2024) divides layers into groups, pairing queries from all\\nlayers in each group with KVs from that group’s bottom layer. MiniCache (Liu et al., 2024a) introduces a novel method\\nthat merges layer-wise KV caches while enabling recovery during compute-in-place operations, optimizing KV cache size.\\nThese methods illustrate various trade-offs between computation, memory usage, and model performance when sharing KV\\ncaches across transformer layers.\\n\\nLong-Context Benchmarks The landscape of long-context model benchmarks has evolved to encompass a wide range\\nof tasks, with particular emphasis on retrieval and comprehension capabilities. Benchmarks for understanding have made\\nsignificant strides, with ∞-Bench (Zhang et al., 2024) pushing the boundaries by presenting challenges that involve more\\nthan 100,000 tokens. LongBench (Bai et al., 2024) has introduced bilingual evaluations, addressing tasks such as long-\\ndocument question answering, summarization, and code completion. Complementing these efforts, ZeroSCROLLS (Shaham\\net al., 2023) and L-Eval (An et al., 2023) have broadened the scope to include a diverse array of practical natural language\\ntasks, including query-driven summarization.\\n\\nIn parallel, retrieval benchmarks have largely relied on synthetic datasets, offering researchers precise control over variables\\nsuch as the length of input tokens. This approach minimizes the impact of disparate parametric knowledge resulting from\\nvaried training methodologies. A significant body of recent work has concentrated on the development of synthetic tasks\\nspecifically for retrieval evaluation (Kamradt, 2023; Mohtashami & Jaggi, 2023; Li et al., 2023; Liu et al., 2024c; Hsieh\\net al., 2024). In addition, researchers have explored the potential of extended contexts in facilitating various forms of\\nreasoning (Tay et al., 2021).\\n\\nThis dual focus on synthetic retrieval tasks and comprehensive understanding benchmarks reflects the field’s commitment to\\nrigorously assessing the capabilities of long-context models across diverse linguistic challenges. Prompting Compression\\nIn the field of prompt compression, various designs effectively combine semantic information to compress natural language.\\nWingate et al. (2022) utilize soft prompts to encode more information with fewer tokens. Chevalier et al. (2023) present\\nAutoCompressor, which uses soft prompts to compress the input sequence and extend the original length of the base\\nmodel. Both Zhou et al. (2023) and Wang et al. (2023) recurrently apply LLMs to summarize input texts, maintaining long\\nshort-term memory for specific purposes such as story writing and dialogue generation. The LLMLingua series (Jiang et al.,\\n2023b; 2024; Fei et al., 2024) explores the potential of compressing LLM prompts in long-context, reasoning, and RAG\\nscenarios. Fei et al. (2024) use pre-trained language models to chunk the long context and summarize semantic information,\\ncompressing the original context.\\n\\n# E. Statistics of Models\\n\\nTable 18 provides configuration parameters for LLMs that we evaluated in our experiments.\\n\\n| Model Name | LLaMA-3-8B-Instruct | Mistral-7B-Instruct-v0.2 & 0.3 | Qwen2-7B-Instruct |\\n| --- | --- | --- | --- |\\n| L (Number of layers) | 32 | 32 | 28 |\\n| N (Number of attention heads) | 32 | 32 | 28 |\\n| D (Dimension of each head) | 128 | 128 | 128 |\\n\\n\\nTable 18: Models Configuration Parameters\\n\\n33ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTable 13: Reusing Indexing Performance Comparison on GSM8K\\n\\n| Model | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Model | 1 | 2 | 3 | 5 | 8 | 10 | 20 | 28/32 |\\n| LLaMA-3-8B-Instruct | 74.5 | 74.6 | 65.9 | 44.1 | 15.3 | 2.20 | 1.60 | 1.80 |\\n| Qwen2-7B-Instruct | 71.2 | 71.2 | 73.0 | 69.4 | 67.4 | 71.1 | 54.0 | 49.4 |\\n\\n\\nB.2. LongBench\\n\\nThe Table 14 shows the average performance of KV cache compression methods in the LongBench English subtask\\ncategories. The ChunkKV achieves the best performance on the overall average, and the Multi-Document QA category,\\nwhich supports that chunk method is more effective for semantic preservation.\\n\\nTable 14: Comprehensive performance comparison of KV cache compression methods across LongBench English subtasks.\\nResults are shown for various models and tasks, highlighting the effectiveness of different compression techniques.\\n\\n| Method | Single-Document QA | Single-Document QA | Single-Document QA | Multi-Document QA A | Multi-Document QA A | Multi-Document QA A | Summarization | Summarization | Summarization | Few-shot Learning | Few-shot Learning | Few-shot Learning | Synthetic Code m n t L c c B P R e R u | Synthetic Code m n t L c c B P R e R u | Synthetic Code m n t L c c B P R e R u | P - Avg. ↑ |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Method | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | P - Avg. ↑ |\\n| Avg len | 18,409 | 3,619 36.82 | 4,559 | 9,151 21.42 14.25 | 4,887 | 11,214 | 8,734 | 10,614 5.00 | 2,113 | 5,177 76.00 | 8,209 | 6,258 | C o 11,141 9,289 | 1,235 26.53 | 4,206 | P - Avg. ↑ |\\n| LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full | LlaMa-3-8B-Instruct, KV Size = Full |\\n| FullKV | 25.70 | 29.75 88.73 | 41.12 | 45.55 49.93 | 35.87 | 22.35 | 25.63 89.22 44.78 | 23.03 | 26.21 | 73.00 | 90.56 | 41.88 | 4.67 | 69.25 58.05 | 50.77 | 41.46 32.28 22.60 25.45 40.31 22.87 25.91 61.32 40.88 |\\n| LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 10% |\\n| StreamingLLM | 20.62 | 13.09 | 22.10 | 36.31 | 28.01 | 15.61 7.00 73.50 60.91 | 21.47 | 21.05 19.39 | 62.00 | 84.18 | 40.27 | 4.62 | 69.10 | 58.84 | 55.26 PyramidKV 61.24 | 35.74 |\\n| H2O | 24.80 | 17.32 | 31.80 | 40.84 | 33.28 | 18.90 | 22.29 | 22.29 | 21.82 40.00 | 90.51 | 40.55 | 5.79 | 69.50 | 58.04 | 55.26 60.91 27.51 | 37.06 |\\n| SnapKV | 25.08 | 22.02 | 37.95 | 43.36 | 35.08 | 20.29 | 22.94 | 22.64 | 21.37 | 71.00 90.47 | 40.15 | 5.66 | 69.25 | 58.69 | 56.50 | 40.15 |\\n| PyramidKV | 25.58 | 20.77 8.72 32.79 | 35.85 29.01 | 43.80 | 33.03 | 21.45 | 23.68 | 22.26 | 21.85 | 71.50 90.47 | 41.66 | 5.84 | 69.25 | 58.52 46.49 | 55.91 | 40.08 |\\n| ChunkKV | 24.89 | 22.96 | 37.64 | 43.27 | 36.45 | 20.65 | 22.80 | 22.97 | 20.82 | 71.50 90.52 | 40.83 | 5.93 4.00 | 69.00 98.50 | 60.49 | 57.48 | 40.51 |\\n| LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 20% |\\n| StreamingLLM ChunkKV | 23.35 29.75 | 18.97 | 32.94 | 42.39 | 29.37 38.67 | 18.76 28.63 ChunkKV | 25.78 | 21.92 60.03 | 25.16 | 71.00 88.85 | 40.82 | 5.04 | 69.00 98.00 | 56.46 59.98 | 51.12 | 38.80 |\\n| H2O | 25.60 | 21.88 26.17 | 35.36 | 42.06 | 32.68 38.72 Qwen2-7B-Instruct, | 19.72 | 23.54 | 22.77 KV Size = | 22.72 45.50 | 90.57 | 41.67 60.88 | 5.51 | 69.25 89.15 | 54.97 | 50.95 52.96 | 37.79 |\\n| SnapKV | 25.50 | 25.95 42.64 | 38.43 | 44.12 | 35.38 6.75 | 20.49 | 24.85 36.38 | 23.36 23.51 23.43 | 72.50 45.04 | 90.52 | 40.91 44.88 | 5.23 | 69.25 FullKV 44.33 42.54 22.69 89.44 40.45 | 56.74 12.81 33.24 44.32 8.00 62.39 | 51.75 61.84 | 40.53 40.71 |\\n| PyramidKV | 25.36 | 26.88 | 37.99 44.29 | 44.21 | 35.65 13.22 | 21.43 | 25.52 | 23.43 | 23.47 72.00 | 90.56 77.00 89.99 | 41.45 | 5.26 | 69.50 25.94 76.50 | 56.55 | 50.93 22.77 | 40.63 |\\n| ChunkKV | 26.13 | 28.43 | 38.59 | 44.46 58.54 | 34.13 | 21.06 26.84 45.96 25.83 40.55 | 24.72 | 23.11 | 22.91 | 71.50 90.56 | 41.51 | 5.09 | 69.00 75.92 | 58.17 | 52.51 | 40.74 50.33 21.49 38.56 |\\n| LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% | LlaMa-3-8B-Instruct, KV Size Compression Ratio = 30% Qwen2-7B-Instruct, KV Size Compression Ratio = 10% |\\n| StreamingLLM | 24.49 | 22.53 | 35.30 41.46 13.66 | 44.33 11.95 26.24 77.50 | 32.81 | 19.00 | 27.12 13.13 | 22.19 | 25.93 72.50 | 89.84 | 41.75 44.54 7.50 | 5.41 50.50 | 69.00 | 60.40 | 55.13 | 40.48 |\\n| H2O H2O | 25.87 | 23.03 | 37.06 9.08 12.46 76.00 61.28 10.52 32.38 8.50 76.50 60.64 | 43.71 74.00 90.02 | 33.68 | 20.93 | 24.56 | 23.14 | 23.58 50.50 | 90.77 | 41.96 SnapKV | 4.91 | 69.25 | 59.38 | 55.39 | 39.23 |\\n| SnapKV | 25.15 | 28.75 | 39.28 | 43.57 27.03 24.76 | 36.16 | 21.58 | 25.56 | 23.19 | 24.30 | 73.00 | 90.52 41.70 45.79 14.27 13.35 32.62 22.70 89.19 44.71 7.50 71.50 | 4.96 | 69.25 25.15 45.42 | 60.27 | 55.74 | 41.43 |\\n| PyramidKV | 25.42 | 27.91 25.11 | 38.81 | 44.15 61.47 | 36.28 | 21.72 | 26.50 | 23.10 | 24.28 | 72.00 90.56 | 41.87 | 4.67 | 69.50 | 60.09 | 55.19 | 41.37 |\\n| ChunkKV | 25.88 | 29.58 | 38.99 60.17 | 43.94 | 34.16 | 21.70 | 26.50 | 23.15 | 23.95 | 72.00 90.56 | 42.47 | 5.34 | 69.25 | 61.68 | 56.35 | 41.59 |\\n| Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full | Mistral-7B-Instruct-v0.3, KV Size = Full |\\n| FullKV | 29.07 | 41.58 9.15 9.91 60.35 61.37 | 52.88 StreamingLLM | 49.37 | 39.01 | 28.58 9.12 | 34.93 | 25.68 | 27.74 76.00 | 88.59 77.50 | 47.59 | 6.00 | 98.50 | 61.41 | 62.39 | 48.08 |\\n| Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 | Mistral-7B-Instruct-v0.3, KV Size Compression Ratio = 10% Full 44.45 43.59 13.35 89.44 44.53 |\\n| StreamingLLM | 25.15 | 25.47 | 30.08 | 44.39 | 32.49 46.39 | 19.40 | 24.11 | 20.85 | 19.55 | 65.00 88.21 | 44.83 27.59 24.99 | 4.50 | 79.50 77.00 | 59.48 | 58.82 46.07 | 40.11 |\\n| H2O | 29.35 26.48 44.19 | 33.39 | 50.39 | 49.58 50.15 | 36.76 38.17 46.71 | 27.42 27.99 | 25.16 26.67 | 24.75 25.21 | 22.12 22.33 | 42.00 72.00 | 89.00 47.04 45.44 | 5.50 5.50 | 98.50 99.00 77.00 | 57.58 59.79 | 59.24 61.63 53.99 | 43.61 46.38 |\\n| SnapKV PyramidKV | 28.54 29.40 15.94 12.60 | 36.88 53.42 89.36 35.39 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |\\n\\n\\n25ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nQuestion: purple-crested turaco eats what food?\\n\\n| Discrete KV methods: 𝑆\" = 𝑓(𝑡\") | ChunkKV: 𝑆! = \\t ∑ #\" $% 𝑓 𝑡\" , \\twhere\\t𝑐\\t = \\t {𝑡%, … , 𝑡#} |\\n| --- | --- |\\n| Discrete KV methods with a low sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… Bird …… …… eat …… …… …… turacos, …… …… …… …… pulp …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… water …… …… …… …… …… …… …… leaves …… …… …… …… …… …… …… …… nuts …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a low sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… |\\n| Discrete KV methods with a high sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… …… …… eat …… …… …… turacos, …… …… …… …… …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a high sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… ………… …… …… …… |\\n\\n\\nFigure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The\\ndiscrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the\\nsubject of the words, maintaining more accurate semantic information. For the equation: S is the score function, and c is a\\nchunk of tokens.\\n\\nTable 1: Comparison of Methods on KV Cache Compression.\\n\\n| Method | KV Cache Compression | Dynamic Policy | Layer-Wise Policy | Semantic Information | Efficient Index Reuse |\\n| --- | --- | --- | --- | --- | --- |\\n| StreamingLLM (Xiao et al., 2024) | ✓ |  |  |  |  |\\n| H2O (Zhang et al., 2023) | ✓ | ✓ |  |  |  |\\n| SnapKV (Li et al., 2024) | ✓ | ✓ |  |  |  |\\n| PyramidInfer (Yang et al., 2024b) | ✓ | ✓ | ✓ |  |  |\\n| PyramidKV (Cai et al., 2024) | ✓ | ✓ | ✓ |  |  |\\n| ChunkKV(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |\\n\\n\\nKV cache indices by ChunkKV exhibit a higher similarity\\ncompared to previous methods. Consequently, we develop a\\ntechnique called layer-wise index reuse, which reduces the\\nadditional computational time introduced by the KV cache\\ncompression method. As outlined in Table 1, recent highly\\nrelevant KV cache compression methods lack the ability to\\nretain semantic information and efficiently reuse indices.\\n\\nTo evaluate ChunkKV’s performance, we conduct com-\\nprehensive experiments across multiple cutting-edge long-\\ncontext benchmarks: long-context tasks including Long-\\nBench (Bai et al., 2024) and Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023), in-context learning tasks such as\\nGSM8K (Cobbe et al., 2021) and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# B.4. Chunk Size\\n\\nTable 15 shows the performance of ChunkKV with different chunk size on the LongBench benchmark.\\n\\nTable 15: LongBench Performance Comparison with different chunk sizes\\n\\n| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| Model | 3 | 5 | 10 | 20 | 30 | Full KV |\\n| LLaMA-3-8B-Instruct | 40.49 | 40.47 | 40.51 | 40.05 | 39.57 | 41.46 |\\n| Mistral-7B-Instruct | 46.45 | 46.51 | 46.71 | 46.42 | 45.98 | 48.08 |\\n| Qwen2-7B-Instruct | 40.38 | 40.33 | 40.66 | 40.88 | 40.73 | 40.71 |\\n\\n\\nTable 16 shows the performance of ChunkKV with different chunk size on the GSM8K benchmark. Figure 19 shows that\\nthe ChunkKV with different chunk sizes on GSM8K displays the same curve pattern as LongBench. The CoT prompt length\\nfor GSM8K is only 1K tokens, so the optimal chunk size range is smaller.\\n\\n![image](/image/placeholder)\\n- Chart Title: GSM8K Performance vs Chunk Size\\n- X-Axis: Chunk Size\\n- Y-Axis: GSM8K Score\\n- Chart Type: line\\n|  | 3 | 5 | 10 | 20 |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 75.0 | 72.5 | 72.5 | 72.0 |\\n\\n\\nFigure 19: GSM8K Performance Comparison with different chunk size\\n\\nTable 16: GSM8K Performance Comparison with different chunk sizes\\n\\n| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\\n| --- | --- | --- | --- | --- | --- |\\n| Model | 3 | 5 | 10 | 20 | Full KV |\\n| LLaMA-3-8B-Instruct | 74.6 | 74.5 | 73.9 | 63.2 | 76.8 |\\n| Qwen2-7B-Instruct | 73.5 | 71.2 | 71.8 | 71.7 | 71.1 |\\n\\n\\n# B.5. Multi-Lingual\\n\\nTable 17 is the Chinese support model Qwen2-7B-Instruct evaluated on the LongBench Chinese subtask, where ChunkKV\\nachieves better performance than other compression methods and the full KV cache performance. Both the English and\\nChinese results indicate that ChunkKV is a promising approach for maintaining crucial information in the KV cache.\\n\\n29ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundation and fine-\\ntuned chat models. ArXiv preprint, abs/2307.09288, 2023.\\nURL https://arxiv.org/abs/2307.09288.\\n\\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,\\nA. MuSiQue: Multihop questions via single-hop ques-\\ntion composition. Transactions of the Association for\\nComputational Linguistics, 10:539–554, 2022. doi: 10.\\n1162/tacl_a_00475. URL https://aclanthology.\\norg/2022.tacl-1.31.\\n\\nWang, Q., Ding, L., Cao, Y., Tian, Z., Wang, S., Tao, D., and\\nGuo, L. Recursively summarizing enables long-term dia-\\nlogue memory in large language models. arXiv preprint\\narXiv:2308.15022, 2023.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\\nelicits reasoning in large language models. Advances in\\nneural information processing systems, 35:24824–24837,\\n2022.\\n\\nWingate, D., Shoeybi, M., and Sorensen, T. Prompt com-\\npression and contrastive conditioning for controllability\\nand toxicity reduction in language models. In Findings of\\nthe Association for Computational Linguistics: EMNLP\\n2022, pp. 5621–5634, Abu Dhabi, United Arab Emi-\\nrates, December 2022. Association for Computational\\nLinguistics. doi: 10.18653/v1/2022.findings-emnlp.\\n412. URL https://aclanthology.org/2022.\\nfindings-emnlp.412.\\n\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024. URL https:\\n//arxiv.org/abs/2405.10637.\\n\\nWu, H., Zhan, M., Tan, H., Hou, Z., Liang, D., and\\nSong, L. VCSUM: A versatile Chinese meeting sum-\\nmarization dataset. In Rogers, A., Boyd-Graber, J.,\\nand Okazaki, N. (eds.), Findings of the Association\\nfor Computational Linguistics: ACL 2023, pp. 6065–\\n6079, Toronto, Canada, 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.\\n377. URL https://aclanthology.org/2023.\\nfindings-acl.377.\\n\\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\\nficient streaming language models with attention sinks.\\nIn The Twelfth International Conference on Learning\\nRepresentations, 2024. URL https://openreview.\\nnet/forum?id=NG7sS51zVF.\\n\\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C.,\\nLi, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical\\nreport. ArXiv preprint, abs/2407.10671, 2024a. URL\\nhttps://arxiv.org/abs/2407.10671.\\n\\nYang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and\\nZhao, H. PyramidInfer: Pyramid KV cache com-\\npression for high-throughput LLM inference. In Ku,\\nL.-W., Martins, A., and Srikumar, V. (eds.), Find-\\nings of the Association for Computational Linguis-\\ntics ACL 2024, pp.'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Databased based Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import chromadb\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "embedding_context_length = 4000\n",
    "\n",
    "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
    "    def __init__(\n",
    "        self,\n",
    "        client,\n",
    "        model_name: str = \"embedding-query\",\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        if not all(isinstance(item, str) for item in input):\n",
    "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
    "\n",
    "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
    "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
    "        return np.array(passage_embedding_list, dtype=np.float32)\n",
    "\n",
    "embedding_fn = UpstageEmbeddingFunction(client)\n",
    "\n",
    "def get_md_with_document_parse(root_path, paper_url, paper_id):\n",
    "    response = requests.get(paper_url)\n",
    "    # Save the PDF to a temporary file\n",
    "    \n",
    "    pdf_path = f\"{root_path}/paper.pdf\"\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "            \n",
    "    split_factor = 1\n",
    "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
    "\n",
    "    markdown = \"\"\n",
    "    total_responses = []\n",
    "    for i, split_pdf in enumerate(split_pdfs):\n",
    "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
    "        \n",
    "        # Append the response to the total_responses list\n",
    "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})        \n",
    "        # Also write the response to a JSON file for persistence\n",
    "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
    "        with open(json_output_path, \"w\") as json_file:\n",
    "            json.dump(upstage_response, json_file, indent=2)\n",
    "\n",
    "        try:\n",
    "            markdown += upstage_response['content']['markdown']\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    collection = chroma_client.create_collection(name=paper_id, embedding_function=embedding_fn)\n",
    "\n",
    "    processed_input = []\n",
    "    if len(markdown) > embedding_context_length:\n",
    "        chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
    "        processed_input.extend(chunks)\n",
    "    else:\n",
    "        processed_input.append(markdown)\n",
    "    \n",
    "    ids = []\n",
    "    for i in range(len(processed_input)):\n",
    "        ids.append(f\"{paper_id}_{i}\")\n",
    "        \n",
    "    collection.add(documents=processed_input, ids=ids)\n",
    "    return collection\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    paper_id = paper_url.split(\"/\")[-1]\n",
    "    root_path = paper_id\n",
    "\n",
    "    if os.path.exists(root_path):\n",
    "        print(f\"Found cached markdown for {paper_id}\")\n",
    "        return f\"we already have the paper content stored in our database in the id of {paper_id}\"\n",
    "        # chunks = get_md_from_fs(paper_id)\n",
    "    else:\n",
    "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
    "        os.makedirs(root_path, exist_ok=True)\n",
    "        collection = get_md_with_document_parse(root_path, paper_url, paper_id)\n",
    "        return f\"we have parsed the paper content and stored in our database in the id of {paper_id}\"\n",
    "    \n",
    "def to_retrive_paper_content_to_answer_question_agent(question: str, paper_id: str):\n",
    "    \"\"\"Use this to answer question about the paper.\"\"\"\n",
    "    collection = chroma_client.get_collection(name=paper_id, embedding_function=embedding_fn)\n",
    "    results = collection.query(query_texts=[question], n_results=10)\n",
    "    results_str = [\"Retrieved Paper Content\\n-----------------------------------\\n\"]\n",
    "    for i in range(len(results['documents'])):\n",
    "        results_str.append(f\"{i}: {results['documents'][i]}\")\n",
    "    return \"\\n\".join(results_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent, to_retrive_paper_content_to_answer_question_agent]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='4b81dad7-1328-4d99-9505-e5e40a64bb77', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='3a37b7a0-4a91-4f09-833a-61e30f68fd67', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "Found cached markdown for 2502.00299\n",
      "[ChatCompletionMessageToolCall(id='5703eec3-13bc-48fc-88db-e88418a7e166', function=Function(arguments='{\"paper_id\":\"2502.00299\",\"question\":\"Provide a comprehensive summary of the paper.\"}', name='to_retrive_paper_content_to_answer_question_agent'), type='function')]\n",
      "Supervisor Agent: to_retrive_paper_content_to_answer_question_agent({'paper_id': '2502.00299', 'question': 'Provide a comprehensive summary of the paper.'})\n",
      "--------------------------------\n",
      "Based on our analysis, the paper titled 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' introduces a new method called ChunkKV for compressing the key-value (KV) cache of different tokens in large language models (LLMs). This method groups tokens into chunks as a basic compressing unit and retains the most informative semantic chunks while discarding less important ones. The paper also proposes a layer-wise index reuse technique to further reduce computational overhead. The authors evaluated ChunkKV on cutting-edge long-context benchmarks, achieving up to 10% performance improvement under aggressive compression ratios compared to existing methods.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    \n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
