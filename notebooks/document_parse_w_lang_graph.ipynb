{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVVYXCpg3x9a9DauaDyjh2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/agentic-system/blob/main/notebooks/document_parse_w_lang_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vbA6tJoU5Hu"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph\n",
        "!pip install openai\n",
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yofEdegIXGVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = \"YOUR_API_KEY\""
      ],
      "metadata": {
        "id": "Y0F7q1TcW1M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_upstage import ChatUpstage\n",
        "\n",
        "tools = []\n",
        "\n",
        "llm = ChatUpstage()\n",
        "llm = llm.bind_tools(\n",
        "    [\n",
        "        to_paper_search_agent,\n",
        "        to_download_and_parse_paper_agent,\n",
        "        to_retrive_paper_content_to_answer_question_agent\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "DHr7nluEW5g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# from state import State\n",
        "# from graph.nodes import chatbot, summarizer, summary_grader # test_edge, test_edge2\n",
        "# from graph.edges import summary_grader_edge\n",
        "\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    found: bool\n",
        "\n",
        "def search_cached_paper_edge(state: State):\n",
        "    if state[\"found\"] is None:\n",
        "        print(\"---Paper Not Found on ChromaDB---\")\n",
        "        return \"qualified\"\n",
        "    else:\n",
        "        print(\"---Paper Found on ChromaDB---\")\n",
        "        return \"qualified\"\n",
        "\n",
        "def chatbot(state: State):\n",
        "    msg = llm.invoke(state[\"messages\"])\n",
        "    if \"new_summary\" in state:\n",
        "        print(\"---chatbot(NEW SUMMARY FOUND)---\")\n",
        "        return {\"messages\": [msg], \"summary\": [state[\"new_summary\"]]}\n",
        "    else:\n",
        "        print(\"---chatbot(NO NEW SUMMARY FOUND)---\")\n",
        "        return {\"messages\": [msg]}\n",
        "\n",
        "def compile_graph(memory: MemorySaver):\n",
        "    graph_builder = StateGraph(State)\n",
        "\n",
        "    graph_builder.add_node(\"chatbot\", chatbot)\n",
        "    graph_builder.add_edge(\"summarizer\", \"summary_grader\")\n",
        "    graph_builder.add_conditional_edges(\n",
        "        \"summary_grader\",\n",
        "        search_cached_paper_edge,\n",
        "        {\n",
        "            \"qualified\": END,\n",
        "            \"unqualified\": \"summary_grader\"\n",
        "        }\n",
        "    )\n",
        "    graph_builder.set_entry_point(\"chatbot\")\n",
        "    return graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "TRlQsJQsVv2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "from graph import compile_graph\n",
        "\n",
        "def stream_graph_updates(graph: StateGraph, user_input: str, config: dict):\n",
        "    messages = [{\"role\": \"user\", \"content\": user_input}]\n",
        "\n",
        "    for event in graph.stream({\"messages\": messages}, config):\n",
        "        for value in event.values():\n",
        "            print(value)\n",
        "            # if \"summary\" in value and len(value[\"summary\"]) > 0:\n",
        "            #     # print(value[\"summary\"])\n",
        "            #     print(\"Summary:\\n\", value[\"summary\"][0])\n",
        "            # elif \"scores\" in value:\n",
        "            #     print(\"Scores:\\n\", value[\"scores\"])\n",
        "            # print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "def main():\n",
        "    memory = MemorySaver()\n",
        "    graph = compile_graph(memory)\n",
        "    config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            user_input = input(\"User: \")\n",
        "            while not user_input.strip():\n",
        "                user_input = input(\"User: \")\n",
        "\n",
        "            if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                print(\"Goodbye!\")\n",
        "                break\n",
        "\n",
        "            stream_graph_updates(graph, user_input, config)\n",
        "        except Exception as e:\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "VleN4GIPVDt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FdV8q4RRXHcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyjKqhlZXHe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7o388cq6XHhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNcJAlwcXHjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FTRx9sUQXHla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import chromadb\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "embedding_context_length = 4000\n",
        "\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model_name: str = \"embedding-query\",\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
        "\n",
        "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
        "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
        "        return np.array(passage_embedding_list, dtype=np.float32)\n",
        "\n",
        "embedding_fn = UpstageEmbeddingFunction(client)\n",
        "\n",
        "def get_md_with_document_parse(root_path, paper_url, paper_id):\n",
        "    response = requests.get(paper_url)\n",
        "    # Save the PDF to a temporary file\n",
        "\n",
        "    pdf_path = f\"{root_path}/paper.pdf\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    split_factor = 1\n",
        "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
        "\n",
        "    markdown = \"\"\n",
        "    total_responses = []\n",
        "    for i, split_pdf in enumerate(split_pdfs):\n",
        "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
        "\n",
        "        # Append the response to the total_responses list\n",
        "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})\n",
        "        # Also write the response to a JSON file for persistence\n",
        "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
        "        with open(json_output_path, \"w\") as json_file:\n",
        "            json.dump(upstage_response, json_file, indent=2)\n",
        "\n",
        "        try:\n",
        "            markdown += upstage_response['content']['markdown']\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    collection = chroma_client.create_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "\n",
        "    processed_input = []\n",
        "    if len(markdown) > embedding_context_length:\n",
        "        chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
        "        processed_input.extend(chunks)\n",
        "    else:\n",
        "        processed_input.append(markdown)\n",
        "\n",
        "    ids = []\n",
        "    for i in range(len(processed_input)):\n",
        "        ids.append(f\"{paper_id}_{i}\")\n",
        "\n",
        "    collection.add(documents=processed_input, ids=ids)\n",
        "    return collection\n",
        "\n",
        "def to_download_and_parse_paper_agent(paper_url: str):\n",
        "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
        "    paper_id = paper_url.split(\"/\")[-1]\n",
        "    root_path = paper_id\n",
        "\n",
        "    if os.path.exists(root_path):\n",
        "        print(f\"Found cached markdown for {paper_id}\")\n",
        "        return f\"we already have the paper content stored in our database in the id of {paper_id}\"\n",
        "        # chunks = get_md_from_fs(paper_id)\n",
        "    else:\n",
        "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "        os.makedirs(root_path, exist_ok=True)\n",
        "        collection = get_md_with_document_parse(root_path, paper_url, paper_id)\n",
        "        return f\"we have parsed the paper content and stored in our database in the id of {paper_id}\"\n",
        "\n",
        "def to_retrive_paper_content_to_answer_question_agent(question: str, paper_id: str):\n",
        "    \"\"\"Use this to answer question about the paper.\"\"\"\n",
        "    collection = chroma_client.get_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "    results = collection.query(query_texts=[question], n_results=10)\n",
        "    results_str = [\"Retrieved Paper Content\\n-----------------------------------\\n\"]\n",
        "    for i in range(len(results['documents'])):\n",
        "        results_str.append(f\"{i}: {results['documents'][i]}\")\n",
        "    return \"\\n\".join(results_str)\n"
      ],
      "metadata": {
        "id": "3zcU8y0JXHnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}