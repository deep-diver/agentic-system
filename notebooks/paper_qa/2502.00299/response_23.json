{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\n![image](/image/placeholder)\n- Chart Title: Owen2-7B-Instruct with H2o\n- Chart Type: line\n|  | top_left | top_right | bottom_left | bottom_right |\n| --- | --- | --- | --- | --- |\n| item_01 | 1 | 2 | 3 | 7 |\n\n\nFigure 13: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on Qwen2-7B-Instruct\n\n![image](/image/placeholder)\nQwen2-7B-Instruct with SnapKV \n0 1.00 0.13 0.24 0.22 0.13 0.23 0.20 0.11 0.15 0.16 0.17 0.20 0.12 0.18 0.06 0.17 0.11 0.16 0.12 0.00 0.14 0.10 0.14 0.10 0.18 0.17 0.10 0.15 \n1 0.13 1.00 0.12 0.13 0.14 0.13 0.13 0.12 0.10 0.17 0.12 0.14 0.10 0.13 0.07 0.11 0.12 0.11 0.10 0.16 0.10 0.11 0.18 0.10 0.09 0.13 0.10 0.16 \n1.0 \n2 0.24 0.12 1.00 0.27 0.16 0.27 0.22 0.12 0.22 0.16 0.21 0.22 0.13 0.15 0.07 0.16 0.12 0.18 0.14 0.00 0.15 0.10 0.15 0.11 0.24 0.15 0.09 0.17 \n3 0.22 0.13 0.27 1.00 0.20 0.30 0.25 0.17 0.20 0.20 0.23 0.27 0.16 0.17 0.07 0.20 0.12 0.19 0.16 0.01 0.22 0.12 0.21 0.11 0.17 0.18 0.11 0.21 \n4 0.13 0.14 0.16 0.20 1.00 0.17 0.14 0.10 0.15 0.18 0.19 0.16 0.14 0.15 0.09 0.14 0.15 0.17 0.14 0.11 0.13 0.16 0.19 0.16 0.11 0.14 0.16 0.22 \n5 0.23 0.13 0.27 0.30 0.17 1.00 0.21 0.16 0.24 0.15 0.23 0.33 0.12 0.15 0.07 0.21 0.12 0.20 0.15 0.00 0.18 0.11 0.19 0.09 0.18 0.16 0.11 0.18 \n6 0.20 0.13 0.22 0.25 0.14 0.21 1.00 0.15 0.15 0.15 0.16 0.21 0.11 0.15 0.05 0.15 0.09 0.14 0.15 0.00 0.15 0.08 0.16 0.08 0.23 0.15 0.09 0.14 \n0.8 \n7 0.11 0.12 0.12 0.17 0.10 0.16 0.15 1.00 0.15 0.12 0.11 0.15 0.09 0.15 0.06 0.16 0.10 0.13 0.15 0.00 0.17 0.08 0.13 0.09 0.09 0.14 0.09 0.11 \n8 0.15 0.10 0.22 0.20 0.15 0.24 0.15 0.15 1.00 0.14 0.28 0.23 0.18 0.20 0.07 0.20 0.12 0.21 0.21 0.01 0.20 0.12 0.17 0.09 0.14 0.19 0.11 0.17 \n9 0.16 0.17 0.16 0.20 0.18 0.15 0.15 0.12 0.14 1.00 0.19 0.16 0.16 0.24 0.11 0.13 0.15 0.16 0.11 0.09 0.19 0.17 0.21 0.12 0.13 0.15 0.14 0.22 \n10 0.17 0.12 0.21 0.23 0.19 0.23 0.16 0.11 0.28 0.19 1.00 0.29 0.30 0.22 0.13 0.21 0.19 0.30 0.33 0.09 0.22 0.20 0.22 0.12 0.16 0.22 0.16 0.30 \n11 0.20 0.14 0.22 0.27 0.16 0.33 0.21 0.15 0.23 0.16 0.29 1.00 0.20 0.21 0.08 0.25 0.15 0.27 0.23 0.00 0.21 0.12 0.20 0.11 0.19 0.20 0.12 0.21 \n0.6 \n12 0.12 0.10 0.13 0.16 0.14 0.12 0.11 0.09 0.18 0.16 0.30 0.20 1.00 0.24 0.10 0.24 0.15 0.27 0.26 0.06 0.28 0.15 0.14 0.10 0.12 0.19 0.14 0.18 \nLayer 14 0.06 0.07 0.07 0.07 0.09 0.07 0.05 0.06 0.07 0.11 0.13 0.08 0.10 0.10 1.00 0.09 0.17 0.14 0.10 0.11 0.08 0.15 0.16 0.11 0.06 0.07 0.16 0.14 \n13 0.18 0.13 0.15 0.17 0.15 0.15 0.15 0.15 0.20 0.24 0.22 0.21 0.24 1.00 0.10 0.26 0.17 0.23 0.20 0.05 0.23 0.16 0.17 0.12 0.17 0.17 0.13 0.17 \n15 0.17 0.11 0.16 0.20 0.14 0.21 0.15 0.16 0.20 0.13 0.21 0.25 0.24 0.26 0.09 1.00 0.14 0.23 0.19 0.01 0.25 0.12 0.14 0.11 0.14 0.21 0.11 0.17 \n16 0.11 0.12 0.12 0.12 0.15 0.12 0.09 0.10 0.12 0.15 0.19 0.15 0.15 0.17 0.17 0.14 1.00 0.24 0.16 0.10 0.13 0.24 0.18 0.18 0.10 0.12 0.19 0.20 0.4 \n17 0.16 0.11 0.18 0.19 0.17 0.20 0.14 0.13 0.21 0.16 0.30 0.27 0.27 0.23 0.14 0.23 0.24 1.00 0.31 0.05 0.25 0.20 0.19 0.14 0.15 0.19 0.16 0.22 \n18 0.12 0.10 0.14 0.16 0.14 0.15 0.15 0.15 0.21 0.11 0.33 0.23 0.26 0.20 0.10 0.19 0.16 0.31 1.00 0.06 0.20 0.15 0.15 0.11 0.14 0.18 0.11 0.21 \n19 0.00 0.16 0.00 0.01 0.11 0.00 0.00 0.00 0.01 0.09 0.09 0.00 0.06 0.05 0.11 0.01 0.10 0.05 0.06 1.00 0.02 0.15 0.14 0.15 0.00 0.02 0.15 0.12 \n20 0.14 0.10 0.15 0.22 0.13 0.18 0.15 0.17 0.20 0.19 0.22 0.21 0.28 0.23 0.08 0.25 0.13 0.25 0.20 0.02 1.00 0.12 0.16 0.10 0.13 0.17 0.11 0.18 \n0.2 \n21 0.10 0.11 0.10 0.12 0.16 0.11 0.08 0.08 0.12 0.17 0.20 0.12 0.15 0.16 0.15 0.12 0.24 0.20 0.15 0.15 0.12 1.00 0.23 0.20 0.10 0.12 0.22 0.24 \n22 0.14 0.18 0.15 0.21 0.19 0.19 0.16 0.13 0.17 0.21 0.22 0.20 0.14 0.17 0.16 0.14 0.18 0.19 0.15 0.14 0.16 0.23 1.00 0.16 0.14 0.15 0.23 0.28 \n23 0.10 0.10 0.11 0.11 0.16 0.09 0.08 0.09 0.09 0.12 0.12 0.11 0.10 0.12 0.11 0.11 0.18 0.14 0.11 0.15 0.10 0.20 0.16 1.00 0.10 0.09 0.21 0.14 \n24 0.18 0.09 0.24 0.17 0.11 0.18 0.23 0.09 0.14 0.13 0.16 0.19 0.12 0.17 0.06 0.14 0.10 0.15 0.14 0.00 0.13 0.10 0.14 0.10 1.00 0.12 0.09 0.13 \n25 0.17 0.13 0.15 0.18 0.14 0.16 0.15 0.14 0.19 0.15 0.22 0.20 0.19 0.17 0.07 0.21 0.12 0.19 0.18 0.02 0.17 0.12 0.15 0.09 0.12 1.00 0.10 0.19 \n26 0.10 0.10 0.09 0.11 0.16 0.11 0.09 0.09 0.11 0.14 0.16 0.12 0.14 0.13 0.16 0.11 0.19 0.16 0.11 0.15 0.11 0.22 0.23 0.21 0.09 0.10 1.00 0.23 \n27 0.15 0.16 0.17 0.21 0.22 0.18 0.14 0.11 0.17 0.22 0.30 0.21 0.18 0.17 0.14 0.17 0.20 0.22 0.21 0.12 0.18 0.24 0.28 0.14 0.13 0.19 0.23 1.00 \n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \nLayer \n\n\nFigure 14: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on Qwen2-7B-Instruct\n\nInstruct and Qwen2-7B-Instruct exhibit significant performance degradation, with LLaMA3-8B-Instruct experiencing a\nsteeper decline after two layers of index reuse than Qwen2-7B-Instruct. This suggests that the Qwen2-7B-Instruct model\nmay be more robust to index reuse.\n\n23",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.171,
          "y": 0.0577
        },
        {
          "x": 0.8039,
          "y": 0.0577
        },
        {
          "x": 0.8039,
          "y": 0.0718
        },
        {
          "x": 0.171,
          "y": 0.0718
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: Owen2-7B-Instruct with H2o\n- Chart Type: line\n|  | top_left | top_right | bottom_left | bottom_right |\n| --- | --- | --- | --- | --- |\n| item_01 | 1 | 2 | 3 | 7 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.248,
          "y": 0.085
        },
        {
          "x": 0.7554,
          "y": 0.085
        },
        {
          "x": 0.7554,
          "y": 0.4173
        },
        {
          "x": 0.248,
          "y": 0.4173
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Figure 13: Layer-wise similarity heatmaps of the preserved KV cache indices by H2O on Qwen2-7B-Instruct",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.127,
          "y": 0.4325
        },
        {
          "x": 0.8485,
          "y": 0.4325
        },
        {
          "x": 0.8485,
          "y": 0.4495
        },
        {
          "x": 0.127,
          "y": 0.4495
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "figure",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\nQwen2-7B-Instruct with SnapKV \n0 1.00 0.13 0.24 0.22 0.13 0.23 0.20 0.11 0.15 0.16 0.17 0.20 0.12 0.18 0.06 0.17 0.11 0.16 0.12 0.00 0.14 0.10 0.14 0.10 0.18 0.17 0.10 0.15 \n1 0.13 1.00 0.12 0.13 0.14 0.13 0.13 0.12 0.10 0.17 0.12 0.14 0.10 0.13 0.07 0.11 0.12 0.11 0.10 0.16 0.10 0.11 0.18 0.10 0.09 0.13 0.10 0.16 \n1.0 \n2 0.24 0.12 1.00 0.27 0.16 0.27 0.22 0.12 0.22 0.16 0.21 0.22 0.13 0.15 0.07 0.16 0.12 0.18 0.14 0.00 0.15 0.10 0.15 0.11 0.24 0.15 0.09 0.17 \n3 0.22 0.13 0.27 1.00 0.20 0.30 0.25 0.17 0.20 0.20 0.23 0.27 0.16 0.17 0.07 0.20 0.12 0.19 0.16 0.01 0.22 0.12 0.21 0.11 0.17 0.18 0.11 0.21 \n4 0.13 0.14 0.16 0.20 1.00 0.17 0.14 0.10 0.15 0.18 0.19 0.16 0.14 0.15 0.09 0.14 0.15 0.17 0.14 0.11 0.13 0.16 0.19 0.16 0.11 0.14 0.16 0.22 \n5 0.23 0.13 0.27 0.30 0.17 1.00 0.21 0.16 0.24 0.15 0.23 0.33 0.12 0.15 0.07 0.21 0.12 0.20 0.15 0.00 0.18 0.11 0.19 0.09 0.18 0.16 0.11 0.18 \n6 0.20 0.13 0.22 0.25 0.14 0.21 1.00 0.15 0.15 0.15 0.16 0.21 0.11 0.15 0.05 0.15 0.09 0.14 0.15 0.00 0.15 0.08 0.16 0.08 0.23 0.15 0.09 0.14 \n0.8 \n7 0.11 0.12 0.12 0.17 0.10 0.16 0.15 1.00 0.15 0.12 0.11 0.15 0.09 0.15 0.06 0.16 0.10 0.13 0.15 0.00 0.17 0.08 0.13 0.09 0.09 0.14 0.09 0.11 \n8 0.15 0.10 0.22 0.20 0.15 0.24 0.15 0.15 1.00 0.14 0.28 0.23 0.18 0.20 0.07 0.20 0.12 0.21 0.21 0.01 0.20 0.12 0.17 0.09 0.14 0.19 0.11 0.17 \n9 0.16 0.17 0.16 0.20 0.18 0.15 0.15 0.12 0.14 1.00 0.19 0.16 0.16 0.24 0.11 0.13 0.15 0.16 0.11 0.09 0.19 0.17 0.21 0.12 0.13 0.15 0.14 0.22 \n10 0.17 0.12 0.21 0.23 0.19 0.23 0.16 0.11 0.28 0.19 1.00 0.29 0.30 0.22 0.13 0.21 0.19 0.30 0.33 0.09 0.22 0.20 0.22 0.12 0.16 0.22 0.16 0.30 \n11 0.20 0.14 0.22 0.27 0.16 0.33 0.21 0.15 0.23 0.16 0.29 1.00 0.20 0.21 0.08 0.25 0.15 0.27 0.23 0.00 0.21 0.12 0.20 0.11 0.19 0.20 0.12 0.21 \n0.6 \n12 0.12 0.10 0.13 0.16 0.14 0.12 0.11 0.09 0.18 0.16 0.30 0.20 1.00 0.24 0.10 0.24 0.15 0.27 0.26 0.06 0.28 0.15 0.14 0.10 0.12 0.19 0.14 0.18 \nLayer 14 0.06 0.07 0.07 0.07 0.09 0.07 0.05 0.06 0.07 0.11 0.13 0.08 0.10 0.10 1.00 0.09 0.17 0.14 0.10 0.11 0.08 0.15 0.16 0.11 0.06 0.07 0.16 0.14 \n13 0.18 0.13 0.15 0.17 0.15 0.15 0.15 0.15 0.20 0.24 0.22 0.21 0.24 1.00 0.10 0.26 0.17 0.23 0.20 0.05 0.23 0.16 0.17 0.12 0.17 0.17 0.13 0.17 \n15 0.17 0.11 0.16 0.20 0.14 0.21 0.15 0.16 0.20 0.13 0.21 0.25 0.24 0.26 0.09 1.00 0.14 0.23 0.19 0.01 0.25 0.12 0.14 0.11 0.14 0.21 0.11 0.17 \n16 0.11 0.12 0.12 0.12 0.15 0.12 0.09 0.10 0.12 0.15 0.19 0.15 0.15 0.17 0.17 0.14 1.00 0.24 0.16 0.10 0.13 0.24 0.18 0.18 0.10 0.12 0.19 0.20 0.4 \n17 0.16 0.11 0.18 0.19 0.17 0.20 0.14 0.13 0.21 0.16 0.30 0.27 0.27 0.23 0.14 0.23 0.24 1.00 0.31 0.05 0.25 0.20 0.19 0.14 0.15 0.19 0.16 0.22 \n18 0.12 0.10 0.14 0.16 0.14 0.15 0.15 0.15 0.21 0.11 0.33 0.23 0.26 0.20 0.10 0.19 0.16 0.31 1.00 0.06 0.20 0.15 0.15 0.11 0.14 0.18 0.11 0.21 \n19 0.00 0.16 0.00 0.01 0.11 0.00 0.00 0.00 0.01 0.09 0.09 0.00 0.06 0.05 0.11 0.01 0.10 0.05 0.06 1.00 0.02 0.15 0.14 0.15 0.00 0.02 0.15 0.12 \n20 0.14 0.10 0.15 0.22 0.13 0.18 0.15 0.17 0.20 0.19 0.22 0.21 0.28 0.23 0.08 0.25 0.13 0.25 0.20 0.02 1.00 0.12 0.16 0.10 0.13 0.17 0.11 0.18 \n0.2 \n21 0.10 0.11 0.10 0.12 0.16 0.11 0.08 0.08 0.12 0.17 0.20 0.12 0.15 0.16 0.15 0.12 0.24 0.20 0.15 0.15 0.12 1.00 0.23 0.20 0.10 0.12 0.22 0.24 \n22 0.14 0.18 0.15 0.21 0.19 0.19 0.16 0.13 0.17 0.21 0.22 0.20 0.14 0.17 0.16 0.14 0.18 0.19 0.15 0.14 0.16 0.23 1.00 0.16 0.14 0.15 0.23 0.28 \n23 0.10 0.10 0.11 0.11 0.16 0.09 0.08 0.09 0.09 0.12 0.12 0.11 0.10 0.12 0.11 0.11 0.18 0.14 0.11 0.15 0.10 0.20 0.16 1.00 0.10 0.09 0.21 0.14 \n24 0.18 0.09 0.24 0.17 0.11 0.18 0.23 0.09 0.14 0.13 0.16 0.19 0.12 0.17 0.06 0.14 0.10 0.15 0.14 0.00 0.13 0.10 0.14 0.10 1.00 0.12 0.09 0.13 \n25 0.17 0.13 0.15 0.18 0.14 0.16 0.15 0.14 0.19 0.15 0.22 0.20 0.19 0.17 0.07 0.21 0.12 0.19 0.18 0.02 0.17 0.12 0.15 0.09 0.12 1.00 0.10 0.19 \n26 0.10 0.10 0.09 0.11 0.16 0.11 0.09 0.09 0.11 0.14 0.16 0.12 0.14 0.13 0.16 0.11 0.19 0.16 0.11 0.15 0.11 0.22 0.23 0.21 0.09 0.10 1.00 0.23 \n27 0.15 0.16 0.17 0.21 0.22 0.18 0.14 0.11 0.17 0.22 0.30 0.21 0.18 0.17 0.14 0.17 0.20 0.22 0.21 0.12 0.18 0.24 0.28 0.14 0.13 0.19 0.23 1.00 \n0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 \nLayer \n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2471,
          "y": 0.4704
        },
        {
          "x": 0.7619,
          "y": 0.4704
        },
        {
          "x": 0.7619,
          "y": 0.8023
        },
        {
          "x": 0.2471,
          "y": 0.8023
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Figure 14: Layer-wise similarity heatmaps of the preserved KV cache indices by SnapKV on Qwen2-7B-Instruct",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1145,
          "y": 0.8158
        },
        {
          "x": 0.861,
          "y": 0.8158
        },
        {
          "x": 0.861,
          "y": 0.8314
        },
        {
          "x": 0.1145,
          "y": 0.8314
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Instruct and Qwen2-7B-Instruct exhibit significant performance degradation, with LLaMA3-8B-Instruct experiencing a\nsteeper decline after two layers of index reuse than Qwen2-7B-Instruct. This suggests that the Qwen2-7B-Instruct model\nmay be more robust to index reuse.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0867,
          "y": 0.8614
        },
        {
          "x": 0.8898,
          "y": 0.8614
        },
        {
          "x": 0.8898,
          "y": 0.9072
        },
        {
          "x": 0.0867,
          "y": 0.9072
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "23",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4765,
          "y": 0.9234
        },
        {
          "x": 0.4986,
          "y": 0.9234
        },
        {
          "x": 0.4986,
          "y": 0.9374
        },
        {
          "x": 0.4765,
          "y": 0.9374
        }
      ],
      "id": 6,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}