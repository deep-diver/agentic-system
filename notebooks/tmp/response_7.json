{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\nTable 7: NIAH Performance Comparison.\n\n| KV cache Size | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\n| --- | --- | --- | --- | --- | --- |\n| LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 |\n| 512 | 32.0% | 68.6% | 71.2 % | 72.6% | 74.5% |\n| 256 | 28.0% | 61.7% | 68.8% | 69.5% | 74.1% |\n| 128 | 23.7% | 47.9% | 58.9% | 65.1% | 73.8% |\n| 96 | 21.5% | 41.0% | 56.2% | 63.2% | 70.3% |\n| Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 |\n| 128 | 44.3% | 88.2% | 91.6% | 99.3% | 99.8% |\n\n\ncates that the method can retrieve the needle at that length\nand depth percentage. The detail visualization of the NIAH\nbenchmark can be found in Appendix B.3. The visualization\nresults demonstrate that ChunkKV outperforms other KV\ncache compression methods.\n\n![image](/image/placeholder)\n(a) ChunkKV, accuracy 73.8%\n(b) PyramidKV, accuracy 65.1%\n(c) SnapKV, accuracy 58.9%\n\n(d) StreamingLLM, accuracy 23.7%\n\nFigure 3: NIAH benchmark for LLaMA3-8B-Instruct with\nKV cache size=128 under 8k context length.\n\n4.3. Index Reuse\n\nThis section will evaluate the performance of the layer-wise\nindex reuse approach with ChunkKV from the two aspects\nof efficiency and performance.\n\nMeasuring Efficiency. We evaluated the latency and\nthroughput of ChunkKV compared to FullKV using\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\nwere conducted with reuse layer is 2, batch size set to 1\nand inference was performed using Flash Attention 2, each\nexperiment was repeated 10 times and the average latency\nand throughput were reported.\n\nTable 8: Latency and throughput comparison between\nChunkKV and FullKV under different input-output configu-\nrations. Percentages in parentheses indicate improvements\nover FullKV baseline.\n\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\n| --- | --- | --- | --- | --- |\n| Method | Input | Output | Latency(s) \u2193 | Throughput(T/S) \u2191 |\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\n\n\nThe results in Table 8 shows that the layer-wise index reuse\nstrategy (ChunkKV_reuse) further boosts performance,\nachieving up to a 20.7% reduction in latency, and through-\nput improvements are particularly notable for longer input\nsequences, with ChunkKV_reuse delivering up to a 26.5%\nimprovement over FullKV.\n\nIndex Reuse Performance on LongBench\n\n![image](/image/placeholder)\n- Chart Title: Index Reuse Performance on LongBench\n- X-Axis: Number of Index Reuse Layers\n- Y-Axis: LongBench Score\n- Chart Type: line\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\n| --- | --- | --- | --- | --- |\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\n\n\nFigure 4: Comparison with different index reuse layers on\nLongBench.\n\nMeasuring Task Performance. This experiment evaluates\nthe performance of the layer-wise index reuse approach by\nmeasuring the performance of the LongBench (Bai et al.,\n2024), the experiment settings are the same as LongBench\nin 4.2. And the number of index reuse layers is set from 1\nto the number of layers in the model, where an index reuse\nlayer of 1 corresponds to the normal ChunkKV without\nindex reuse, and our method set reuse layer to 2.\n\nFigure 4 illustrates the performance of ChunkKV with vary-\ning index reuse layers on the LongBench benchmark. Gen-\nerally, reuse layer set to 2 can achieve the minimal perfor-\nmance degradation across all models. For more experiments\n\n7",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1722,
          "y": 0.0574
        },
        {
          "x": 0.8025,
          "y": 0.0574
        },
        {
          "x": 0.8025,
          "y": 0.0721
        },
        {
          "x": 0.1722,
          "y": 0.0721
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 7: NIAH Performance Comparison.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.14,
          "y": 0.0834
        },
        {
          "x": 0.4217,
          "y": 0.0834
        },
        {
          "x": 0.4217,
          "y": 0.0999
        },
        {
          "x": 0.14,
          "y": 0.0999
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| KV cache Size | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\n| --- | --- | --- | --- | --- | --- |\n| LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 | LlaMa-3.1-8B-Instruct FullKV: 74.6% \u2191 |\n| 512 | 32.0% | 68.6% | 71.2 % | 72.6% | 74.5% |\n| 256 | 28.0% | 61.7% | 68.8% | 69.5% | 74.1% |\n| 128 | 23.7% | 47.9% | 58.9% | 65.1% | 73.8% |\n| 96 | 21.5% | 41.0% | 56.2% | 63.2% | 70.3% |\n| Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 | Mistral-7B-Instruct FullKV: 99.8% \u2191 |\n| 128 | 44.3% | 88.2% | 91.6% | 99.3% | 99.8% |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0917,
          "y": 0.1126
        },
        {
          "x": 0.4717,
          "y": 0.1126
        },
        {
          "x": 0.4717,
          "y": 0.2325
        },
        {
          "x": 0.0917,
          "y": 0.2325
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "cates that the method can retrieve the needle at that length\nand depth percentage. The detail visualization of the NIAH\nbenchmark can be found in Appendix B.3. The visualization\nresults demonstrate that ChunkKV outperforms other KV\ncache compression methods.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0879,
          "y": 0.2452
        },
        {
          "x": 0.4757,
          "y": 0.2452
        },
        {
          "x": 0.4757,
          "y": 0.3204
        },
        {
          "x": 0.0879,
          "y": 0.3204
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "figure",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n(a) ChunkKV, accuracy 73.8%\n(b) PyramidKV, accuracy 65.1%\n(c) SnapKV, accuracy 58.9%",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0887,
          "y": 0.3364
        },
        {
          "x": 0.4839,
          "y": 0.3364
        },
        {
          "x": 0.4839,
          "y": 0.6499
        },
        {
          "x": 0.0887,
          "y": 0.6499
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "(d) StreamingLLM, accuracy 23.7%",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.176,
          "y": 0.655
        },
        {
          "x": 0.3951,
          "y": 0.655
        },
        {
          "x": 0.3951,
          "y": 0.6688
        },
        {
          "x": 0.176,
          "y": 0.6688
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Figure 3: NIAH benchmark for LLaMA3-8B-Instruct with\nKV cache size=128 under 8k context length.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0881,
          "y": 0.6802
        },
        {
          "x": 0.4755,
          "y": 0.6802
        },
        {
          "x": 0.4755,
          "y": 0.7112
        },
        {
          "x": 0.0881,
          "y": 0.7112
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "4.3. Index Reuse",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0873,
          "y": 0.7245
        },
        {
          "x": 0.207,
          "y": 0.7245
        },
        {
          "x": 0.207,
          "y": 0.7396
        },
        {
          "x": 0.0873,
          "y": 0.7396
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "This section will evaluate the performance of the layer-wise\nindex reuse approach with ChunkKV from the two aspects\nof efficiency and performance.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0881,
          "y": 0.7483
        },
        {
          "x": 0.4758,
          "y": 0.7483
        },
        {
          "x": 0.4758,
          "y": 0.7937
        },
        {
          "x": 0.0881,
          "y": 0.7937
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Measuring Efficiency. We evaluated the latency and\nthroughput of ChunkKV compared to FullKV using\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\nwere conducted with reuse layer is 2, batch size set to 1\nand inference was performed using Flash Attention 2, each\nexperiment was repeated 10 times and the average latency\nand throughput were reported.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0874,
          "y": 0.8011
        },
        {
          "x": 0.4761,
          "y": 0.8011
        },
        {
          "x": 0.4761,
          "y": 0.9073
        },
        {
          "x": 0.0874,
          "y": 0.9073
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 8: Latency and throughput comparison between\nChunkKV and FullKV under different input-output configu-\nrations. Percentages in parentheses indicate improvements\nover FullKV baseline.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4994,
          "y": 0.0845
        },
        {
          "x": 0.8912,
          "y": 0.0845
        },
        {
          "x": 0.8912,
          "y": 0.1442
        },
        {
          "x": 0.4994,
          "y": 0.1442
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\n| --- | --- | --- | --- | --- |\n| Method | Input | Output | Latency(s) \u2193 | Throughput(T/S) \u2191 |\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.506,
          "y": 0.1577
        },
        {
          "x": 0.8992,
          "y": 0.1577
        },
        {
          "x": 0.8992,
          "y": 0.3442
        },
        {
          "x": 0.506,
          "y": 0.3442
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The results in Table 8 shows that the layer-wise index reuse\nstrategy (ChunkKV_reuse) further boosts performance,\nachieving up to a 20.7% reduction in latency, and through-\nput improvements are particularly notable for longer input\nsequences, with ChunkKV_reuse delivering up to a 26.5%\nimprovement over FullKV.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5002,
          "y": 0.3709
        },
        {
          "x": 0.8914,
          "y": 0.3709
        },
        {
          "x": 0.8914,
          "y": 0.4613
        },
        {
          "x": 0.5002,
          "y": 0.4613
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Index Reuse Performance on LongBench",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5813,
          "y": 0.4767
        },
        {
          "x": 0.8329,
          "y": 0.4767
        },
        {
          "x": 0.8329,
          "y": 0.4875
        },
        {
          "x": 0.5813,
          "y": 0.4875
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: Index Reuse Performance on LongBench\n- X-Axis: Number of Index Reuse Layers\n- Y-Axis: LongBench Score\n- Chart Type: line\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\n| --- | --- | --- | --- | --- |\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5244,
          "y": 0.4801
        },
        {
          "x": 0.8665,
          "y": 0.4801
        },
        {
          "x": 0.8665,
          "y": 0.6694
        },
        {
          "x": 0.5244,
          "y": 0.6694
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "Figure 4: Comparison with different index reuse layers on\nLongBench.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.499,
          "y": 0.6839
        },
        {
          "x": 0.8892,
          "y": 0.6839
        },
        {
          "x": 0.8892,
          "y": 0.7131
        },
        {
          "x": 0.499,
          "y": 0.7131
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Measuring Task Performance. This experiment evaluates\nthe performance of the layer-wise index reuse approach by\nmeasuring the performance of the LongBench (Bai et al.,\n2024), the experiment settings are the same as LongBench\nin 4.2. And the number of index reuse layers is set from 1\nto the number of layers in the model, where an index reuse\nlayer of 1 corresponds to the normal ChunkKV without\nindex reuse, and our method set reuse layer to 2.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5002,
          "y": 0.7177
        },
        {
          "x": 0.89,
          "y": 0.7177
        },
        {
          "x": 0.89,
          "y": 0.8391
        },
        {
          "x": 0.5002,
          "y": 0.8391
        }
      ],
      "id": 16,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Figure 4 illustrates the performance of ChunkKV with vary-\ning index reuse layers on the LongBench benchmark. Gen-\nerally, reuse layer set to 2 can achieve the minimal perfor-\nmance degradation across all models. For more experiments",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5009,
          "y": 0.8469
        },
        {
          "x": 0.8907,
          "y": 0.8469
        },
        {
          "x": 0.8907,
          "y": 0.9076
        },
        {
          "x": 0.5009,
          "y": 0.9076
        }
      ],
      "id": 17,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "7",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4802,
          "y": 0.9238
        },
        {
          "x": 0.4947,
          "y": 0.9238
        },
        {
          "x": 0.4947,
          "y": 0.9372
        },
        {
          "x": 0.4802,
          "y": 0.9372
        }
      ],
      "id": 18,
      "page": 1
    }
  ],
  "model": "document-parse-250116",
  "usage": {
    "pages": 1
  }
}