{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\nProof. (Fang & Xie, 2022) By the Taylor expansion on \u03b8, we have for any \u03b8 in B that\n\n$$\\sum_{j=2}^{k}{\\bf K}{\\bf L}_{i}(\\theta^{\\star}||\\theta)\\geq{\\frac{1}{2}}\\sum_{j=2}^{k}(\\theta-\\theta^{\\star})^{\\top}{\\cal I}_{j,\\theta^{\\star}}(\\theta-\\theta^{\\star})+(k-1){\\cal O}(||\\theta-\\theta^{\\star}||^{3})$$\n\n$$\\geq{\\frac{1}{2}}(k-1)\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})||\\theta-\\theta^{\\star}||^{2}$$\n\n$$\\Longrightarrow\\parallel(\\theta-\\theta^{\\star}||^{2}\\leq{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))}{{\\frac{1}{2}}(k-1)(\\operatorname*{min}_{j}\\ \\lambda_{\\operatorname*{min}}(I_{j,\\theta^{\\star}}))}}.$$\n\n(10)\n\n(11)\n\n(12)\n\nWe can bound the last KL term (k-th token) with the above term:\n\n$$\\begin{array}{l}{{\\mathrm{KL}_{k}(\\theta^{\\star}\\|\\theta)=\\displaystyle{\\frac{1}{2}}(\\theta-\\theta^{\\star})^{\\top}I_{k,\\theta^{\\star}}(\\theta-\\theta^{\\star})+O(\\|\\theta-\\theta^{\\star}\\|^{3})}}\\\\ {{\\leq\\displaystyle{\\frac{1}{2}}(\\operatorname*{max}\\lambda_{\\mathrm{max}}(I_{j,\\theta^{\\star}}))\\|\\theta-\\theta^{\\star}\\|^{2}+O(|\\theta-\\theta^{\\star}\\|^{3})}}\\\\ {{\\leq\\displaystyle{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))(\\mathrm{max_{j}~\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})+O(1)}{(k-1)\\mathrm{min}_{j}~\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})}}.}\\end{array}$$\n\n(13)\n\n(14)\n\n(15)\n\nRearranging above equation, and with KLk(\u03b8\u22c6||\u03b8) = Extest\u223cpprompt[KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8))], there is\n\n$$\\mathbb{E}_{x_{\\mathrm{res}}\\sim p_{\\mathrm{pere}}}[K L(p_{\\mathrm{peopt}}(y_{\\mathrm{kest}}|x_{\\mathrm{kest}})||p(y_{\\mathrm{kest}}|x_{\\mathrm{test}},\\theta))|\\leq{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))(\\mathrm{max_{j}}\\ \\lambda_{\\mathrm{min}}(I_{j,\\theta^{*}})+O(1))}{(k-1)\\ \\mathrm{min}_{j}\\ \\lambda_{\\mathrm{min}}(I_{j,\\theta^{*}})}}$$\n\n(16)\n\nCombining Equation 16 with Equation 6 into Lemma 1 completes the proof.\n\nKV Cache Sparsification. Revisiting the Equation 4 in Condition 2, the \u03be\u03b8(r) is enlarged with the sparsity ratio r. The\nhigher compression ratio r (means that more KV cache are discarded), the more noise \u03be\u03b8(r). Then it leads to the higher\nbound of the limn\u2192\u221e L0\u22121(fn) in Equation 5 in Lemma 1. Next, we discuss how KV cache compression influences the\nEquation 4.\n\nToken-level Importance Measure. The token-level KV cache methods usually calculate the importance of different tokens.\nThen, the KV cache with indexes that have higher importance will be preserved. Such indexes are normaly choosed as the\nattention score. Considering that the case in Figure 1, where each token in the i-th training 2 example sequence (Oi = [xi, yi])\nmight be compressed, and tokens are sparsified concretely without out dependency to other tokens. However, in the generation\nprocess of the i-th training example, Oi = [xi, yi] is sampled from p(Oi|hstart , \u03b8\u22c6) and pj \u03b8(o) := p(O[j] = o|O[1 : j \u2212 1], \u03b8)\ni\nof the j-th token with previous tokens and the analogous distribution pj prompt := pprompt(O[j] = o|O[1 : j \u2212 1]). And the\nKL divergence is defined as KLj(\u03b8\u22c6||\u03b8) := EO[1:j\u22121]\u223cpprompt[KL(pj prompt||pj \u03b8)], which means that in a training example\nOi = [xi, yi] = Oi[1 : k], each token Oi[j] has strong dependency with Oi[1 : j \u2212 1], noises on previous any j-th token\nwill influence the distinguishability on the following tokens, i.e. requiring larger {KLu(\u03b8\u22c6||\u03b8)}u>j.\n\nOn the other hand, the token-level sparsity enlarges the requirement on the distinguishability uniformly for each example Oi\n(the case in Figure 1), which uniformly loses the bound of L0-1(fn) as in Equation 9.\n\nChunk-level Importance Measure. Different from token-level importance measure, ChunkKV regards tokens in a\ncontinuous window as a basic unit that should be left or discarded as a whole. The preserved window can be regarded\nas saving the complete Oi = [xi, yi] without noise. Thus, ChunkKV reduces the noise \u03be\u03b8(r) for the preserved Oi, which\nlowers the bound of L0-1(fn).\n\nMore intuitively, ChunkKV focus on reducing the noise on some complete training examples, but some other examples\noverall with low importance will be discarded. Then, the model identifies the xtest from those clean and more related training\nexamples Oi and neglect those Oi with less importance.\n\nNote that here, we do not provide the rigorous proof on how KV cache sparsity enhances the requirement of the distin-\nguishability and how different KLj(\u03b8\u22c6||\u03b8) on Oi = [xi, yi] influences the bound L0-1(fn). We left this as the future work to\nanalyze how KV cache sparsity influences the in-context learning.\n\n2Here, training means prompt learning (Fang & Xie, 2022).\n\n32",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1726,
          "y": 0.0574
        },
        {
          "x": 0.8024,
          "y": 0.0574
        },
        {
          "x": 0.8024,
          "y": 0.0722
        },
        {
          "x": 0.1726,
          "y": 0.0722
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Proof. (Fang & Xie, 2022) By the Taylor expansion on \u03b8, we have for any \u03b8 in B that",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0873,
          "y": 0.0851
        },
        {
          "x": 0.6574,
          "y": 0.0851
        },
        {
          "x": 0.6574,
          "y": 0.1016
        },
        {
          "x": 0.0873,
          "y": 0.1016
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\sum_{j=2}^{k}{\\bf K}{\\bf L}_{i}(\\theta^{\\star}||\\theta)\\geq{\\frac{1}{2}}\\sum_{j=2}^{k}(\\theta-\\theta^{\\star})^{\\top}{\\cal I}_{j,\\theta^{\\star}}(\\theta-\\theta^{\\star})+(k-1){\\cal O}(||\\theta-\\theta^{\\star}||^{3})$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2496,
          "y": 0.1107
        },
        {
          "x": 0.7357,
          "y": 0.1107
        },
        {
          "x": 0.7357,
          "y": 0.1569
        },
        {
          "x": 0.2496,
          "y": 0.1569
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\geq{\\frac{1}{2}}(k-1)\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})||\\theta-\\theta^{\\star}||^{2}$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.3563,
          "y": 0.1592
        },
        {
          "x": 0.5782,
          "y": 0.1592
        },
        {
          "x": 0.5782,
          "y": 0.1847
        },
        {
          "x": 0.3563,
          "y": 0.1847
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\Longrightarrow\\parallel(\\theta-\\theta^{\\star}||^{2}\\leq{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))}{{\\frac{1}{2}}(k-1)(\\operatorname*{min}_{j}\\ \\lambda_{\\operatorname*{min}}(I_{j,\\theta^{\\star}}))}}.$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2484,
          "y": 0.1871
        },
        {
          "x": 0.5718,
          "y": 0.1871
        },
        {
          "x": 0.5718,
          "y": 0.2238
        },
        {
          "x": 0.2484,
          "y": 0.2238
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(10)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8566,
          "y": 0.1246
        },
        {
          "x": 0.8888,
          "y": 0.1246
        },
        {
          "x": 0.8888,
          "y": 0.1409
        },
        {
          "x": 0.8566,
          "y": 0.1409
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(11)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8567,
          "y": 0.1635
        },
        {
          "x": 0.8885,
          "y": 0.1635
        },
        {
          "x": 0.8885,
          "y": 0.1804
        },
        {
          "x": 0.8567,
          "y": 0.1804
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(12)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8572,
          "y": 0.1954
        },
        {
          "x": 0.888,
          "y": 0.1954
        },
        {
          "x": 0.888,
          "y": 0.2115
        },
        {
          "x": 0.8572,
          "y": 0.2115
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "We can bound the last KL term (k-th token) with the above term:",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0866,
          "y": 0.2305
        },
        {
          "x": 0.5159,
          "y": 0.2305
        },
        {
          "x": 0.5159,
          "y": 0.2473
        },
        {
          "x": 0.0866,
          "y": 0.2473
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\begin{array}{l}{{\\mathrm{KL}_{k}(\\theta^{\\star}\\|\\theta)=\\displaystyle{\\frac{1}{2}}(\\theta-\\theta^{\\star})^{\\top}I_{k,\\theta^{\\star}}(\\theta-\\theta^{\\star})+O(\\|\\theta-\\theta^{\\star}\\|^{3})}}\\\\ {{\\leq\\displaystyle{\\frac{1}{2}}(\\operatorname*{max}\\lambda_{\\mathrm{max}}(I_{j,\\theta^{\\star}}))\\|\\theta-\\theta^{\\star}\\|^{2}+O(|\\theta-\\theta^{\\star}\\|^{3})}}\\\\ {{\\leq\\displaystyle{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))(\\mathrm{max_{j}~\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})+O(1)}{(k-1)\\mathrm{min}_{j}~\\lambda_{\\mathrm{min}}(I_{j,\\theta^{\\star}})}}.}\\end{array}$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2796,
          "y": 0.2544
        },
        {
          "x": 0.6948,
          "y": 0.2544
        },
        {
          "x": 0.6948,
          "y": 0.3538
        },
        {
          "x": 0.2796,
          "y": 0.3538
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(13)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8574,
          "y": 0.2624
        },
        {
          "x": 0.8878,
          "y": 0.2624
        },
        {
          "x": 0.8878,
          "y": 0.279
        },
        {
          "x": 0.8574,
          "y": 0.279
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(14)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8581,
          "y": 0.2931
        },
        {
          "x": 0.8875,
          "y": 0.2931
        },
        {
          "x": 0.8875,
          "y": 0.3091
        },
        {
          "x": 0.8581,
          "y": 0.3091
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(15)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8569,
          "y": 0.3272
        },
        {
          "x": 0.8881,
          "y": 0.3272
        },
        {
          "x": 0.8881,
          "y": 0.3437
        },
        {
          "x": 0.8569,
          "y": 0.3437
        }
      ],
      "id": 12,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Rearranging above equation, and with KLk(\u03b8\u22c6||\u03b8) = Extest\u223cpprompt[KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8))], there is",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0867,
          "y": 0.3608
        },
        {
          "x": 0.8289,
          "y": 0.3608
        },
        {
          "x": 0.8289,
          "y": 0.38
        },
        {
          "x": 0.0867,
          "y": 0.38
        }
      ],
      "id": 13,
      "page": 1
    },
    {
      "category": "equation",
      "content": {
        "html": "",
        "markdown": "$$\\mathbb{E}_{x_{\\mathrm{res}}\\sim p_{\\mathrm{pere}}}[K L(p_{\\mathrm{peopt}}(y_{\\mathrm{kest}}|x_{\\mathrm{kest}})||p(y_{\\mathrm{kest}}|x_{\\mathrm{test}},\\theta))|\\leq{\\frac{(\\epsilon_{\\theta}+\\xi_{\\theta}(r))(\\mathrm{max_{j}}\\ \\lambda_{\\mathrm{min}}(I_{j,\\theta^{*}})+O(1))}{(k-1)\\ \\mathrm{min}_{j}\\ \\lambda_{\\mathrm{min}}(I_{j,\\theta^{*}})}}$$",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1742,
          "y": 0.387
        },
        {
          "x": 0.8094,
          "y": 0.387
        },
        {
          "x": 0.8094,
          "y": 0.4266
        },
        {
          "x": 0.1742,
          "y": 0.4266
        }
      ],
      "id": 14,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "(16)",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.8568,
          "y": 0.3968
        },
        {
          "x": 0.8885,
          "y": 0.3968
        },
        {
          "x": 0.8885,
          "y": 0.4139
        },
        {
          "x": 0.8568,
          "y": 0.4139
        }
      ],
      "id": 15,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Combining Equation 16 with Equation 6 into Lemma 1 completes the proof.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.087,
          "y": 0.4313
        },
        {
          "x": 0.5922,
          "y": 0.4313
        },
        {
          "x": 0.5922,
          "y": 0.4482
        },
        {
          "x": 0.087,
          "y": 0.4482
        }
      ],
      "id": 16,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "KV Cache Sparsification. Revisiting the Equation 4 in Condition 2, the \u03be\u03b8(r) is enlarged with the sparsity ratio r. The\nhigher compression ratio r (means that more KV cache are discarded), the more noise \u03be\u03b8(r). Then it leads to the higher\nbound of the limn\u2192\u221e L0\u22121(fn) in Equation 5 in Lemma 1. Next, we discuss how KV cache compression influences the\nEquation 4.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0869,
          "y": 0.4618
        },
        {
          "x": 0.8899,
          "y": 0.4618
        },
        {
          "x": 0.8899,
          "y": 0.5228
        },
        {
          "x": 0.0869,
          "y": 0.5228
        }
      ],
      "id": 17,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Token-level Importance Measure. The token-level KV cache methods usually calculate the importance of different tokens.\nThen, the KV cache with indexes that have higher importance will be preserved. Such indexes are normaly choosed as the\nattention score. Considering that the case in Figure 1, where each token in the i-th training 2 example sequence (Oi = [xi, yi])\nmight be compressed, and tokens are sparsified concretely without out dependency to other tokens. However, in the generation\nprocess of the i-th training example, Oi = [xi, yi] is sampled from p(Oi|hstart , \u03b8\u22c6) and pj \u03b8(o) := p(O[j] = o|O[1 : j \u2212 1], \u03b8)\ni\nof the j-th token with previous tokens and the analogous distribution pj prompt := pprompt(O[j] = o|O[1 : j \u2212 1]). And the\nKL divergence is defined as KLj(\u03b8\u22c6||\u03b8) := EO[1:j\u22121]\u223cpprompt[KL(pj prompt||pj \u03b8)], which means that in a training example\nOi = [xi, yi] = Oi[1 : k], each token Oi[j] has strong dependency with Oi[1 : j \u2212 1], noises on previous any j-th token\nwill influence the distinguishability on the following tokens, i.e. requiring larger {KLu(\u03b8\u22c6||\u03b8)}u>j.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0867,
          "y": 0.5311
        },
        {
          "x": 0.8924,
          "y": 0.5311
        },
        {
          "x": 0.8924,
          "y": 0.6723
        },
        {
          "x": 0.0867,
          "y": 0.6723
        }
      ],
      "id": 18,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "On the other hand, the token-level sparsity enlarges the requirement on the distinguishability uniformly for each example Oi\n(the case in Figure 1), which uniformly loses the bound of L0-1(fn) as in Equation 9.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0884,
          "y": 0.6791
        },
        {
          "x": 0.8872,
          "y": 0.6791
        },
        {
          "x": 0.8872,
          "y": 0.7097
        },
        {
          "x": 0.0884,
          "y": 0.7097
        }
      ],
      "id": 19,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Chunk-level Importance Measure. Different from token-level importance measure, ChunkKV regards tokens in a\ncontinuous window as a basic unit that should be left or discarded as a whole. The preserved window can be regarded\nas saving the complete Oi = [xi, yi] without noise. Thus, ChunkKV reduces the noise \u03be\u03b8(r) for the preserved Oi, which\nlowers the bound of L0-1(fn).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0876,
          "y": 0.7167
        },
        {
          "x": 0.8896,
          "y": 0.7167
        },
        {
          "x": 0.8896,
          "y": 0.7772
        },
        {
          "x": 0.0876,
          "y": 0.7772
        }
      ],
      "id": 20,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "More intuitively, ChunkKV focus on reducing the noise on some complete training examples, but some other examples\noverall with low importance will be discarded. Then, the model identifies the xtest from those clean and more related training\nexamples Oi and neglect those Oi with less importance.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0873,
          "y": 0.7848
        },
        {
          "x": 0.89,
          "y": 0.7848
        },
        {
          "x": 0.89,
          "y": 0.8303
        },
        {
          "x": 0.0873,
          "y": 0.8303
        }
      ],
      "id": 21,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Note that here, we do not provide the rigorous proof on how KV cache sparsity enhances the requirement of the distin-\nguishability and how different KLj(\u03b8\u22c6||\u03b8) on Oi = [xi, yi] influences the bound L0-1(fn). We left this as the future work to\nanalyze how KV cache sparsity influences the in-context learning.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0874,
          "y": 0.8376
        },
        {
          "x": 0.89,
          "y": 0.8376
        },
        {
          "x": 0.89,
          "y": 0.8836
        },
        {
          "x": 0.0874,
          "y": 0.8836
        }
      ],
      "id": 22,
      "page": 1
    },
    {
      "category": "footnote",
      "content": {
        "html": "",
        "markdown": "2Here, training means prompt learning (Fang & Xie, 2022).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.108,
          "y": 0.8914
        },
        {
          "x": 0.462,
          "y": 0.8914
        },
        {
          "x": 0.462,
          "y": 0.9084
        },
        {
          "x": 0.108,
          "y": 0.9084
        }
      ],
      "id": 23,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "32",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4767,
          "y": 0.9236
        },
        {
          "x": 0.4986,
          "y": 0.9236
        },
        {
          "x": 0.4986,
          "y": 0.9375
        },
        {
          "x": 0.4767,
          "y": 0.9375
        }
      ],
      "id": 24,
      "page": 1
    }
  ],
  "model": "document-parse-250116",
  "usage": {
    "pages": 1
  }
}