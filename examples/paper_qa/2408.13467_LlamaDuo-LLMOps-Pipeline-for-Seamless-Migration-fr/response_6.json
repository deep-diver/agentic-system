{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by\nGPT4o, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset.\nEach entry is presented as mean score / coverage percentage (%) with 50 score / coverage percentage (%) with 70\nscore. The best results from service and local LLMs are highlighted in bold. \u201cP-Match\u201d represents performance\nmatching, which is defined as the best performance of the local LLM divided by the best performance of the service\nLLM, with the best results highlighted in bold across different judges.\n\n| Task | Model | GPT4o | GPT4o | Claude 3 Sonnet | Claude 3 Sonnet | Gemini 1.5 Flash | Gemini 1.5 Flash |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Task | Model | Precision\u2191 | Similarity\u2191 | Precision\u2191 | Similarity\u2191 | Precision\u2191 | Similarity\u2191 |\n| Summarization | GPT4o Claude 3 Sonnet Gemini 1.5 Flash | 90.71 / 97 % / 96% 88.04 / 97% / 92% 87.90 / 96% / 96% | 82.00 / 95% / 89% 78.18 / 95% / 78% 79.14 / 95% / 88% | 93.25 / 100% / 100% 93.39 / 100% / 99% 91.95 / 100% / 98% | 86.60 / 100% / 95% 85.55 / 100% / 95% 85.05 / 100% / 95% | 87.10 / 100% / 92% 86.70 / 100% / 92% 85.65 / 98% / 96% | 67.45 / 85% / 48% 64.10 / 80% / 36% 66.45 / 89% / 40% |\n| Summarization | Gemma 2B | 57.60 / 64% / 35% | 54.49 / 61% / 35% | 74.89 / 86% / 69% | 64.09 / 73% / 50% | 61.90 / 78% / 40% | 42.15 / 38% / 12% |\n| Summarization | Gemma 7B | 73.54 / 85% / 65% | 68.58 / 85% / 59% | 86.19 / 99% / 93% | 77.41 / 94% / 77% | 74.59 / 95% / 69% | 53.92 / 65% / 22% |\n| Summarization | Mistral 7B | 76.38 / 93% / 70% | 69.65 / 88% / 56% | 86.20 / 99% / 92% | 78.44 / 96% / 80% | 72.74 / 95% / 62% | 50.15 / 54% / 14% |\n| Summarization | LLaMA3 8B | 75.67 / 88% / 75% | 70.54 / 86% / 69% | 87.02 / 99% / 94% | 78.42 / 93% / 86% | 72.74 / 91% / 64% | 52.23 / 64% / 25% |\n| Summarization | P-Match\u2191 | 84.20% / 95.88% / 78.13% | 86.02% / 92.63% / 77.53% | 93.18% / 99% / 94% | 90.58% / 96% / 90.53% | 85.64% / 95% / 71.88% | 79.94% / 73.03% / 52.08% |\n| Classification | GPT4o | 83.62 / 94% / 81% | 74.45 / 80% / 66% | 87.50 / 92% / 92% | 72.28 / 72% / 66% | 82.68 / 94% / 80% | 63.06 / 67% / 44% |\n| Classification | Claude 3 Sonnet | 82.32 / 92% / 78% | 71.53 / 81% / 70% | 92.89 / 100% / 100% | 75.07 / 81% / 73% | 87.34 / 97% / 97% | 67.18 / 80% / 45% |\n| Classification | Gemini 1.5 Flash | 85.43 / 94% / 91% | 72.73 / 81% / 75% | 89.03 / 94% / 89% | 77.96 / 81% / 81% | 83.35 / 94% / 84% | 64.25 / 78% / 47% |\n| Classification | Gemma 2B | 58.47 / 58% / 42% | 52.76 / 50% / 39% | 69.98 / 73% / 62% | 56.31 / 58% / 47% | 62.17 / 62% / 48% | 48.54 / 50% / 39% |\n| Classification | Gemma 7B | 70.73 / 69% / 55% | 64.67 / 62% / 53% | 78.78 / 81% / 75% | 67.76 / 69% / 62% | 70.73 / 75% / 61% | 59.77 / 59% / 52% |\n| Classification | Mistral 7B | 67.53 / 70% / 53% | 61.65 / 67% / 47% | 76.01 / 80% / 72% | 64.43 / 70% / 52% | 67.90 / 73% / 53% | 54.27 / 53% / 45% |\n| Classification | LLaMA3 8B | 81.64 / 88% / 73% | 78.02 / 77% / 67% | 89.20 / 94% / 94% | 82.18 / 88% / 75% | 83.63 / 94% / 77% | 72.54 / 73% / 64% |\n| Classification | P-Match\u2191 | 95.56% / 93.62% / 80.22% | 104.80% / 95.06% / 89.33% | 96.03% / 94% / 94% | 105.41% / 108.64% / 92.59% | 95.75% / 96.91% / 79.38% | 107.98% / 91.25% / 136.17% |\n| Coding | GPT4o | 90.31 / 100% / 98% | 75.18 / 92% / 70% | 94.57 / 100% / 100% | 86.32 / 100% / 91% | 90.78 / 100% / 100% | 58.43 / 62% / 25% |\n| Coding | Claude 3 Sonnet | 88.76 / 100% / 92% | 75.23 / 94% / 67% | 93.82 / 100% / 100% | 87.42 / 100% / 100% | 89.84 / 100% / 100% | 60.46 / 69% / 31% |\n| Coding | Gemini 1.5 Flash | 88.51 |  | 93.59 / 100% / 100% | 82.92 / 97% / 84% | 90.62 / 100% / 98% | 64.21 / 84% / 41% |\n| Coding | Gemma 2B | / 98% / 94% 62.31 / 70% / 44% | 75.62 / 91% / 73% 56.48 / 66% / 41% | 80.92 / 89% / 84% | 67.24 / 78% / 48% | 72.98 / 89% / 66% | 44.08 / 50% / 8% |\n| Coding | Gemma 7B | 80.56 / 92% / 80% | 71.92 / 89% / 70% | 90.47 / 100% / 98% | 80.26 / 92% / 84% | 84.66 / 100% / 88% | 61.23 / 72% / 36% |\n| Coding | Mistral 7B | / / | / 69% / | 81.25 / 92% / | / 83% | / 86% / | 45.25 / 50% / 8% |\n| Coding | LLaMA3 8B | 68.32 77% 56% 77.47 / 88% / 72% | 61.01 45% | 81% 83.97 / 94% / 83% | 69.10 / 55% 73.51 / 88% / 67% | 72.39 69% | 51.10 / 58% / 17% |\n| Coding | P-Match\u2191 | 89.20% / 92% / | 69.46 / 88% / 61% |  |  | 75.55 / 89% / 73% |  |\n| Coding |  | 81.63% | 95.11% / 94.68% / 95.89% | 95.66% / 100% / | 98% 91.81% / 92% / 84% | 93.26% / 100% / 88% | 95.36% / 85.71% / 97.80% |\n|  | GPT4o | 95.45 / 100% / 100% | 84.23 / 93% / 80% | 97.21 / 100% / 100% | 92.56 / 100% / 97% | 93.58 / 100% / 100% | 75.58 / 85% / 63% |\n| Claude 3 Sonnet | 94.03 / 100% / 98% | 85.28 / 100% / | 82% 97.60 / 100% / 100% | 93.95 / 100% / 100% | 93.66 / 100% / 100% | / 92% / |  |\n| Gemini 1.5 Flash | 94.63 / 100% 97% | 87.43 / 95% / |  |  |  | 76.33 65% |  |\n|  | / |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n\n\ncluding summarization, classification, coding, and\nclosed QA. We utilize GPT4o, Claude 3 Sonnet,\nand Gemini 1.5 Flash as judges to evaluate the\nfine-tuned model performance on test subsets of\nthe coverage dataset. As demonstrated in Table\n1, the fine-tuned local LLMs, despite their signif-\nicantly smaller scale, achieve comparable perfor-\nmance on diverse tasks compared to much larger\nservice LLMs. For instance, in the summarization\ntask, LLaMA3 8B achieved a comparable preci-\nsion score of 87.02 / 99% / 94%, compared to\nGPT4o\u2019s score of 93.25 / 100% / 100%, Claude 3\nSonnet\u2019s score of 93.39 / 100% / 99%, and Gem-\nini 1.5 Flash\u2019s score of 91.95 / 100% / 98%, with\nClaude 3 Sonnet serving as judge. These results\nunderscore the efficacy of LlamaDuo in seamlessly\ntransferring knowledge and capabilities from ser-\nvice LLMs to smaller local LLMs without a sub-\nstantial decrease in performance.\n\nIn Table 1, we observe distinct performance\nacross four fine-tuned models when applied to dif-\nferent tasks. Specifically, Mistral 7B stands out\nin summarization tasks, achieving the best perfor-\nmance in 7 out of 12 cases. Moreover, LLaMA3\n8B consistently outperforms competitors across all\nmetrics and evaluators in the classification task.\nConversely, in coding tasks, Gemma 7B is iden-\ntified as the leading model, excelling across all\n\nmetrics and evaluations. Mistral 7B shows supe-\nrior performance in the closed QA task, leading\nin 8 out of 12 cases. Within the realm of ser-\nvice LLMs, Claude 3 Sonnet and Gemini 1.5 Flash\ndemonstrate exceptional performance in classifica-\ntion and closed QA tasks, securing the best results\nin 8 and 10 out of 12 cases, respectively. Lastly,\nGPT4o emerges as the leading model in summa-\nrization and coding tasks, achieving the best per-\nformance in 10 and 7 out of 12 cases, respectively.\nNotably, although Gemma 2B exhibits inferior per-\nformance compared to larger 7B models overall,\nthe disparity in results is not markedly substantial,\nwith Gemma 2B attaining closely comparable per-\nformance in certain tasks. For example, in closed\nQA tasks, Gemma 2B secures a mean precision\nscore of 80.22, while Gemma 7B achieves 88.83,\nMistral 7B reaches 88.25, and LLaMA3 8B obtains\n86.03, as evaluated by Claude 3 Sonnet. This ob-\nservation lends further support to the notion that\nthrough the strategic fine-tuning of smaller local\nLLMs on synthetic datasets via the LlamaDuo, it\nis possible to closely approximate the performance\nof their larger counterparts. Consequently, it of-\nfers increased flexibility and solutions for users\nand scenarios with budgetary considerations. More\nexperimental results are presented in Appendix C.",
    "text": ""
  },
  "elements": [
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 1: Performance of the service LLMs and local LLMs fine-tuned on 128K synthetic dataset produced by\nGPT4o, evaluated by GPT4o, Claude 3 Sonnet, and Gemini 1.5 Flash as judges on test subsets of coverage dataset.\nEach entry is presented as mean score / coverage percentage (%) with 50 score / coverage percentage (%) with 70\nscore. The best results from service and local LLMs are highlighted in bold. \u201cP-Match\u201d represents performance\nmatching, which is defined as the best performance of the local LLM divided by the best performance of the service\nLLM, with the best results highlighted in bold across different judges.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1147,
          "y": 0.0821
        },
        {
          "x": 0.8845,
          "y": 0.0821
        },
        {
          "x": 0.8845,
          "y": 0.17
        },
        {
          "x": 0.1147,
          "y": 0.17
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| Task | Model | GPT4o | GPT4o | Claude 3 Sonnet | Claude 3 Sonnet | Gemini 1.5 Flash | Gemini 1.5 Flash |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Task | Model | Precision\u2191 | Similarity\u2191 | Precision\u2191 | Similarity\u2191 | Precision\u2191 | Similarity\u2191 |\n| Summarization | GPT4o Claude 3 Sonnet Gemini 1.5 Flash | 90.71 / 97 % / 96% 88.04 / 97% / 92% 87.90 / 96% / 96% | 82.00 / 95% / 89% 78.18 / 95% / 78% 79.14 / 95% / 88% | 93.25 / 100% / 100% 93.39 / 100% / 99% 91.95 / 100% / 98% | 86.60 / 100% / 95% 85.55 / 100% / 95% 85.05 / 100% / 95% | 87.10 / 100% / 92% 86.70 / 100% / 92% 85.65 / 98% / 96% | 67.45 / 85% / 48% 64.10 / 80% / 36% 66.45 / 89% / 40% |\n| Summarization | Gemma 2B | 57.60 / 64% / 35% | 54.49 / 61% / 35% | 74.89 / 86% / 69% | 64.09 / 73% / 50% | 61.90 / 78% / 40% | 42.15 / 38% / 12% |\n| Summarization | Gemma 7B | 73.54 / 85% / 65% | 68.58 / 85% / 59% | 86.19 / 99% / 93% | 77.41 / 94% / 77% | 74.59 / 95% / 69% | 53.92 / 65% / 22% |\n| Summarization | Mistral 7B | 76.38 / 93% / 70% | 69.65 / 88% / 56% | 86.20 / 99% / 92% | 78.44 / 96% / 80% | 72.74 / 95% / 62% | 50.15 / 54% / 14% |\n| Summarization | LLaMA3 8B | 75.67 / 88% / 75% | 70.54 / 86% / 69% | 87.02 / 99% / 94% | 78.42 / 93% / 86% | 72.74 / 91% / 64% | 52.23 / 64% / 25% |\n| Summarization | P-Match\u2191 | 84.20% / 95.88% / 78.13% | 86.02% / 92.63% / 77.53% | 93.18% / 99% / 94% | 90.58% / 96% / 90.53% | 85.64% / 95% / 71.88% | 79.94% / 73.03% / 52.08% |\n| Classification | GPT4o | 83.62 / 94% / 81% | 74.45 / 80% / 66% | 87.50 / 92% / 92% | 72.28 / 72% / 66% | 82.68 / 94% / 80% | 63.06 / 67% / 44% |\n| Classification | Claude 3 Sonnet | 82.32 / 92% / 78% | 71.53 / 81% / 70% | 92.89 / 100% / 100% | 75.07 / 81% / 73% | 87.34 / 97% / 97% | 67.18 / 80% / 45% |\n| Classification | Gemini 1.5 Flash | 85.43 / 94% / 91% | 72.73 / 81% / 75% | 89.03 / 94% / 89% | 77.96 / 81% / 81% | 83.35 / 94% / 84% | 64.25 / 78% / 47% |\n| Classification | Gemma 2B | 58.47 / 58% / 42% | 52.76 / 50% / 39% | 69.98 / 73% / 62% | 56.31 / 58% / 47% | 62.17 / 62% / 48% | 48.54 / 50% / 39% |\n| Classification | Gemma 7B | 70.73 / 69% / 55% | 64.67 / 62% / 53% | 78.78 / 81% / 75% | 67.76 / 69% / 62% | 70.73 / 75% / 61% | 59.77 / 59% / 52% |\n| Classification | Mistral 7B | 67.53 / 70% / 53% | 61.65 / 67% / 47% | 76.01 / 80% / 72% | 64.43 / 70% / 52% | 67.90 / 73% / 53% | 54.27 / 53% / 45% |\n| Classification | LLaMA3 8B | 81.64 / 88% / 73% | 78.02 / 77% / 67% | 89.20 / 94% / 94% | 82.18 / 88% / 75% | 83.63 / 94% / 77% | 72.54 / 73% / 64% |\n| Classification | P-Match\u2191 | 95.56% / 93.62% / 80.22% | 104.80% / 95.06% / 89.33% | 96.03% / 94% / 94% | 105.41% / 108.64% / 92.59% | 95.75% / 96.91% / 79.38% | 107.98% / 91.25% / 136.17% |\n| Coding | GPT4o | 90.31 / 100% / 98% | 75.18 / 92% / 70% | 94.57 / 100% / 100% | 86.32 / 100% / 91% | 90.78 / 100% / 100% | 58.43 / 62% / 25% |\n| Coding | Claude 3 Sonnet | 88.76 / 100% / 92% | 75.23 / 94% / 67% | 93.82 / 100% / 100% | 87.42 / 100% / 100% | 89.84 / 100% / 100% | 60.46 / 69% / 31% |\n| Coding | Gemini 1.5 Flash | 88.51 |  | 93.59 / 100% / 100% | 82.92 / 97% / 84% | 90.62 / 100% / 98% | 64.21 / 84% / 41% |\n| Coding | Gemma 2B | / 98% / 94% 62.31 / 70% / 44% | 75.62 / 91% / 73% 56.48 / 66% / 41% | 80.92 / 89% / 84% | 67.24 / 78% / 48% | 72.98 / 89% / 66% | 44.08 / 50% / 8% |\n| Coding | Gemma 7B | 80.56 / 92% / 80% | 71.92 / 89% / 70% | 90.47 / 100% / 98% | 80.26 / 92% / 84% | 84.66 / 100% / 88% | 61.23 / 72% / 36% |\n| Coding | Mistral 7B | / / | / 69% / | 81.25 / 92% / | / 83% | / 86% / | 45.25 / 50% / 8% |\n| Coding | LLaMA3 8B | 68.32 77% 56% 77.47 / 88% / 72% | 61.01 45% | 81% 83.97 / 94% / 83% | 69.10 / 55% 73.51 / 88% / 67% | 72.39 69% | 51.10 / 58% / 17% |\n| Coding | P-Match\u2191 | 89.20% / 92% / | 69.46 / 88% / 61% |  |  | 75.55 / 89% / 73% |  |\n| Coding |  | 81.63% | 95.11% / 94.68% / 95.89% | 95.66% / 100% / | 98% 91.81% / 92% / 84% | 93.26% / 100% / 88% | 95.36% / 85.71% / 97.80% |\n|  | GPT4o | 95.45 / 100% / 100% | 84.23 / 93% / 80% | 97.21 / 100% / 100% | 92.56 / 100% / 97% | 93.58 / 100% / 100% | 75.58 / 85% / 63% |\n| Claude 3 Sonnet | 94.03 / 100% / 98% | 85.28 / 100% / | 82% 97.60 / 100% / 100% | 93.95 / 100% / 100% | 93.66 / 100% / 100% | / 92% / |  |\n| Gemini 1.5 Flash | 94.63 / 100% 97% | 87.43 / 95% / |  |  |  | 76.33 65% |  |\n|  | / |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n|  |  |  |  |  |  |  |  |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1198,
          "y": 0.1759
        },
        {
          "x": 0.8787,
          "y": 0.1759
        },
        {
          "x": 0.8787,
          "y": 0.4568
        },
        {
          "x": 0.1198,
          "y": 0.4568
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "cluding summarization, classification, coding, and\nclosed QA. We utilize GPT4o, Claude 3 Sonnet,\nand Gemini 1.5 Flash as judges to evaluate the\nfine-tuned model performance on test subsets of\nthe coverage dataset. As demonstrated in Table\n1, the fine-tuned local LLMs, despite their signif-\nicantly smaller scale, achieve comparable perfor-\nmance on diverse tasks compared to much larger\nservice LLMs. For instance, in the summarization\ntask, LLaMA3 8B achieved a comparable preci-\nsion score of 87.02 / 99% / 94%, compared to\nGPT4o\u2019s score of 93.25 / 100% / 100%, Claude 3\nSonnet\u2019s score of 93.39 / 100% / 99%, and Gem-\nini 1.5 Flash\u2019s score of 91.95 / 100% / 98%, with\nClaude 3 Sonnet serving as judge. These results\nunderscore the efficacy of LlamaDuo in seamlessly\ntransferring knowledge and capabilities from ser-\nvice LLMs to smaller local LLMs without a sub-\nstantial decrease in performance.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1156,
          "y": 0.4722
        },
        {
          "x": 0.4913,
          "y": 0.4722
        },
        {
          "x": 0.4913,
          "y": 0.7764
        },
        {
          "x": 0.1156,
          "y": 0.7764
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "In Table 1, we observe distinct performance\nacross four fine-tuned models when applied to dif-\nferent tasks. Specifically, Mistral 7B stands out\nin summarization tasks, achieving the best perfor-\nmance in 7 out of 12 cases. Moreover, LLaMA3\n8B consistently outperforms competitors across all\nmetrics and evaluators in the classification task.\nConversely, in coding tasks, Gemma 7B is iden-\ntified as the leading model, excelling across all",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1149,
          "y": 0.7778
        },
        {
          "x": 0.4916,
          "y": 0.7778
        },
        {
          "x": 0.4916,
          "y": 0.9229
        },
        {
          "x": 0.1149,
          "y": 0.9229
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "metrics and evaluations. Mistral 7B shows supe-\nrior performance in the closed QA task, leading\nin 8 out of 12 cases. Within the realm of ser-\nvice LLMs, Claude 3 Sonnet and Gemini 1.5 Flash\ndemonstrate exceptional performance in classifica-\ntion and closed QA tasks, securing the best results\nin 8 and 10 out of 12 cases, respectively. Lastly,\nGPT4o emerges as the leading model in summa-\nrization and coding tasks, achieving the best per-\nformance in 10 and 7 out of 12 cases, respectively.\nNotably, although Gemma 2B exhibits inferior per-\nformance compared to larger 7B models overall,\nthe disparity in results is not markedly substantial,\nwith Gemma 2B attaining closely comparable per-\nformance in certain tasks. For example, in closed\nQA tasks, Gemma 2B secures a mean precision\nscore of 80.22, while Gemma 7B achieves 88.83,\nMistral 7B reaches 88.25, and LLaMA3 8B obtains\n86.03, as evaluated by Claude 3 Sonnet. This ob-\nservation lends further support to the notion that\nthrough the strategic fine-tuning of smaller local\nLLMs on synthetic datasets via the LlamaDuo, it\nis possible to closely approximate the performance\nof their larger counterparts. Consequently, it of-\nfers increased flexibility and solutions for users\nand scenarios with budgetary considerations. More\nexperimental results are presented in Appendix C.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5116,
          "y": 0.4719
        },
        {
          "x": 0.8861,
          "y": 0.4719
        },
        {
          "x": 0.8861,
          "y": 0.9068
        },
        {
          "x": 0.5116,
          "y": 0.9068
        }
      ],
      "id": 4,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250404",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}