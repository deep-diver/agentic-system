{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\n![image](/image/placeholder)\n- Chart Title: Ownen2-7B-Instruct with Chunkov\n- Chart Type: line\n|  | P | M | A | B | C | D | E | F | G | H | I | J | K | L | N | O | Q | R | T | U | V | W | X | Y Z |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n\n\nFigure 15: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on Qwen2-7B-Instruct\n\n![image](/image/placeholder)\n- Chart Title: Index Reuse Performance on GSM8K\n- X-Axis: Index Reuse Depth\n- Y-Axis: GSM8K Score\n- Chart Type: line\n|  | LLaMA-3-8B-Inst | Qwen2-7B-Inst |\n| --- | --- | --- |\n| item_01 | 70 | 0 |\n\n\nFigure 16: GSM8K Performance Comparison with different index reuse layers\n\nTable 13 shows the performance of ChunkKV with different numbers of index reuse layers in GSM8K. The number of\nindex reuse layers is set from 1 to the number of layers in the model, where a index reuse layer of 1 corresponds to\nthe normal ChunkKV without index reuse, and 28/32 is the maximum number of layers for LLaMA-3-8B-Instruct and\nQwen2-7B-Instruct. The significant performance drop of LLaMA-3-8B-Instruct raises another question: whether the KV\ncache compression method is more sensitive to the model\u2019s mathematical reasoning ability.\n\n24",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1716,
          "y": 0.0575
        },
        {
          "x": 0.804,
          "y": 0.0575
        },
        {
          "x": 0.804,
          "y": 0.0717
        },
        {
          "x": 0.1716,
          "y": 0.0717
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: Ownen2-7B-Instruct with Chunkov\n- Chart Type: line\n|  | P | M | A | B | C | D | E | F | G | H | I | J | K | L | N | O | Q | R | T | U | V | W | X | Y Z |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2491,
          "y": 0.0879
        },
        {
          "x": 0.7488,
          "y": 0.0879
        },
        {
          "x": 0.7488,
          "y": 0.4176
        },
        {
          "x": 0.2491,
          "y": 0.4176
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "Figure 15: Layer-wise similarity heatmaps of the preserved KV cache indices by ChunkKV on Qwen2-7B-Instruct",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1089,
          "y": 0.4323
        },
        {
          "x": 0.8671,
          "y": 0.4323
        },
        {
          "x": 0.8671,
          "y": 0.4499
        },
        {
          "x": 0.1089,
          "y": 0.4499
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: Index Reuse Performance on GSM8K\n- X-Axis: Index Reuse Depth\n- Y-Axis: GSM8K Score\n- Chart Type: line\n|  | LLaMA-3-8B-Inst | Qwen2-7B-Inst |\n| --- | --- | --- |\n| item_01 | 70 | 0 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2116,
          "y": 0.4972
        },
        {
          "x": 0.7686,
          "y": 0.4972
        },
        {
          "x": 0.7686,
          "y": 0.7436
        },
        {
          "x": 0.2116,
          "y": 0.7436
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "Figure 16: GSM8K Performance Comparison with different index reuse layers",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2262,
          "y": 0.7595
        },
        {
          "x": 0.7486,
          "y": 0.7595
        },
        {
          "x": 0.7486,
          "y": 0.7769
        },
        {
          "x": 0.2262,
          "y": 0.7769
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 13 shows the performance of ChunkKV with different numbers of index reuse layers in GSM8K. The number of\nindex reuse layers is set from 1 to the number of layers in the model, where a index reuse layer of 1 corresponds to\nthe normal ChunkKV without index reuse, and 28/32 is the maximum number of layers for LLaMA-3-8B-Instruct and\nQwen2-7B-Instruct. The significant performance drop of LLaMA-3-8B-Instruct raises another question: whether the KV\ncache compression method is more sensitive to the model\u2019s mathematical reasoning ability.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0865,
          "y": 0.8307
        },
        {
          "x": 0.8914,
          "y": 0.8307
        },
        {
          "x": 0.8914,
          "y": 0.9081
        },
        {
          "x": 0.0865,
          "y": 0.9081
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "24",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.477,
          "y": 0.9243
        },
        {
          "x": 0.4983,
          "y": 0.9243
        },
        {
          "x": 0.4983,
          "y": 0.9368
        },
        {
          "x": 0.477,
          "y": 0.9368
        }
      ],
      "id": 6,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}