{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent():\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_download_and_parse_paper_agent():\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_paper_analysis_agent():\n",
    "    \"\"\"Use this to analyze only when the contnet of paper(text) is found.\n",
    "Paper content is indicated by \"Retrieved Paper Content\"\n",
    "\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
    ")\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in supervisor_agent.tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_PoWiNSPjypnstlOPWwElOPyj', function=Function(arguments='{}', name='to_paper_search_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Initial trial\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_doRYRQFjppzZzQ5FMKbeKeJV', function=Function(arguments='{}', name='to_download_and_parse_paper_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Second)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n",
    "        \"content\": \"Paper URL: https://arxiv.org/abs/2502.00299\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" addresses the challenge of reducing memory costs during long-context inference with large language models (LLMs). Traditional methods for compressing the key-value (KV) cache often evaluate token importance in isolation, which overlooks the interdependencies inherent in natural language. The authors introduce ChunkKV, a novel approach that clusters tokens into chunks to use as compression units, prioritizing the retention of semantically informative chunks while discarding less crucial ones.\n",
      "\n",
      "The ChunkKV method capitalizes on the observed similarity in the indices preserved across different layers, allowing for layer-wise index reuse to minimize computational demands. The researchers evaluated ChunkKV using state-of-the-art long-context benchmarks such as LongBench and Needle-In-A-Haystack, along with in-context learning benchmarks like GSM8K and JailbreakV. The conducted experiments, involving instruction tuning and multi-step reasoning tasks (O1 and R1), demonstrated up to a 10% performance boost at aggressive compression ratios when compared to existing compression techniques.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Third)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id, \n",
    "        \"content\": \"Retrieved Paper Content\\n\"\n",
    "        \"--------------------------------\\n\"\n",
    "        \"Title: ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\"\n",
    "        \"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the Dummy Tools (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent(paper_title: str):\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "\n",
    "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
    "    headers = {\n",
    "        'X-API-KEY': SERPER_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    search_results = response.json()['organic']\n",
    "    \n",
    "    if len(search_results) == 0:\n",
    "        return \"Count not find the URL to download the paper\"\n",
    "    \n",
    "    first_result = search_results[0]\n",
    "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
    "        return \"Could not find the URL to download the paper\"\n",
    "    \n",
    "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    response = requests.get(paper_url)\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    text = \"Retrieved Paper Content\\n-----------------------------------\\n\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text.strip()[:10000]\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]#, to_paper_analysis_agent]#, to_triage, to_end_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(client, messages, supervisor_agent):\n",
    "    # Loop through the conversation steps\n",
    "    while True:\n",
    "        # Prepare tools for the current step\n",
    "        tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in supervisor_agent.tools}\n",
    "        \n",
    "        # Get model response\n",
    "        response = client.chat.completions.create(\n",
    "            model=supervisor_agent.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            print(response.choices[0].message.tool_calls)\n",
    "        else:\n",
    "            print(\"--------------------------------\")\n",
    "            print(response.choices[0].message.content)\n",
    "            print(\"--------------------------------\")\n",
    "            break\n",
    "        \n",
    "        # Add model response to messages\n",
    "        messages.append(response.choices[0].message)\n",
    "        \n",
    "        # Add tool response to messages\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_response = execute_tool_call(tool_call, tools, supervisor_agent.name)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\", \n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"content\": tool_response\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_a7iOBDwap3BKsI42WVqwlSBm', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='call_cVrZIdsNWb1LQwLyoqa5vxoE', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" presents a novel method to address the memory constraints in Large Language Models (LLMs) when inferring long contexts. The work primarily focuses on the key-value (KV) cache used during token processing, highlighting that traditional compression methods overlook the dependency between tokens and only assess their importance individually, potentially leading to a loss of crucial semantic information.\n",
      "\n",
      "To address this, the authors propose ChunkKV, a method that compresses the KV cache by grouping tokens into chunks and only preserving the most semantically informative ones. This approach ensures that the complete semantic information, typically appearing in continuous sequences, is retained. The method is shown to better capture the relationships among subjects, predicates, and objects by maintaining contextual integrity, compared to previous methods which might overly focus on discrete, isolated tokens.\n",
      "\n",
      "Additionally, ChunkKV introduces a layer-wise index reuse strategy, which further decreases computational overhead by reusing similar preserved indices across different layers. The authors evaluate ChunkKV on several cutting-edge benchmarks such as LongBench and Needle-In-A-HayStack, alongside in-context learning tasks like GSM8K and JailbreakV. The results show that ChunkKV offers up to a 10% improvement in performance under aggressive compression scenarios compared with existing methods.\n",
      "\n",
      "Overall, the paper suggests that the proposed ChunkKV method significantly enhances efficiency and accuracy in LLM inference by maintaining the semantic integrity of the KV cache while reducing memory requirements.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the same with Upstage's Solar-Pro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='5b2c1208-db8d-4410-b7b8-6be146b00008', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='bab6f011-f4af-4d00-bd40-fa9e0f46f737', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper titled 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' is a comprehensive study on the optimization of Large Language Models (LLMs) for long-context inference. The authors propose a simple yet effective KV cache compression method called ChunkKV, which uses the fragmentation method to retain essential information through selective chunk retention. They also propose the layer-wise index reuse technique to reduce the additional computational time introduced by the KV cache compression method. The authors evaluate ChunkKV on cutting-edge long-context benchmarks and multi-step reasoning LLMs, achieving state-of-the-art performance. The paper is a significant contribution to the field of KV cache compression for LLMs and provides valuable insights for future research.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent.model = \"solar-pro\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]\n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
