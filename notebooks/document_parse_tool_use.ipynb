{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func) -> dict:\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\",\n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent():\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_download_and_parse_paper_agent():\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_paper_analysis_agent():\n",
    "    \"\"\"Use this to analyze only when the contnet of paper(text) is found.\n",
    "Paper content is indicated by \"Retrieved Paper Content\"\n",
    "\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
    ")\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in supervisor_agent.tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_DBx9Z4YlhZMaSDHEdB0JB5b9', function=Function(arguments='{}', name='to_paper_search_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Initial trial\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_QN28xcbOxhNPwACXdMsUb06B', function=Function(arguments='{}', name='to_download_and_parse_paper_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Second)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n",
    "        \"content\": \"Paper URL: https://arxiv.org/abs/2502.00299\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" introduces a novel method, ChunkKV, to reduce memory costs associated with long-context inference in Large Language Models (LLMs). Previous methods focused on compressing key-value (KV) caches based on individual token importance, but they often overlooked the interaction and dependency among different tokens in natural language. \n",
      "\n",
      "ChunkKV addresses this by grouping tokens into chunks, treating each chunk as a unit for compression. This approach allows ChunkKV to preserve the most informative semantic chunks while discarding less important ones. The method also introduces layer-wise index reuse, leveraging the observed similarity in preserved indices across different layers to further decrease computational costs.\n",
      "\n",
      "The efficacy of ChunkKV was tested on several benchmarks, including LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV in-context learning benchmarks. The results demonstrated up to a 10% performance improvement under aggressive compression ratios compared to existing methods, showing promise for more efficient and effective long-context LLM inference.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Third)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id, \n",
    "        \"content\": \"Retrieved Paper Content\\n\"\n",
    "        \"--------------------------------\\n\"\n",
    "        \"Title: ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\"\n",
    "        \"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the Dummy Tools (Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent(paper_title: str):\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "\n",
    "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
    "    headers = {\n",
    "        'X-API-KEY': SERPER_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    search_results = response.json()['organic']\n",
    "    \n",
    "    if len(search_results) == 0:\n",
    "        return \"Count not find the URL to download the paper\"\n",
    "    \n",
    "    first_result = search_results[0]\n",
    "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
    "        return \"Could not find the URL to download the paper\"\n",
    "    \n",
    "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    response = requests.get(paper_url)\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    text = \"Retrieved Paper Content\\n-----------------------------------\\n\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text.strip()[:10000]\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]#, to_paper_analysis_agent]#, to_triage, to_end_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(client, messages, supervisor_agent):\n",
    "    # Loop through the conversation steps\n",
    "    while True:\n",
    "        # Prepare tools for the current step\n",
    "        tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in supervisor_agent.tools}\n",
    "        \n",
    "        # Get model response\n",
    "        response = client.chat.completions.create(\n",
    "            model=supervisor_agent.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            print(response.choices[0].message.tool_calls)\n",
    "        else:\n",
    "            print(\"--------------------------------\")\n",
    "            print(response.choices[0].message.content)\n",
    "            print(\"--------------------------------\")\n",
    "            break\n",
    "        \n",
    "        # Add model response to messages\n",
    "        messages.append(response.choices[0].message)\n",
    "        \n",
    "        # Add tool response to messages\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_response = execute_tool_call(tool_call, tools, supervisor_agent.name)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\", \n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"content\": tool_response\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_td4d3gSzmIjXLckS2b5m0D8z', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='call_zg8EX15mTG0LBnGf4Op3EcrA', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper titled \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" presents a new method for reducing memory costs in long-context inference with Large Language Models (LLMs). Here’s a comprehensive summary of the paper:\n",
      "\n",
      "### Introduction\n",
      "- Large Language Models (LLMs) are critical for processing natural language tasks that require interpreting long contexts. These tasks sometimes involve tens of thousands of tokens.\n",
      "- A significant challenge in using LLMs for long-context processing is the GPU memory requirements, primarily due to the key-value (KV) cache.\n",
      "- Existing approaches to address memory consumption in KV caching focus on compressing the cache by pruning non-important parts of the prompt tokens.\n",
      "\n",
      "### Main Contributions\n",
      "1. **ChunkKV Method**: \n",
      "   - Introduces a novel compression method by grouping tokens into chunks instead of measuring token importance individually, which traditional methods do.\n",
      "   - This approach retains the most informative semantic chunks while discarding less important ones, effectively reducing memory usage without losing crucial semantic information.\n",
      "\n",
      "2. **Layer-Wise Index Reuse**:\n",
      "   - Observes that ChunkKV exhibits higher similarity in the preserved indices across different layers, leading to the development of the layer-wise index reuse technique.\n",
      "   - This technique minimizes the additional computational time typically introduced by KV cache compression.\n",
      "\n",
      "3. **Evaluation and Results**:\n",
      "   - Evaluated ChunkKV on several benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV.\n",
      "   - Demonstrated up to 10% performance improvement under aggressive compression ratios compared to existing methods.\n",
      "\n",
      "### Methodology\n",
      "- Instead of focusing on discrete tokens, ChunkKV takes into account the continuity and dependencies in language, maintaining complete chunks to preserve semantic integrity.\n",
      "- The paper highlights the constraints of prior methods which often miss out on essential semantic information due to isolated token importance measurement.\n",
      "\n",
      "### Experiments and Findings\n",
      "- Conducted detailed experiments using various benchmarks and different models.\n",
      "- Found that ChunkKV outperforms existing methods by effectively preserving semantic information while efficiently reducing computational overhead.\n",
      "\n",
      "### Conclusion\n",
      "- ChunkKV provides a straightforward solution to the KV cache compression problem by focusing on semantic preservation.\n",
      "- It successfully reduces memory use while maintaining, or even improving, model performance.\n",
      "\n",
      "Overall, the paper proposes a simple yet effective approach to KV cache compression, emphasizing the semantic importance of contiguous token chunks, which enhances the efficiency of long-context LLM inference.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the same with Upstage's Solar-Pro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='3e45e34a-a053-4b8a-9425-2fb99aa4ff95', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='82640c4a-0263-4b0f-b646-2d52a50080ec', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' presents a novel approach to KV cache compression that aims to preserve semantic information in the cache. The authors propose a method called ChunkKV, which groups tokens into chunks and retains the most informative semantic chunks while discarding the less important ones. They also introduce a layer-wise index reuse technique to further reduce computational overhead. The paper evaluates ChunkKV on various benchmarks and demonstrates its superior performance over existing methods. The key contributions of the paper include the identification of the phenomenon in which discrete KV cache compression methods inadvertently prune necessary semantic information, the proposal of ChunkKV as a simple yet effective KV cache compression method, and the development of the layer-wise index reuse technique to reduce additional computational time.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent.model = \"solar-pro\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]\n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Document Parse as a Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin/:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tokens_if_needed(tokenizer, messages, content, max_token_limit=32000):\n",
    "    \"\"\"\n",
    "    Truncate the markdown content if the total tokens exceed the maximum limit.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding/decoding\n",
    "        messages: List of message dictionaries for the conversation\n",
    "        content: The markdown content to potentially truncate\n",
    "        max_token_limit: Maximum token limit (default: 32000)\n",
    "        \n",
    "    Returns:\n",
    "        truncated_markdown: The potentially truncated markdown\n",
    "        base_token_numbers: Number of tokens in the base conversation\n",
    "        paper_token_numbers: Number of tokens in the paper after potential truncation\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": supervisor_agent.instructions}\n",
    "        ] + messages\n",
    "    )\n",
    "    base_token_numbers = len(inputs)\n",
    "    encoded_content = tokenizer.encode(content)\n",
    "    paper_token_numbers = len(encoded_content)\n",
    "\n",
    "    print(f\"Base token numbers: {base_token_numbers}\")\n",
    "    print(f\"Paper token numbers: {paper_token_numbers}\")\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "\n",
    "    total_token_numbers = base_token_numbers + paper_token_numbers\n",
    "\n",
    "    if total_token_numbers > max_token_limit:\n",
    "        # Calculate how many tokens we need to truncate\n",
    "        tokens_to_keep = max_token_limit - base_token_numbers\n",
    "        # Truncate the encoded markdown\n",
    "        encoded_content = encoded_content[:tokens_to_keep]\n",
    "        # Update the paper token count\n",
    "        paper_token_numbers = len(encoded_content)\n",
    "        # Update the markdown string by decoding the truncated tokens\n",
    "        truncated_content = tokenizer.decode(encoded_content)\n",
    "        print(f\"Truncated paper tokens to: {paper_token_numbers}\")\n",
    "    else:\n",
    "        print(\"No truncation needed\")\n",
    "        truncated_content = content\n",
    "\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "    return truncated_content, base_token_numbers, paper_token_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
    "message_template = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper below, \\n\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
    "    # Open the PDF\n",
    "    pdf = PdfReader(input_pdf_path)\n",
    "    total_pages = len(pdf.pages)\n",
    "    \n",
    "    # Calculate number of output PDFs needed\n",
    "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
    "    \n",
    "    output_paths = []\n",
    "    \n",
    "    # Create output directory using input filename\n",
    "    base_name = os.path.splitext(input_pdf_path)[0]\n",
    "    os.makedirs(base_name, exist_ok=True)\n",
    "    \n",
    "    # Split into multiple PDFs\n",
    "    for i in range(num_pdfs):\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Calculate start and end pages for this split\n",
    "        start_page = i * pages_per_pdf\n",
    "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
    "        \n",
    "        # Add pages to writer\n",
    "        for page_num in range(start_page, end_page):\n",
    "            writer.add_page(pdf.pages[page_num])\n",
    "            \n",
    "        # Save the split PDF\n",
    "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
    "        with open(output_path, \"wb\") as output_file:\n",
    "            writer.write(output_file)\n",
    "        output_paths.append(output_path)\n",
    "        \n",
    "    return output_paths\n",
    "\n",
    "def get_document_parse_response(filename, api_key):\n",
    "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    files = {\"document\": open(filename, \"rb\")}\n",
    "    data = {\"output_formats\": \"['markdown']\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    upstage_response = json.loads(response.text)\n",
    "    return upstage_response\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    response = requests.get(paper_url)\n",
    "    # Save the PDF to a temporary file\n",
    "    root_path = \"tmp\"\n",
    "    temp_pdf_path = \"temp_paper.pdf\"\n",
    "    with open(temp_pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    shutil.rmtree(root_path, ignore_errors=True)\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "\n",
    "    split_factor = 1\n",
    "    split_pdfs = split_pdf_by_pages(temp_pdf_path, root_path, split_factor) # by 10\n",
    "\n",
    "    markdown = \"\"\n",
    "    total_responses = []\n",
    "    for i, split_pdf in enumerate(split_pdfs):\n",
    "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
    "        \n",
    "        # Append the response to the total_responses list\n",
    "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})        \n",
    "        # Also write the response to a JSON file for persistence\n",
    "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
    "        with open(json_output_path, \"w\") as json_file:\n",
    "            json.dump(upstage_response, json_file, indent=2)\n",
    "\n",
    "        try:\n",
    "            markdown += upstage_response['content']['markdown']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    markdown = \"Retrieved Paper Content\\n-----------------------------------\\n\" + markdown\n",
    "    markdown, _, _ = truncate_tokens_if_needed(tokenizer, message_template, markdown)\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]#, to_paper_analysis_agent]#, to_triage, to_end_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='9521a151-6c08-463b-a377-2e2c0df1563d', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='f85864f5-f429-46dd-a900-cc97f7773ed8', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (53672 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base token numbers: 115\n",
      "Paper token numbers: 53672\n",
      "Total token numbers: 53787\n",
      "Truncated paper tokens to: 31885\n",
      "Total token numbers: 32000\n",
      "--------------------------------\n",
      "The assistant summarizes the key findings from the experiments and analysis conducted on the ChunkKV method, which is a novel KV cache compression technique for efficient long-context inference in large language models (LLMs). The method retains the most informative semantic chunks from the original KV cache, leading to improved performance compared to existing methods. The experiments were conducted on various LLMs and benchmarks, demonstrating the effectiveness of ChunkKV in preserving essential contextual information for complex reasoning tasks, long-context understanding, and safety evaluations. The method’s chunk-based approach maintains crucial contextual information, leading to superior performance in challenging scenarios and benchmarks. The proposed layer-wise index reuse technique provides significant computational efficiency gains with minimal performance impact, achieving up to a 20.7% reduction in latency and\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent.model = \"solar-pro\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]\n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
