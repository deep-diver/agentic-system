{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/agentic-system/blob/main/notebooks/document_parse_tool_use_w_vdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-txBK6TNkA9"
      },
      "source": [
        "# Document Parse as a Tool w/ LLMs and Vector Database\n",
        "\n",
        "In this notebook, we are going to dive into the tutorial how we can cache already downloaded paper's content for the next question. This tutorial consists of two parts:\n",
        "\n",
        "- File system based caching: [Upstage's Documet Parse](https://www.upstage.ai/products/document-parse) returns the parsed information of a given document in JSON format. This approach simply saves that JSON files into file system, then reuse it when someone asks about it.\n",
        "\n",
        "- Vector Database based caching: Instead of simple apporach of using file system based caching, this **Vector Database based caching** chops the document off into multiple pieces, calculate embedding vectors or each piece via [Upstages' embedding API](https://console.upstage.ai/docs/capabilities/embeddings), then store the embedding vectors into vector database. When someone makes a query about this information, we are going to calculate the embedding vector of that query, then retrieve the top K most relevant pieces of information from the vector databse.\n",
        "\n",
        "> â“ In this tutorial, we are going to use [ChromaDB](https://www.trychroma.com/) which is an open source vector database solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_THNLaw5NkA9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import inspect\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "import PyPDF2\n",
        "from openai import OpenAI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n8EVIx0NkA-"
      },
      "outputs": [],
      "source": [
        "# Run this cell if you are using macos\n",
        "\n",
        "import os\n",
        "os.environ[\"PATH\"] = \"/opt/homebrew/bin/:\" + os.environ[\"PATH\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbhwJE4NkA-"
      },
      "source": [
        "## Setting up the API Keys\n",
        "\n",
        "Once `load_dotenv()` is called successfully, you will see `True` is returned and printed out. At this point, all the variables from `.env` file is loaded up as environment variable. Hence, you can access them with `os.getenv()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-79S7777NkA-",
        "outputId": "d33e0199-09a2-4a30-8353-314c1b993e34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73152PNNkA-"
      },
      "source": [
        "Basically, we are going to need the following three API keys:\n",
        "- `SERPER_API_KEY`: API key for Google Search API service from [Serper.dev](https://serper.dev/)\n",
        "- `UPSTAGE_API_KEY`: API key for accessing [Upstage](https://www.upstage.ai/)'s Solar models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54slr1cgNkA-"
      },
      "outputs": [],
      "source": [
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
        "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld86fvYdNkA-"
      },
      "source": [
        "## Defining Helper Functions\n",
        "\n",
        "Here, we are defining two helper functions necessary for tool(function) calling:\n",
        "- `function_to_schema()`: Converts a Python function into a JSON schema format that can be used for function calling with LLMs. This can be thought as just structured string that is going to be injected into LLM as context so that LLM can understand what kind of functions are available to call\n",
        "- `execute_tool_call()`: Executes the function call based on the LLM's response and returns the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB2A0WUFNkA-"
      },
      "outputs": [],
      "source": [
        "def function_to_schema(func) -> dict:\n",
        "    \"\"\"\n",
        "    Converts a Python function into a JSON schema format for LLM function calling.\n",
        "\n",
        "    Args:\n",
        "        func: The Python function to convert to schema\n",
        "\n",
        "    Returns:\n",
        "        dict: JSON schema describing the function's interface\n",
        "\n",
        "    The schema includes:\n",
        "    - Function name\n",
        "    - Description from docstring\n",
        "    - Parameters with their types\n",
        "    - Required parameters list\n",
        "    \"\"\"\n",
        "    # Map Python types to JSON schema types\n",
        "    type_map = {\n",
        "        str: \"string\",\n",
        "        int: \"integer\",\n",
        "        float: \"number\",\n",
        "        bool: \"boolean\",\n",
        "        list: \"array\",\n",
        "        dict: \"object\",\n",
        "        type(None): \"null\",\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Get function signature using inspect\n",
        "        signature = inspect.signature(func)\n",
        "    except ValueError as e:\n",
        "        raise ValueError(\n",
        "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
        "        )\n",
        "\n",
        "    # Build parameters dictionary\n",
        "    parameters = {}\n",
        "    for param in signature.parameters.values():\n",
        "        try:\n",
        "            # Get JSON type for parameter, default to string if type not found\n",
        "            param_type = type_map.get(param.annotation, \"string\")\n",
        "        except KeyError as e:\n",
        "            raise KeyError(\n",
        "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
        "            )\n",
        "        parameters[param.name] = {\"type\": param_type}\n",
        "\n",
        "    # Get list of required parameters (those without default values)\n",
        "    required = [\n",
        "        param.name\n",
        "        for param in signature.parameters.values()\n",
        "        if param.default == inspect._empty\n",
        "    ]\n",
        "\n",
        "    # Return complete schema\n",
        "    return {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": func.__name__,\n",
        "            \"description\": (func.__doc__ or \"\").strip(),\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": parameters,\n",
        "                \"required\": required,\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "def execute_tool_call(tool_call, tools, agent_name):\n",
        "    \"\"\"\n",
        "    Executes a function call based on the LLM's response.\n",
        "\n",
        "    Args:\n",
        "        tool_call: Object containing function call details from LLM\n",
        "        tools: Dictionary mapping function names to actual functions\n",
        "        agent_name: Name of the agent making the call, for logging\n",
        "\n",
        "    Returns:\n",
        "        The result of executing the specified function with given arguments\n",
        "\n",
        "    This function:\n",
        "    1. Extracts function name and arguments from tool_call\n",
        "    2. Logs the function call\n",
        "    3. Executes the function with provided arguments\n",
        "    \"\"\"\n",
        "    # Extract function name and parse arguments from JSON\n",
        "    name = tool_call.function.name\n",
        "    args = json.loads(tool_call.function.arguments)\n",
        "\n",
        "    # Log the function call\n",
        "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
        "\n",
        "    # Execute the function with unpacked arguments\n",
        "    return tools[name](**args)  # call corresponding function with provided arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqMT9VKtNkA_"
      },
      "outputs": [],
      "source": [
        "def truncate_tokens_if_needed(tokenizer, messages, content, max_token_limit=32000):\n",
        "    \"\"\"\n",
        "    Truncate the markdown content if the total tokens exceed the maximum limit.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: The tokenizer to use for encoding/decoding\n",
        "        messages: List of message dictionaries for the conversation\n",
        "        content: The markdown content to potentially truncate\n",
        "        max_token_limit: Maximum token limit (default: 32000)\n",
        "\n",
        "    Returns:\n",
        "        truncated_markdown: The potentially truncated markdown\n",
        "        base_token_numbers: Number of tokens in the base conversation\n",
        "        paper_token_numbers: Number of tokens in the paper after potential truncation\n",
        "    \"\"\"\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": supervisor_agent.instructions}\n",
        "        ] + messages\n",
        "    )\n",
        "    base_token_numbers = len(inputs)\n",
        "    encoded_content = tokenizer.encode(content)\n",
        "    paper_token_numbers = len(encoded_content)\n",
        "\n",
        "    print(f\"Base token numbers: {base_token_numbers}\")\n",
        "    print(f\"Paper token numbers: {paper_token_numbers}\")\n",
        "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
        "\n",
        "    total_token_numbers = base_token_numbers + paper_token_numbers\n",
        "\n",
        "    if total_token_numbers > max_token_limit:\n",
        "        # Calculate how many tokens we need to truncate\n",
        "        tokens_to_keep = max_token_limit - base_token_numbers\n",
        "        # Truncate the encoded markdown\n",
        "        encoded_content = encoded_content[:tokens_to_keep]\n",
        "        # Update the paper token count\n",
        "        paper_token_numbers = len(encoded_content)\n",
        "        # Update the markdown string by decoding the truncated tokens\n",
        "        truncated_content = tokenizer.decode(encoded_content, skip_special_tokens=True)\n",
        "        print(f\"Truncated paper tokens to: {paper_token_numbers}\")\n",
        "    else:\n",
        "        print(\"No truncation needed\")\n",
        "        truncated_content = content\n",
        "\n",
        "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
        "    return truncated_content, base_token_numbers, paper_token_numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5c-1nS1NkA_"
      },
      "source": [
        "## Filesystem based Caching\n",
        "\n",
        "This is pretty much the same as the previous [notebook](https://github.com/deep-diver/agentic-system/blob/main/notebooks/document_parse_tool_use.ipynb) except we have updated version of `to_download_and_parse_paper_agent()` function. In the updated version, we have a sort of routing logic to determine whether of not to reuse cached contents by simply looking up if such information is in the filesystem.\n",
        "\n",
        "Consider the following code snippet:\n",
        "```python\n",
        "if os.path.exists(root_path):\n",
        "    print(f\"Found cached markdown for {paper_id}\")\n",
        "    markdown = get_md_from_fs(root_path)\n",
        "else:\n",
        "    print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "    os.makedirs(root_path, exist_ok=True)\n",
        "    markdown = get_md_with_document_parse(root_path, paper_url)\n",
        "```\n",
        "\n",
        "Here, `root_path` is simply the ID of each paper following the same ID as in arXiv. Hence, we can just check if `root_path` has been created before. If so, we grasp all the JSON files under the `root_path`. Otherwise, we create `root_path`, then parse the document using Upstage's Document Parse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvsMJk8gNkA_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import shutil\n",
        "import requests\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
        "message_template = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a comprehensive summary of the paper below, \\n\"\n",
        "    },\n",
        "]\n",
        "\n",
        "def to_paper_search_agent(paper_title: str):\n",
        "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
        "    headers = {\n",
        "        'X-API-KEY': SERPER_API_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    search_results = response.json()['organic']\n",
        "\n",
        "    if len(search_results) == 0:\n",
        "        return \"Count not find the URL to download the paper\"\n",
        "\n",
        "    first_result = search_results[0]\n",
        "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
        "        return \"Could not find the URL to download the paper\"\n",
        "\n",
        "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
        "\n",
        "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
        "    # Open the PDF\n",
        "    pdf = PdfReader(input_pdf_path)\n",
        "    total_pages = len(pdf.pages)\n",
        "\n",
        "    # Calculate number of output PDFs needed\n",
        "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
        "\n",
        "    output_paths = []\n",
        "\n",
        "    # Split into multiple PDFs\n",
        "    for i in range(num_pdfs):\n",
        "        writer = PdfWriter()\n",
        "\n",
        "        # Calculate start and end pages for this split\n",
        "        start_page = i * pages_per_pdf\n",
        "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
        "\n",
        "        # Add pages to writer\n",
        "        for page_num in range(start_page, end_page):\n",
        "            writer.add_page(pdf.pages[page_num])\n",
        "\n",
        "        # Save the split PDF\n",
        "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
        "        with open(output_path, \"wb\") as output_file:\n",
        "            writer.write(output_file)\n",
        "        output_paths.append(output_path)\n",
        "\n",
        "    return output_paths\n",
        "\n",
        "def get_document_parse_response(filename, api_key):\n",
        "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    files = {\"document\": open(filename, \"rb\")}\n",
        "    data = {\"output_formats\": \"['markdown']\"}\n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files, data=data)\n",
        "    upstage_response = json.loads(response.text)\n",
        "    return upstage_response\n",
        "\n",
        "def get_md_with_document_parse(root_path, paper_url):\n",
        "    response = requests.get(paper_url)\n",
        "    # Save the PDF to a temporary file\n",
        "\n",
        "    pdf_path = f\"{root_path}/paper.pdf\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    split_factor = 1\n",
        "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
        "\n",
        "    markdown = \"\"\n",
        "    total_responses = []\n",
        "    for i, split_pdf in enumerate(split_pdfs):\n",
        "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
        "\n",
        "        # Append the response to the total_responses list\n",
        "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})\n",
        "        # Also write the response to a JSON file for persistence\n",
        "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
        "        with open(json_output_path, \"w\") as json_file:\n",
        "            json.dump(upstage_response, json_file, indent=2)\n",
        "\n",
        "        try:\n",
        "            markdown += upstage_response['content']['markdown']\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    return markdown\n",
        "\n",
        "def get_md_from_fs(root_path):\n",
        "    markdown = \"\"\n",
        "    for file in os.listdir(root_path):\n",
        "        if file.endswith(\".json\"):\n",
        "            with open(os.path.join(root_path, file), \"r\") as f:\n",
        "                upstage_response = json.load(f)\n",
        "                markdown += upstage_response['content']['markdown']\n",
        "    return markdown\n",
        "\n",
        "def to_download_and_parse_paper_agent(paper_url: str):\n",
        "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
        "    paper_id = paper_url.split(\"/\")[-1]\n",
        "    root_path = paper_id\n",
        "\n",
        "    if os.path.exists(root_path):\n",
        "        print(f\"Found cached markdown for {paper_id}\")\n",
        "        markdown = get_md_from_fs(root_path)\n",
        "    else:\n",
        "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "        os.makedirs(root_path, exist_ok=True)\n",
        "        markdown = get_md_with_document_parse(root_path, paper_url)\n",
        "\n",
        "    markdown = \"Retrieved Paper Content\\n-----------------------------------\\n\" + markdown\n",
        "    markdown, _, _ = truncate_tokens_if_needed(tokenizer, message_template, markdown)\n",
        "    return markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxYL5FdNQ6uY"
      },
      "source": [
        "Let's run the same Triage Agentic System from the previous notebook with updated functions. Below, we are going to use exactly the same code snippets. We just included them to let people clearly see end to end workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9G96PErwNkA_"
      },
      "outputs": [],
      "source": [
        "class Agent(BaseModel):\n",
        "    name: str = \"Agent\"\n",
        "    model: str = \"solar-pro\"\n",
        "    instructions: str = \"You are a helpful Agent\"\n",
        "    tools: list = []\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.upstage.ai/v1\",\n",
        "    api_key=UPSTAGE_API_KEY\n",
        ")\n",
        "\n",
        "supervisor_agent = Agent(\n",
        "    name=\"Supervisor Agent\",\n",
        "    instructions=(\n",
        "        \"You are a academic paper analyzer. \"\n",
        "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
        "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
        "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
        "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
        "    ),\n",
        "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkBVgo1dNkA_"
      },
      "outputs": [],
      "source": [
        "def run(client, messages, supervisor_agent):\n",
        "    # Loop through the conversation steps\n",
        "    while True:\n",
        "        # Prepare tools for the current step\n",
        "        tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
        "        tools = {tool.__name__: tool for tool in supervisor_agent.tools}\n",
        "\n",
        "        # Get model response\n",
        "        response = client.chat.completions.create(\n",
        "            model=supervisor_agent.model,\n",
        "            messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
        "            tools=tool_schemas or None,\n",
        "            tool_choice=\"auto\",\n",
        "        )\n",
        "\n",
        "        if response.choices[0].message.tool_calls:\n",
        "            print(response.choices[0].message.tool_calls)\n",
        "        else:\n",
        "            print(\"--------------------------------\")\n",
        "            print(response.choices[0].message.content)\n",
        "            print(\"--------------------------------\")\n",
        "            break # escape the loop when there is no need for tool(function) call anymore\n",
        "\n",
        "        # Add model response to messages\n",
        "        messages.append(response.choices[0].message)\n",
        "\n",
        "        # Add tool response to messages\n",
        "        if response.choices[0].message.tool_calls:\n",
        "            for tool_call in response.choices[0].message.tool_calls:\n",
        "                tool_response = execute_tool_call(tool_call, tools, supervisor_agent.name)\n",
        "\n",
        "                messages.append({\n",
        "                    \"role\": \"tool\",\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"content\": tool_response\n",
        "                })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMQWPh5xRS6f"
      },
      "source": [
        "Let's run the first interaction. Since this is the first interaction with the LLM Agent, we should see something like \"No cache...\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Wvm0PGyNkA_",
        "outputId": "542be98b-25b8-402e-accc-2b00cfa3c7fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ChatCompletionMessageToolCall(id='10c7b285-2f29-41a1-9714-b7ab7f9ca486', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
            "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
            "[ChatCompletionMessageToolCall(id='1cc34b5e-0822-4e7d-8991-cf0fba0aa44e', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
            "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
            "No cached markdown found for 2502.00299, parsing from URL\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (57243 > 4096). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base token numbers: 115\n",
            "Paper token numbers: 57243\n",
            "Total token numbers: 57358\n",
            "Truncated paper tokens to: 31885\n",
            "Total token numbers: 32000\n",
            "--------------------------------\n",
            "The assistant should now reach the point in the conversation where it can provide a final answer based on the generated output of the function call.\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
        "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
        "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
        "    },\n",
        "]\n",
        "\n",
        "run(client, messages, supervisor_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nk5PlKoRjFu"
      },
      "source": [
        "As you can see from the output of the previous cell above, there is a message `No cached markdown fount for 2502.00299...`, then it calls Upstage's Document Parse API to parse that specific paper.\n",
        "\n",
        "At this point, the parsed information is stored in the file system for later use. Let's check if that works in the following code cell. To make things as much simple as possible, we simply throw the same user message to the Agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlSrzy3XNkA_",
        "outputId": "c11a4ef5-6846-4c78-e214-2210afb896ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ChatCompletionMessageToolCall(id='3f6464ec-120f-41a0-8822-1a6db6c7ae82', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
            "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
            "[ChatCompletionMessageToolCall(id='a17098c5-3cbb-4393-88f0-a7908540bbb6', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
            "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
            "Found cached markdown for 2502.00299\n",
            "Base token numbers: 115\n",
            "Paper token numbers: 57243\n",
            "Total token numbers: 57358\n",
            "Truncated paper tokens to: 31885\n",
            "Total token numbers: 32000\n",
            "--------------------------------\n",
            "ChunkKV is a simple yet effective KV cache compression method that retains essential information through selective chunk retention. It achieves better performance than existing KV cache compression methods by preserving semantic information and efficiently reusing indices. In experiments across various long-context benchmarks, including LongBench and Needle-In-A-HayStack, as well as GSM8K and JailbreakV in-context learning benchmarks, ChunkKV consistently outperforms other methods. This establishes ChunkKV as a promising approach for maintaining crucial information in the KV cache.\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
        "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
        "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
        "    },\n",
        "]\n",
        "\n",
        "run(client, messages, supervisor_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I1BEwBsSCZb"
      },
      "source": [
        "As you can see from the output of the previous cell above, there is a message `Found cached markdown for 2502.00299`, then it doesn't have to call Upstage's Document Parse API. Rather, it can simply grasp all the JSON files produced from the last time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QUphKz9NkA_"
      },
      "source": [
        "## Vector Database based Caching\n",
        "\n",
        "For vector database integration, we are going to use ChromaDB which is an open source solution to vector database. The approach how this notebook leverages vectot database follows the below steps:\n",
        "\n",
        "1. When a new paper is requested, we use Upstage's Document Parse to parse the paper document.\n",
        "2. We create a new collection in the vector database with the name of paper ID (identical to arXiv ID).\n",
        "3. We chop the document off into the size of context window that Upstage's embedding model allows. As of writing this tutorial, it is about 4,000 (4K). We call a single piece of 4K a `chunk`.\n",
        "4. We calculates the embedding vectors of all the chunks via Upstage's embedding model, then we add them to the designated collection in the vector database.\n",
        "\n",
        "When user makes queries about the papers that are already cached in the vector database, the agent system simply retrieve the collection of the ID, then retrieve the top-k relevant pieces of chunks. Those chunks will be injected into the LLM as context to answer the user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSZZ-DXtNkA_"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3z64N5UL52p"
      },
      "source": [
        "When using ChromaDB with a custom / unsupported embedding models, you explicitly need to implement a class that inherits `chromadb.EmbeddingFunction` class. It's basically just calling APIs or functions to get embedding vectors calculated. We have implemented such class named as `UpstageEmbeddingFunction`.\n",
        "\n",
        "```python\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model_name: str = \"embedding-query\",\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
        "\n",
        "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
        "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
        "        return np.array(passage_embedding_list, dtype=np.float32)\n",
        "```\n",
        "\n",
        "Once you have `UpstageEmbeddingFunction`, all you need to do is simply calling the `add()` method on the target collection.\n",
        "\n",
        "```python\n",
        "processed_input = []\n",
        "if len(markdown) > embedding_context_length:\n",
        "    chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
        "    processed_input.extend(chunks)\n",
        "else:\n",
        "    processed_input.append(markdown)\n",
        "\n",
        "ids = []\n",
        "for i in range(len(processed_input)):\n",
        "    ids.append(f\"{paper_id}_{i}\")\n",
        "\n",
        "collection.add(documents=processed_input, ids=ids)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqF6ogo2NkA_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import chromadb\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "embedding_context_length = 4000\n",
        "\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model_name: str = \"embedding-query\",\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
        "\n",
        "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
        "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
        "        return np.array(passage_embedding_list, dtype=np.float32)\n",
        "\n",
        "embedding_fn = UpstageEmbeddingFunction(client)\n",
        "\n",
        "def get_md_with_document_parse(root_path, paper_url, paper_id):\n",
        "    response = requests.get(paper_url)\n",
        "    # Save the PDF to a temporary file\n",
        "\n",
        "    pdf_path = f\"{root_path}/paper.pdf\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    split_factor = 1\n",
        "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
        "\n",
        "    markdown = \"\"\n",
        "    total_responses = []\n",
        "    for i, split_pdf in enumerate(split_pdfs):\n",
        "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
        "\n",
        "        # Append the response to the total_responses list\n",
        "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})\n",
        "        # Also write the response to a JSON file for persistence\n",
        "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
        "        with open(json_output_path, \"w\") as json_file:\n",
        "            json.dump(upstage_response, json_file, indent=2)\n",
        "\n",
        "        try:\n",
        "            markdown += upstage_response['content']['markdown']\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    collection = chroma_client.create_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "\n",
        "    processed_input = []\n",
        "    if len(markdown) > embedding_context_length:\n",
        "        chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
        "        processed_input.extend(chunks)\n",
        "    else:\n",
        "        processed_input.append(markdown)\n",
        "\n",
        "    ids = []\n",
        "    for i in range(len(processed_input)):\n",
        "        ids.append(f\"{paper_id}_{i}\")\n",
        "\n",
        "    collection.add(documents=processed_input, ids=ids)\n",
        "    return collection\n",
        "\n",
        "def to_download_and_parse_paper_agent(paper_url: str):\n",
        "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
        "    paper_id = paper_url.split(\"/\")[-1]\n",
        "    root_path = paper_id\n",
        "\n",
        "    if os.path.exists(root_path):\n",
        "        print(f\"Found cached markdown for {paper_id}\")\n",
        "        return f\"we already have the paper content stored in our database in the id of {paper_id}\"\n",
        "        # chunks = get_md_from_fs(paper_id)\n",
        "    else:\n",
        "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "        os.makedirs(root_path, exist_ok=True)\n",
        "        collection = get_md_with_document_parse(root_path, paper_url, paper_id)\n",
        "        return f\"we have parsed the paper content and stored in our database in the id of {paper_id}\"\n",
        "\n",
        "def to_retrive_paper_content_to_answer_question_agent(question: str, paper_id: str):\n",
        "    \"\"\"Use this to answer question about the paper.\"\"\"\n",
        "    collection = chroma_client.get_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "    results = collection.query(query_texts=[question], n_results=10)\n",
        "    results_str = [\"Retrieved Paper Content\\n-----------------------------------\\n\"]\n",
        "    for i in range(len(results['documents'])):\n",
        "        results_str.append(f\"{i}: {results['documents'][i]}\")\n",
        "    return \"\\n\".join(results_str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4YeFU2jNg7P"
      },
      "source": [
        "Let's see this in action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2zPOXyRNkBA"
      },
      "outputs": [],
      "source": [
        "supervisor_agent = Agent(\n",
        "    name=\"Supervisor Agent\",\n",
        "    instructions=(\n",
        "        \"You are a academic paper analyzer. \"\n",
        "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
        "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
        "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
        "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
        "    ),\n",
        "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent, to_retrive_paper_content_to_answer_question_agent]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz4-j2NQNnpI"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
        "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
        "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
        "    },\n",
        "]\n",
        "\n",
        "run(client, messages, supervisor_agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uODQ2ecrNofN"
      },
      "source": [
        "As you see in the below code cell's output, it precisely answered the user queries based on `the cached markdown for 2502.00299`. If you compare the result to the previous file system based approach, the respons is much better and richer because:\n",
        "- file system based approach simply cuts off from top to middle of the entire document to fit contents into the allowed context size. Hence, some details at the later part of the paper could be missing.\n",
        "- vector database based approach brings top-k relevant information to the user queries across entire document, then answer to user queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkdgFVNuNkBA",
        "outputId": "f1c0a4e4-1cfa-4d88-f4c3-adb1bc752fb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ChatCompletionMessageToolCall(id='4b81dad7-1328-4d99-9505-e5e40a64bb77', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
            "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
            "[ChatCompletionMessageToolCall(id='3a37b7a0-4a91-4f09-833a-61e30f68fd67', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
            "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
            "Found cached markdown for 2502.00299\n",
            "[ChatCompletionMessageToolCall(id='5703eec3-13bc-48fc-88db-e88418a7e166', function=Function(arguments='{\"paper_id\":\"2502.00299\",\"question\":\"Provide a comprehensive summary of the paper.\"}', name='to_retrive_paper_content_to_answer_question_agent'), type='function')]\n",
            "Supervisor Agent: to_retrive_paper_content_to_answer_question_agent({'paper_id': '2502.00299', 'question': 'Provide a comprehensive summary of the paper.'})\n",
            "--------------------------------\n",
            "Based on our analysis, the paper titled 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' introduces a new method called ChunkKV for compressing the key-value (KV) cache of different tokens in large language models (LLMs). This method groups tokens into chunks as a basic compressing unit and retains the most informative semantic chunks while discarding less important ones. The paper also proposes a layer-wise index reuse technique to further reduce computational overhead. The authors evaluated ChunkKV on cutting-edge long-context benchmarks, achieving up to 10% performance improvement under aggressive compression ratios compared to existing methods.\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
        "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
        "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
        "    },\n",
        "]\n",
        "\n",
        "run(client, messages, supervisor_agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkjlyt0vNkBA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "agentic-system",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
