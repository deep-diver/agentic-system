{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/agentic-system/blob/main/notebooks/document_parse_w_lang_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vbA6tJoU5Hu"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install -qU langchain-core langchain-upstage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayr6Ncp445Zc",
        "outputId": "f7883f24-25bf-49bf-ee62-62453b1f963c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "Y0F7q1TcW1M3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = \"XXXXX\"\n",
        "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
        "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "id": "wB1KdL1qxbhv"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db_3\")\n",
        "embedding_context_length = 4000\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.upstage.ai/v1\",\n",
        "    api_key=UPSTAGE_API_KEY\n",
        ")\n",
        "\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model_name: str = \"embedding-query\",\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
        "\n",
        "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
        "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
        "        return np.array(passage_embedding_list, dtype=np.float32)\n",
        "\n",
        "embedding_fn = UpstageEmbeddingFunction(client)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "655THXfo4fzk"
      },
      "outputs": [],
      "source": [
        "!rm -rf 2502.00299"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "fzHCPTkcpTV6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def to_paper_search_agent(paper_title: str):\n",
        "    \"\"\"Use this tool to search for paper's arXiv URL on the internet\"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
        "    headers = {\n",
        "        'X-API-KEY': SERPER_API_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    search_results = response.json()['organic']\n",
        "\n",
        "    if len(search_results) == 0:\n",
        "        return \"Count not find the URL to download the paper\"\n",
        "\n",
        "    first_result = search_results[0]\n",
        "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
        "        return \"Could not find the URL to download the paper\"\n",
        "\n",
        "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
        "\n",
        "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
        "    # Open the PDF\n",
        "    pdf = PdfReader(input_pdf_path)\n",
        "    total_pages = len(pdf.pages)\n",
        "\n",
        "    # Calculate number of output PDFs needed\n",
        "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
        "\n",
        "    output_paths = []\n",
        "\n",
        "    # Split into multiple PDFs\n",
        "    for i in range(num_pdfs):\n",
        "        writer = PdfWriter()\n",
        "\n",
        "        # Calculate start and end pages for this split\n",
        "        start_page = i * pages_per_pdf\n",
        "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
        "\n",
        "        # Add pages to writer\n",
        "        for page_num in range(start_page, end_page):\n",
        "            writer.add_page(pdf.pages[page_num])\n",
        "\n",
        "        # Save the split PDF\n",
        "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
        "        with open(output_path, \"wb\") as output_file:\n",
        "            writer.write(output_file)\n",
        "        output_paths.append(output_path)\n",
        "\n",
        "    return output_paths\n",
        "\n",
        "def get_document_parse_response(filename, api_key):\n",
        "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    files = {\"document\": open(filename, \"rb\")}\n",
        "    data = {\"output_formats\": \"['markdown']\"}\n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files, data=data)\n",
        "    upstage_response = json.loads(response.text)\n",
        "    return upstage_response\n",
        "\n",
        "def get_md_with_document_parse(root_path, paper_url, paper_id):\n",
        "    response = requests.get(paper_url)\n",
        "    # Save the PDF to a temporary file\n",
        "\n",
        "    pdf_path = f\"{root_path}/paper.pdf\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    split_factor = 1\n",
        "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
        "\n",
        "    markdown = \"\"\n",
        "    total_responses = []\n",
        "    for i, split_pdf in enumerate(split_pdfs):\n",
        "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
        "\n",
        "        # Append the response to the total_responses list\n",
        "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})\n",
        "        # Also write the response to a JSON file for persistence\n",
        "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
        "        with open(json_output_path, \"w\") as json_file:\n",
        "            json.dump(upstage_response, json_file, indent=2)\n",
        "\n",
        "        try:\n",
        "            markdown += upstage_response['content']['markdown']\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    collection = chroma_client.create_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "\n",
        "    processed_input = []\n",
        "    if len(markdown) > embedding_context_length:\n",
        "        chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
        "        processed_input.extend(chunks)\n",
        "    else:\n",
        "        processed_input.append(markdown)\n",
        "\n",
        "    ids = []\n",
        "    for i in range(len(processed_input)):\n",
        "        ids.append(f\"{paper_id}_{i}\")\n",
        "\n",
        "    collection.add(documents=processed_input, ids=ids)\n",
        "    return collection\n",
        "\n",
        "@tool\n",
        "def to_download_and_parse_paper_agent(paper_url: str):\n",
        "    \"\"\"Use this tool to download and parse paper. Use this tool when paper URL is already found.\"\"\"\n",
        "    paper_id = paper_url.split(\"/\")[-1]\n",
        "    root_path = paper_id\n",
        "\n",
        "    if os.path.exists(root_path):\n",
        "        print(f\"Found cached markdown for {paper_id}\")\n",
        "        return f\"we already have the paper content stored in our database in the id of {paper_id}\"\n",
        "        # chunks = get_md_from_fs(paper_id)\n",
        "    else:\n",
        "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "        print(\"Parsing in progress.....\")\n",
        "        os.makedirs(root_path, exist_ok=True)\n",
        "        _ = get_md_with_document_parse(root_path, paper_url, paper_id)\n",
        "        print(\"Parsing done ✅\")\n",
        "        return f\"we have parsed the paper content and stored in our database in the id of {paper_id}. Next step would be to retrieve it and answer user question.\"\n",
        "\n",
        "@tool\n",
        "def to_retrive_paper_content_to_answer_question_agent(question: str, paper_id: str):\n",
        "    \"\"\"Use this tool to retrieve information about the paper from the database, then answer user query about the paper.\"\"\"\n",
        "    collection = chroma_client.get_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "    results = collection.query(query_texts=[question], n_results=10)\n",
        "    results_str = [\"Below is the retrieved content of the Paper.\\n-----------------------------------\\n\"]\n",
        "    for i in range(len(results['documents'])):\n",
        "        results_str.append(f\"{i}: {results['documents'][i]}\")\n",
        "    results_str.append(\"Based on the retrieved content, answer the user question.\")\n",
        "    return \"\\n\".join(results_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "DHr7nluEW5g7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import ToolMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    found: bool\n",
        "\n",
        "tools = [\n",
        "    to_paper_search_agent,\n",
        "    to_download_and_parse_paper_agent,\n",
        "    to_retrive_paper_content_to_answer_question_agent\n",
        "]\n",
        "\n",
        "llm = ChatUpstage()\n",
        "llm = llm.bind_tools(tools)\n",
        "\n",
        "system_prompt = \"You are a academic paper analyzer. \"\n",
        "\"- Basiclly, you don't have knowledge of the requested paper.\"\n",
        "\"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
        "\"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
        "\"- If enough information is collected to complete the user request, you should directly answer to the user request. \"\n",
        "\n",
        "# Define the node that will call the LLM\n",
        "def chatbot(state: State):\n",
        "    messages = state['messages']\n",
        "\n",
        "    # --- Specify the System Prompt Here ---\n",
        "    system_prompt = \"You are a helpful assistant that translates English to French.\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt), # Static system message\n",
        "        MessagesPlaceholder(variable_name=\"messages\"), # Placeholder for history/user input\n",
        "    ])\n",
        "\n",
        "    # Chain the prompt and model\n",
        "    chain = prompt | llm\n",
        "\n",
        "    # Invoke the chain with the current messages from the state\n",
        "    # Note: We pass the messages directly to the placeholder\n",
        "    response = chain.invoke({\"messages\": messages})\n",
        "\n",
        "    # Return the AI's response to be added to the state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=tool_result,\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    return END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "nAGnRL6JptGc"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = BasicToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "2R3gFLSjp1fD",
        "outputId": "f3773763-d06b-4edb-dd24-734c63b9ec8b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANgAAAD5CAIAAADKsmwpAAAAAXNSR0IArs4c6QAAIABJREFUeJztnXd8U1Xj/8/NXk26d0sXXbRllFlUNsLDqAVBEH+KIigUAdlDFLAgjygKiAuUCgVZDxQFZE+ZljJaWrr3Ttuk2fP+/gjfgiEtBXpzTprzfvFHmntzzqfNmzvOPYMgSRJgMLChwQ6AwQAsIgYVsIgYJMAiYpAAi4hBAiwiBgkYsAM8DxqVob5Sq5QZlDK9Xk/qtTbQAsXm0hgsgufA4AnpHn4c2HGQw5ZEVDTp8tIVhZnypnqdgzOT50DnOTCEzkxgC02hRgOoKdYoZQomm1b6QBkYxQ+K5gdFC2DnQgXCJhq0jQby6p/14kqNizcrKErgE8KFneiFUCsNRZmK8jxlZaE6brRL5+4OsBPBxwZEvH9deuFAXdwYl+4DnWBnaWea6nVXj9ZrlIbh/8+TK6DDjgMT1EW8cKCWw6P1HeUKOwiFiKs0qVsrRrzj6duZBzsLNJAW8XRKjWcgJ7q/CHYQa3B4a8XLCa6u3mzYQeCAroip31eEdBNExdmFhSYOby2P7u8Y0s0e72AQbUe8nFoXEMm3KwsBAAmJvtf/qm+s0cIOAgEURcxJlzGYtG4DHWEHgcCUpf7nD9Qie5qiDhRFvHigrsdge7QQAEAQREAk/+qf9bCDWBvkRLx1pjGqv5DNtd+2jB6DnbJuNKkVBthBrApaIpIkWZqjjBvdkRtr2sIr49zuXJTATmFV0BKxMEPB5qIVCQr+YbzMq1LYKawKWt96UaYiMIpv5UqXLFny559/PscHhw4dWllZSUEiwBXQHV1ZVcUqKgpHE7RElNTpgqKtLWJ2dvZzfKq6uloiofDsGdpTUJarpK581EBIRLXC0Firpe42JTU1deLEif379x8yZMiiRYtqamoAAD179qysrFy9evXAgQMBAAaD4ccff3zttdfi4uJGjhy5fv16lerhYWno0KF79uyZM2dOv379Ll++PHr0aADA2LFjFyxYQEVavpAhLrenBkUSGcSV6t3rSygqPD09PTY29tChQ2VlZRkZGe+///7UqVNJkqypqYmNjd27d69EIiFJcufOnX369Dl58mRJScm1a9dGjBixYcMGUwmvvvrq+PHjN23adPfuXZVKderUqdjY2OzsbLlcTkXgqiLV/m9KqSgZTRDqj6hoMvCFVB0OCwoK2Gz2mDFjGAyGr6/v+vXrq6qqAAAikQgAwOPxTC9GjhzZr1+/kJAQAIC/v//w4cOvXLliKoEgCA6HM2fOHNOPfD4fACAUCk0v2h2+iK6Q2lELDkIikkaSRdktc8+ePQmCeP/99+Pj4/v06ePt7e3i4vLkbo6OjseOHUtKSqqtrdXr9Uqlksd71CMmJiaGonhPQmcQLA5CF05Ug9CvyhMypHU6igoPCAjYsWOHr6/vli1bxo4dO3Xq1MzMzCd327Bhw/bt2ydOnLht27Y9e/YkJCQ8vlUgsF53BLlET2cQVqsOOgiJyBfSFU0Unow6d+6clJR0+vTpn376iU6nz5s3T6v9192AwWA4cuTIO++885///MfHx8fV1VUul1OXp3UovVBBEIRE5DkwnD2ZRiMlz/szMzPv3bsHAKDT6bGxsTNnzpRIJPX1Dx/pmjoZGI1Gg8FgulgEACgUikuXLrXe/4C63gkapcHNz476JiIkIgCAw6MXZiioKPnq1avz588/e/ZseXl5Tk7O3r17vby8PD092Ww2m81OT0/PyckhCCIsLOzo0aPl5eV5eXnz5s3r379/U1NTcXGxXq83K1AoFAIA/v7778LCQioC59ySeQXY9tCcZwItEQO68IvvUyLie++9l5CQ8O23377++uuJiYkkSW7evJkgCADA1KlTz5w5M2vWLJVK9emnnxoMhokTJy5btmzSpEmJiYmenp5vv/12bW2tWYERERFxcXHffPPNl19+2e5pDXqyIl/lH25HIwfQ6qGtkutPpdTEf+gDOwhkiu7Ly3JVryS4wQ5iPdA6InIFDCcP1l0763jyJFf/qLe33ukItSOa6D/G9aelBV0HWO4YazAYhgwZYnGTVqtlsVgWNwUGBu7YsaNdYz4iOTk5OTnZ4iaBQNDSfXdERMQPP/xgcdODtCZ3P46zh+XfpaOC1qnZxJ2LEoIgu75ieRSzTCaz+L5Go2GxWKbLPjNoNBpFzz9M9Zo1AzWj0+mYTKbFTXQ6/fGm8sc5ur1ywOtuDo6WP9hRQVFE05fRpa/I+l3CoGO3vzha14jNjH7f+9KhuvpqDewgVuXcvlrPAI4dWojuEdH06Hnf12WvjHPzDraL5rTz+2t9O3Ptdh4cRI+IAACCRkxa5H/teH32zSbYWajFaCAPb61w9mTZrYVIHxGbuXpUXJqtjBvj2iEbeP851ZCTJhs4wc2eJ76xDREBAHUVmqt/ivlChncwNzCKz+XbfG+A2jJ1aY4y7VRjt4GOvUc402h21NHGIrYhoonyPGVOmqwoU+Hmxxa5MvlCBl/I4AnpRiPsZG2ATgBpg04hNZCAfPCPjC9khHTlx7ziyGShe3VkTWxJxGaqilTiCq2iSa9o0tMIQilvz85jSqWypKQkIiKiHcsEADg4MUmS5IvoDs5M32AuX4TcowS42KSIlJKdnb127dqUlBTYQewLfF7AIAEWEYMEWERzCILw9/eHncLuwCKaQ5JkaWkp7BR2BxbRAtYcrYcxgUW0AMTBe3YLFtEcgiBcXe19gkbrg0U0hyRJsVgMO4XdgUU0h0ajBQYGwk5hd2ARzTEajUVFRbBT2B1YRAwSYBHNIQiiedYRjNXAIppDkqRUal8TqaMAFtECjo52utwQRLCIFqB0lnaMRbCIGCTAIppDEISPj73PAmV9sIjmkCRZUVEBO4XdgUXEIAEW0RyCIDp16gQ7hd2BRTSHJMmSkhLYKewOLCIGCbCI5uDeN1DAIpqDe99AAYuIQQIsojl4OCkUsIjm4OGkUMAiYpAAi2gBPK7Z+mARLYDHNVsfLKI5NBrN19cXdgq7A4tojtFoLC8vh53C7sAiYpAAi2gOQRDOzs6wU9gdWERzSJJsaGiAncLuwCKaQ6PRAgICYKewO7CI5hiNxuLiYtgp7A4sojn4iAgFLKI5+IgIBSyiOTQazd3dHXYKuwMv+POQyZMny+VygiC0Wq1cLndyciIIQqPRnDx5EnY0uwAfER8ycuTI2trayspKsVisVqurqqoqKysdHOx33Vorg0V8yKRJk/z8/B5/hyCIAQMGwEtkX2ARH8JisV577TU6/dECvP7+/q+//jrUUHYEFvEREydObJ71hiCIQYMGeXl5wQ5lL2ARH8FiscaPH286KPr7+0+YMAF2IjsCi/gvJk6c6O3tbTocenh4wI5jR9jA8tU6jbGhRquUGkjCGtXFD5tx4cKFl3qML8xUWKE6Gg04ebBELkwr1IUyqLcjXj1an39HzuLQBI5MowHpqM+HwIlR9kAhcmP1GubkE8KFHQcaSIt4dl8tm0PvOtAFdhDK0agNp3dWDprg5hnAgZ0FDuheI148VMfhMezBQgAAm0MfPcPv9O6axhot7CxwQFRESZ22sVob84p99ZTuO8b9n9ONsFPAAVERG6q1NDqi2ahD5MosfaCEnQIOiH7Zcone0Z0FO4W14fIZfCFDozbCDgIBREUkSaDTonsXRR1N9VoaYZVmKsRAVESMvYFFxCABFhGDBFhEDBJgETFIgEXEIAEWEYMEWEQMEmARMUiARcQgARYRgwQdX8QJb4z85dfvX6SEz1YtXrBwZvslwlig44v4fKxaveTEyT9fpITDqfvXf7mq3QJ1dLCIlsnNzYZegl1hA6P42ohOp0v+7adTp4/J5bKQkLAPps+Jiupq2kSj0X7bue3IHwfkcln37r2WLl7l5OQMAGhsbPjhp2/T02/KZE1ubh7jXntj3LhJAIBBQ3oCAP775eqt33/955ELpvH2x/86smvX9voGcVBgyPz5K0I7h5sKP3Y8df+BlMrKci6X16d33MwPP3Z2dpk3f8bdu+kAgJMnj545dePxCSQwFuk4R8Qffvzm2PHUWTPnf/vNNh8fv8VLZ1dWVZg2nb9wWipt/GLdpk9WrM3Kupf820+m97/8ak3W/XsrV6zb/vPvb06euvWHjX9fuQAA2L/3OADgo9mLUnYdMe1ZUlp09uyJZUvXbPjvVq1O+8nK+TqdDgBw6tSxr75OGj5s1K/b961ZtSE378Gy5XNJkkxaszG0c/jgQcNTD53BFraFDnJEVCqVx46nfjBj7qCBwwAACz5eoVIqKyrKvL18AAB8vmDOR4sBAGGhEZf/Pp+dnWn6VOKsBTQazbSPn1+nI0cOpKVdf6n/QKFQBADg8Xgioci0p0TS+Mv2fUIHIQBg5ocfL14y+87dW7169j1wcHf//gOmvPmuqYSPZi9atDgxM/NudHQ3OoPBZLFEIkeofxiboYOIWFJapNVqI8K7mH5kMpmrV33ZvLVLZEzzaydH5yxlhuk1l8Pdszf5zp00qVRiNBplsiYfH78nygYAgKDAEJOFAIDIiGgAQGlpcfduPQsK8wYNGt68W1hYJAAgvyA3OrobNb9oh6WDiCiXywAAbLblQcFc7qOB6wTxsCe+Xq9fvHS2wWCYnbjQ3y+ATqd/8umClsrn8x8tE2kqTaNRq9QqkiR5PH7zJh6XBwBQqex0ANSL0EFENJ0BlcpnmCQkOzuzsDB/0zfbYmK6m96RShq9PL0t7qxSq5pfK5VKAACHw+VyuDQa7fFKFUqFmbWYNtJBblZ8vP04HM7de+mmH41G49yPp588ebSVj2i0GgCA8P+uAu/fv1dVXfn4vBePvy4uLmhesjQnNwsAEBAQxGAwQoJDMzLvNO+Wdf9e8wnarARM63QQEfl8/sgRY3fv+fXUqWM5udkbv1mXm5sd1eqFWkhwKIvFOnR4b329+J+065u3fNmrZ9+y8pLGxgY2m81ms+/eS8/Lz9Hr9QAAHo+/4as1xcWFhYX523/Z6unhFRPdHQAwYcJb16//vf9ASnV11e07aVu2ftW1a4/wsEgAgIPAIT8/Jy8/B+vYFjrIqRkA8MGMuQSN9uPPm1QqZWBgyBdrN/l4t7baraOj0+JFn23f/t2p08dCQyOWLF5VJ679PGnZ/IUf7vhl/+RJU/fu++3atcspu1L1Bn2XyJjY2D5Ll8+prxd37hye9PlGBoMBABg6ZIRGo95/IGXb9u/4fMFL/Qd+8MFcU/kJCZO+WP/pnLnT/jxywbQzphUQnYTp7iWJuErfe4Qr7CDWZs+6gvfWBDHZdje0uYOcmjG2DhYRgwRYRAwSYBExSIBFxCABFhGDBFhEDBJgETFIgEXEIAEWEYMEWEQMEmARMUiARcQgAaIisjgEi4toNkpx8WETdjnoD9Ev29GdVZlvdyM/Gms1GqWRwbC7PmDoiujpz6HTgU5rX0vf1JaqQ7vb6XgXREUkaETcGJczKZWwg1iP0gfygjtNvV61r+UHm0G0h7aJ2nJN6taK2OEuIleWgyMT4aQvRH2VWtaoK86UvzHfl6DZ43kZdREBAGql4daZxqoitVph0OseRtVqtXQ6naKpPIwGg1an43CstG6yjmgUOQrDu7vEvGzfc0KQtkZJScm3335LXfmrVq0aPHjwtWvXqKvicWQy2fLly61TF8qgfkR8HKlUWl1d7enpKRKJKKoiKyvrk08+KS0tjYuL27x5M0W1WGTfvn0xMTERERHWrBQdEL1ZeRKxWJyQkBAYGEidhQCA33//vbS0FACQm5t75coV6ip6klGjRq1du1YikVizUnSwDRFra2tLS0vPnTvHYlG4iHN2dnZ6+sO5IsRi8Z49e6ir60kEAkFKSgoAICMjo7y83JpVo4ANiDh//nySJHv06EF1Rbt3766pqWn+MSsry8oHRQCAo6NjSEhIYmJiXV2dlauGC9IikiR569at+Ph4Dw8PquvKyspqPhyakEqlpkOUleFyuUeOHNFqtVKp1DThkz2Aroi3b99WKBTR0dEDBgywQnU7d+6sqakxGo3N93EAgAcPHlihaov4+Pjw+fxXX33V7L9HhwXqPXuLZGRkTJs2DUrVWVlZU6ZMgVK1RXbs2AE7gjVA9IjY2Ni4fft2WLV36tQJVtVPMnXqVADAihUrxGIx7CwUgpyIH3/8MQDg5ZdfhhVApVLV1tbCqr0lFi5c+Nlnn8FOQSFoiXjgwIGEhAS4GVQqlZubG9wMT+Lk5LR161YAwNmzZ2FnoQS0RBw0aNArr7wCN4NYLLbag+bnwMPDY8qUKbBTtD9IiKjVagcOHAgAcHWFPyGiVCr18fGBnaJFoqKiVq5cKZFIZDIZ7CztCRIiJicnX7hwAXaKhxQUFFih2fJFCA8Pd3R0TE9PP3fuHOws7QZkEQ0GQ01NzYwZM+DGMCMgIAB2hKczYMCAv/76SyqVwg7SPsDsfdPU1BQfH3/+/HlYASzSq1evGzdu0GhInCueikQiqa6uDg8Phx3kRYH25zY9vkPNwgcPHvTr189WLDQ9m+bxeJ9++insIC8KtL94VlaW6QYFKa5evRoWFgY7xbPh7+/fp08fW+8/BkfEyZMnM5nM/1uMDCEuX74MsS39uRk1ahSNRmtoaIAd5PmBIOKtW7c2btwYGhpq/apbRyqVCoXCmJiYNuyLHEKh8ObNmytWrIAd5Dmx9s2KXq8nCALNJYx//fVXlUqVmJgIO8jzU1ZWJpVKo6KiYAd5Zqx6RMzOzp46dSqaFgIADh06NG7cONgpXgg/P7+AgACF4hkWx0QEq4p4/vz5H3/80Zo1tp0rV6706tXLy8sLdpAXRSAQLF269OrVq7CDPBu2NIqPUt544421a9eGhITADtI+HDp0aNSoUWw2G3aQtmKlI6JMJlu8eLF16noOTp8+HRgY2GEsBACMGzfOhiy03uqkW7Zs6dOnj3Xqeg42bdqUnJwMO0U789133/H5/HfffRd2kDZhjVOzwWAQi8XI9iTYvHmzSCR65513YAdpfxYtWrR8+XInJyfYQZ6ONUTU6/UkSTKZTKoreg6Ki4tXrly5a9cu2EHsHWtcI06bNi0nJ8cKFT0H8+bNW7duHewUFHLy5EmbGCJNuYhSqZTNZqPZxJqUlPTOO+/4+fnBDkIhfD4/KSkJdoqnY7/NN2fPnr1x48by5cthB6GctLS08PBwgQDpuWgpF1EikTAYDNT+CqWlpXPnzj18+DDsIJiHUH5qXr9+/bVr16iu5VmZOHHi/v37YaewEiqV6s0334Sd4ilQLqKDgwNqPe+XLVuWnJyM5l08FXC5XBcXF8Qf+tndNeKiRYtGjhw5ePBg2EGsilqt1mq1QqEQdpAWofyIWF5ertfrqa6ljWzYsCE2NtbeLAQAcDgclC20hohLlizJz8+nupa2cPDgQQ8Pj0mTJsEOAodx48ZVV1fDTtEilIsYGRlpMBioruWp7Nu3r7Cw8O2334YdBBo9evTIzc2FnaJF7OIa8Y8//rh9+3bHnsTI1qG8941pdJmjI7RFRE6cOPHPP/98/vnnsAIgwsNpCFEdKUt5rLS0tC+++ILqWlri4MGDly5dwhaa1kl46623YKdoEcpPzbW1tePHjxeJRDKZTCaTWXMi3pSUFAcHh/j4eKvViDJNTU3jx48/ffo07CCWoUrEGTNm3Lt3z6zhxtXVdd26dVZYHwAAcOTIkfT09NWrV1uhLsyLQ9Wp+eeff36yVwubzbbOqOFdu3YVFBRgC82oqalBoQXDIhReI86ePdvb27v5R5IkIyMjGQzKb49SUlLq6+vnz59PdUU2x4cfflhRUQE7hWUoFHHAgAGjR4/m8/mmHzkcjhWGrWzcuJFGo82bN4/qimwRNput0Whgp7AMtXfNM2bM6N27t6nJwMnJKTo6mtLq1qxZ4+HhgX5PE1gkJycHBwfDTmEZyptv1q1bFxwcbDQaRSIRpX+FpUuXdu3atUPOL91eqFQqZK8R23TXrNcZVXLjc9eRn5+/bt26/v37T5s27bkLaZ3PPv1s5NiBw4YNo6j8jsGcOXOmT59O9Xnp+XiKiNk3m+5dljZUa7kCRCesMd0GsfjGxkoyMIrfY7CjVyAXdiK06NGjB0EQJEk2zwNIkmRoaOjevXthR3tEa/ewN081iCt1L4/zdHC2gT6kJElK63QX/lcTN8qlUwQPdhyECAsLy8nJefzhnkAgmD59OtRQ5rR4jXjjRIO0Tv9ygodNWAgAIAjC0Z01errfjRMNJdn2sqhnW5g0aRKX+6+zRKdOnYYMGQIvkQUsi9hYqxVXaPqOdrd6nnZgyBSv2+cbYadAiPj4+MdXjuHxeAjOQ2JZRHGFhiSRm1e4jbDYdEmdrqlBBzsIQkyZMoXFYpleBwUFDRo0CHYicyyLKJca3PzQXQbsqfiF8RtrsYiPiI+P9/X1NY23Ny13ihqWRdRpjDr187fXQEcu0ZGGjt/h95mYMmUKk8kMCgpCcDEH601Lh3kmSh4oZI16ZZNBqzKqVe3TBM0HfQd2+ahLly5nfq9pnwKFDKOB5AsZfCHdM5Dj4PRCN7VYRITISWvKva0oyVJ4hwp1OpLOoNOZDEBrt1aL3v1GAQBk7dSioFATeq3OWKoljWTTITGXTw/pxu8SJxSInicwFhEJ8m7LLqfWO3nz6Wx+l2FuCK5A0zrunYFKpikrUmbdrAyM5L30mguD+WxPj7GIkDEYyGO/VCtkwLerF4trw18H14HNdWC7Bjo1lEl/XlY0cIJbZJ9nGEltw795B6C2TH3g2/LgPt5CP1ua77p1nP1Ezn6ijGt1dRWaAePc2vgpRMd02QPSeu3xHbVdhgZyHDqOhc14hLnVi2mXU+vbuD8WEQ7VJerU76sDevm0YV9bxdnPsbYa/PVbm6aXwCJCQK8zHtpS0alnR7bQhEsnR6WClnbm6U9csYgQOPZrTXDfjm+hCZdAl5IcTVneU1ZlwyJam/vXpAoFwebbRp+mdoHnKrz4v6dcLGIRrc2VPxvcg5xhp7AqXCGbxmDk3Za1sg9CIn62avGChTNhp6CWzKtSl04ODDai3d3vZp5duLKPQiFp95JdAp3vX5e3skO7iXg4df/6L1e1V2kdlQdpcjbfhrs1PTdsHrOhWttYo21ph3YTMTc3u72K6qjoNMa6MrXAxU6H1PBdeYUZLR4U2+fJyrz5M+7eTQcAnDx59OefdncOCcvIuLPtl+9yc7MJgogIj5o+/aOI8C6mnY8dT91/IKWyspzL5fXpHTfzw4+dnV3MCjx2PPXg//ZUVVWw2ZyuMT1mJy50d0d0Kb+2U5ytcA10oK782/dOXbyyp6auiM3mdY8ePnLoTBaLAwDYuXc5QYCwzv3OX9opldW5u3ZKGL2wk180AMBg0B85/k36vROk0RgZ9lJIUE/q4jm48apLW7xMbJ8jYtKajaGdwwcPGp566ExQYEhZWcnCxbPcXN23bkn+bvMOLo+3cNHM2toaAMCpU8e++jpp+LBRv27ft2bVhty8B8uWzzUbSXjv3u2vvk4aP27yL9v3fbFuk7RJsvrzpe2SEy7SOr1BR1Vvhsysi7sPrAwN6b0gMeWNhJX37p87+MfD2QDpdEZRyd3SsvvzZu1cteQEjyfad+jhWlTnLv12Iy117Mh5H8/aGRjQ7czFXymKBwBgshlVhaqWtraPiAKBgM5gMFkskciRTqcf+eMgl8tbtnRNcHDn4ODOK5Yl6fX6k6eOAgAOHNzdv/+AKW++6+fXqVu32I9mL8rNe5CZeffx0oqKC9hs9ohXx/h4+0ZGRH22cn3irAXtkhMucomeutuUc5d3BgX0+M+wWa4ufhGhcaOGJ6bfPSGRPux6qNWqxo6cx2ZxWSxOj5gRteJirVYNALh196+oyAG9e4xxdfGL6z0+NJjCOWGYHIZa0WLfSkrumnPzskM7hzfPt8Tj8fz8OhUU5Or1+oLCvMiIRwO8w8IiAQD5Bf+a27l7t54EQcyZ9/7RY4erqiudnV0iI1Bcyu9ZUcoNFIloNBrLK7NDQ3o3vxMU0AMAUFX9cBp9Vxc/02kaAMDjCgEASlWTXq8T15f5+UQ2f8rftwsV8Zph8+mKJstDOCjpfaNUKlycXR9/h8fjK5UKlVpFkiSPx3/0PpcHAFCp/tVX098/4LvNO37f99vP27bINq6NiIianbiwA7hI3ZSoOp3aaDScOrft9PlfHn+/SSY2vWAwnuxXQWq1KgAA87FNbDa148FJA9lSV0tKROTzBQrFv+6PFAq5i7Mrl8Ol0WhK5aOnPQqlwrS/WQnBwZ0/WZ5kMBgyMu78suP75Svm7d97vHkcmo0iENHr6iiZeobJ5NDpjJf6vtEnduy/auS31nLOZHEAACrNo29KpWqtzfkFIUlSqzbyHCwr156n5uZ7jrDQyJzcbJ3u4UFYJpeVlhaHh3dhMBghwaEZmXeaP5J1/17zCbqZ7OzM+/fvAQDodHq3brHvvTtTKpU0NLS1QxGyCBwZei0lItJoNB+v8EZJlbtbgOmfs5MPjcbg8VrrmspksJwcvaqq85rfyS24SUU8E3qNgcNv8cqk3UR0EDjk5+fk5edIpZL4+AkajfrLr9aUlZUUFuYnrV3B5wteHT4aADBhwlvXr/+9/0BKdXXV7TtpW7Z+1bVrj/B/i3jj5tUVK+dfvHS2orI8Lz/n0KG9nh5eHh6e7RUVFo5uTAadqrGRA196KyPr/LlLv9XWlVRU5uw5+NnW7TPU6qd0NegePTwz6+L1tNSq6vyLV3ZXVlG4EItWpfcKarENtd1OzQkJk75Y/+mcudNWr9rQu1e/Df/d+vP2Le/PmEyn06Ojun3z9U+Ojk4AgKFDRmg06v0HUrZt/47PF7zUf+AHH8w1K+qtKe/p9boff/xWXF/H5wuiorqu/2KzzQ3jeJKALvwTv1W7Brm2Yd9nJqbLoMnjV5+/vPPk2Z85HEGAf8zM977ncPitf2qVi9dZAAADPElEQVTY4PcVSsnRE5uNpDEitP+o4bN37ltmJCn536IQKzrHtNgF2PJsYDdPNmjVoOtAW302f+73yq4viwK6POVrsD6Ht1YyhA4OrvY4R1TB1bLX5/mIXCx3O0Ko04M9EN5boJEjOnkwpajlWldfdksW4sFT1iail/Da0WKhh4DFtfyVZGZf2nvI8mIIfK5IoZJa3NQ39rXRIz5qr5BFJXd+SbH8BMFoNNAIGrB0mdSv17hRwxNbKlNc2PDSmNZWH8MiWpuXX3P552yjdxfLM62FBveeP2uXxU1arbq5UdoMNrs9L0J8vSNayqDTaeh0psV11FrJoGhUM5lkQGRrIbGI1qZzd4e8Owq1TGNx8B6LxXFmeVv6nPVgMtnOTu2ZQd0oGzThKbdo+BoRAv9517PwZqXRaBfTRNXk1oV157o/bXI5LCIcJi/2L7xeDjsF5dTk1bt50aLiRE/dE4sIByd31ptLfPL+LjXobXj6v9apK6gPjmQOntimeYexiNDgCZhvLPDN+7tU0dhiLz0bxag3VmRWB4Qyeg51auNHsIgwETozP/xvMNOoKL9bpWrqIO2LdUWNOZdKXxrl2Gv4MzwQwXfN8Bn+lkdZrvLSYTFbwKaxWEI3PrLD/FpBXq+Si5VNtfKurzhOmPXMS4xhEZHAL5Q3ZYl/SZYi946i8GaFkxdXqzYyWAw6i0HQEH3ITqPTdCqtQWcApLGxSuXux4mM5Uf2DXjWmRFNYBERolMkv1MkHwBQU6qWNeqVTXq10qhRIrp6HldAEjQGX8jmCRlegZ5M1gtd5mERUcTDn+PhDzuEdbEsIotDGAGiZ4S2wHdk0ug2nN8OsXw4dXBi1pXYcJtCabbc2dO2xxXYG5ZFdPdj224/VJVc7+rDFjjiqw5bosUjok8I59L/2jTXJ2qcSansNayt7agYRGhtveb716R5d+RdB7g4ebDoDNSbvtVKQ5NYe+VI7Yi3Pdz97XGiI5vmKQuHF91X3LkoqS5S0xlIn6pFrsymBl1AJL/nMCcnd3x1aHs8RcRmNCqkn82TRsDho37MxrRCW0XEYCgFH0UwSIBFxCABFhGDBFhEDBJgETFIgEXEIMH/B+nyrNCjvCmYAAAAAElFTkSuQmCC",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3LBl1Pbo84R",
        "outputId": "c8dbcd01-b229-4064-b979-7660b7bfed7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv. \n",
            "Assistant: URL to download 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference': https://arxiv.org/pdf/2502.00299\n",
            "\n",
            "No cached markdown found for 2502.00299, parsing from URL\n",
            "Parsing in progress.....\n",
            "Parsing done ✅\n",
            "Assistant: we have parsed the paper content and stored in our database in the id of 2502.00299. Next step would be to retrieve it and answer user question.\n",
            "\n",
            "Assistant: Below is the retrieved content of the Paper.\n",
            "-----------------------------------\n",
            "\n",
            "0: ['\\nsame as in LongBench in Section 4.2. The chunk size is\\nset from the range {1, 3, 5, 10, 20, 30}. Figure 5 shows the\\nperformance of the ChunkKV with different chunk size on\\nthe LongBench and NIAH benchmarks. The three colorful\\ncurves represent three LLMs with different chunk sizes, and\\nthe colorful dashed line is the corresponding FullKV perfor-\\nmance. For more experiments on the size of the chunks with\\ndifferent compression ratios, refer to the Appendix B.4.\\n\\nTable 9: LongBench Performance with Different Chunk\\nSizes and Compression Ratios for LLaMA-3-8B-Instruct\\n\\n| Compression | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Rate | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 10% | 37.32 | 40.49 | 40.47 | 40.51 | 40.21 | 40.05 | 39.57 |\\n| 20% | 38.80 | 40.66 | 40.57 | 40.74 | 40.53 | 40.46 | 40.04 |\\n| 30% | 39.23 | 41.02 | 41.29 | 41.59 | 41.38 | 41.33 | 41.02 |\\n\\n\\nFrom Figure 5, we can observe that the LongBench per-\\nformance of ChunkKV is not significantly affected by the\\nchunk size, with performance variations less than 1%. The\\nthree curves are closely aligned, indicating that chunk sizes\\nin the range of {10, 20} exhibit better performance.\\n\\nTable 9 and 10 show the performance of ChunkKV with\\ndifferent comperession ratios and different chunk sizes on\\nthe LongBench and NIAH. We conducted extensive exper-\\niments across different compression ratios and KV cache\\nsizes to shows the effectiveness of ChunkKV and the chunk\\nsize is robust.\\n\\nTable 10: NIAH Performance with Different Chunk Sizes\\nand KV Cache Sizes for LLaMA-3-8B-Instruct\\n\\n| KV Cache | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Size | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 96 | 41.0 | 63.2 | 65.2 | 70.3 | 67.2 | 65.3 | 53.1 |\\n| 128 | 47.9 | 65.6 | 69.1 | 73.8 | 72.3 | 72.0 | 71.2 |\\n| 256 | 61.7 | 70.3 | 71.2 | 74.1 | 73.2 | 72.3 | 71.1 |\\n| 512 | 68.6 | 72.6 | 72.5 | 74.5 | 74.3 | 74.0 | 72.6 |\\n\\n\\nFrom the chunk size ablation study, we can observe that\\nacross different tasks (LongBench and NIAH) and vari-\\nous compression settings, a chunk size of 10 consistently\\ndelivers optimal or near-optimal performance. This em-\\npirical finding suggests that a chunk size of 10 strikes a\\ngood balance between preserving semantic information and\\ncompression efficiency, making it a robust default choice\\nfor ChunkKV. Therefore, we adopt this chunk size setting\\nthroughout our experiments.\\n\\n# 6. Conclusion\\n\\nWe introduced ChunkKV, a novel KV cache compression\\nmethod that preserves semantic information by retaining\\nmore informative chunks. Through extensive experiments\\nacross multiple state-of-the-art LLMs (including DeepSeek-\\nR1, LLaMA-3, Qwen2, and Mistral) and diverse bench-\\nmarks (GSM8K, LongBench, NIAH, and JailbreakV), we\\ndemonstrate that ChunkKV consistently outperforms ex-\\nisting methods while using only a fraction of the memory.\\nOur comprehensive analysis shows that ChunkKV’s chunk-\\nbased approach maintains crucial contextual information,\\nleading to superior performance in complex reasoning tasks,\\nlong-context understanding, and safety evaluations. The\\nmethod’s effectiveness is particularly evident in challenging\\nscenarios like many-shot GSM8K and multi-document QA\\ntasks, where semantic coherence is crucial. Furthermore,\\nour proposed layer-wise index reuse technique provides\\nsignificant computational efficiency gains with minimal per-\\nformance impact, achieving up to 20.7% latency reduction\\nand 26.5% throughput improvement. These findings, sup-\\nported by detailed quantitative analysis and ablation stud-\\nies, establish ChunkKV as a significant advancement in KV\\ncache compression technology, offering an effective solution\\nfor deploying LLMs in resource-constrained environments\\nwhile maintaining high-quality outputs.\\n\\n8ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n', 'hami, A. and Jaggi, M. Landmark attention:\\nRandom-access infinite context length for transformers.\\nArXiv preprint, abs/2305.16300, 2023. URL https:\\n//arxiv.org/abs/2305.16300.\\n\\nOpenAI. Gpt-4o-mini: Advancing cost-efficient intelli-\\ngence, 2023. Accessed: 2023-12-14.\\n\\nPan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C.,\\nand Zhang, T. Lisa: Layerwise importance sampling\\nfor memory-efficient large language model fine-tuning.\\nArXiv preprint, abs/2403.17919, 2024a. URL https:\\n//arxiv.org/abs/2403.17919.\\n\\nPan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K.,\\nZhang, J., Pi, R., and Zhang, T. Plum: Prompt learning us-\\ning metaheuristics. In Ku, L.-W., Martins, A., and Sriku-\\nmar, V. (eds.), Findings of the Association for Computa-\\ntional Linguistics ACL 2024, pp. 2177–2197, Bangkok,\\nThailand and virtual meeting, August 2024b. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2024.\\nfindings-acl.129. URL https://aclanthology.\\norg/2024.findings-acl.129.\\n\\nPires, B. Á. and Szepesvári, C. Multiclass classification\\ncalibration functions. arXiv preprint arXiv:1609.06385,\\n2016.\\n\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lill-\\nicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Fi-\\nrat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of\\ncontext. ArXiv preprint, abs/2403.05530, 2024. URL\\nhttps://arxiv.org/abs/2403.05530.\\n\\nShaham, U., Ivgi, M., Efrat, A., Berant, J., and\\nLevy, O. ZeroSCROLLS: A zero-shot benchmark for\\nlong text understanding. In Bouamor, H., Pino, J.,\\nand Bali, K. (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023, pp. 7977–\\n7989, Singapore, 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.\\n536. URL https://aclanthology.org/2023.\\nfindings-emnlp.536.\\n\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V.,\\nSmith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis,\\nM. In-context pretraining: Language modeling beyond\\ndocument boundaries. In The Twelfth International Con-\\nference on Learning Representations.\\n\\nSmith, B. and Troynikov, A. Evaluating chunking\\nstrategies for retrieval. Technical report, Chroma,\\n2024. URL https://research.trychroma.\\ncom/evaluating-chunking.\\n\\nSteinwart, I. How to compare different loss functions and\\ntheir risks. Constructive Approximation, 26:225–287,\\n2007. URL https://api.semanticscholar.\\norg/CorpusID:16660598.\\n\\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma,\\nS., Zhang, Q., Wang, J., and Wei, F. You only cache\\nonce: Decoder-decoder architectures for language models.\\narXiv preprint arXiv:2405.05254, 2024.\\n\\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\\nS. Quest: Query-aware sparsity for efficient long-context\\nllm inference. ArXiv preprint, abs/2406.10774, 2024.\\nURL https://arxiv.org/abs/2406.10774.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,\\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\\nD. Long range arena : A benchmark for efficient trans-\\nformers. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=qVyeW-grC2k.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\\nUnifying language learning paradigms. ArXiv preprint,\\nabs/2205.05131, 2022. URL https://arxiv.org/\\nabs/2205.05131.\\n\\n12ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundatio', ') and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nreasoning (O1 and R1) LLMs, achieving state-of-the-art\\nperformance.\\n\\n# 2. Related Work\\n\\nKV Cache Compression. KV cache compression technol-\\nogy has developed rapidly in the era of LLM, with meth-\\nods mainly focused on evicting unimportant tokens. The\\ncompression process occurs before the attention blocks, op-\\ntimizing both the prefilling time and GPU memory. Xiao\\net al. (2024) and Han et al. (2024) propose that initial and re-\\ncent tokens consistently have high attention scores between\\ndifferent layers and attention heads. As a result, retaining\\nthese tokens in the KV cache is more likely to preserve im-\\nportant information. Furthermore, FastGen (Ge et al., 2023)\\nevicts tokens based on observed patterns. H2O (Zhang et al.,\\n2023) and SnapKV (Li et al., 2024) employ dynamic KV\\ncache compression methods, evaluating the importance of\\ntokens based on attention scores and then evicting the less\\nimportant ones. As inference scenarios become increas-\\ningly complex, dynamic KV cache compression methods\\ndemonstrate powerful performance. Recently, Yang et al.\\n(2024b) and Cai et al. (2024) have closely examined the dis-\\ntributions of attention scores during the pre-filling stage of\\nthe Retrieval-Augmented Generation (RAG) task, discover-\\ning a pyramidal KV cache compression pattern in different\\ntransformer layers.\\n\\nAlthough these KV cache compression methods have ex-\\nplored efficient GPU memory management while maintain-\\ning original performance, our study focuses more on the\\nsemantic information of the prompt. We find that chunks\\nof the original KV cache are more important than discrete\\ntokens.\\n\\nChunking Method. The chunking methodology is widely\\nused in the field of NLP due to its simplicity and effective-\\nness (Tjong Kim Sang & Veenstra, 1999). In the era of\\nLLMs, chunking is primarily applied in data pre-processing.\\nFor example, Shi et al. suggest grouping related train-\\ning data into chunks to achieve better convergence curves\\nto pre-train LLMs. Fei et al. (2024) apply a topic-based\\nchunking method to improve the semantic compression of\\nprompts. Furthermore, chunking plays an important role in\\nthe Retrieval-Augmented Generation (RAG) field (Yepes\\net al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It\\nserves to divide documents into units of information with\\nsemantic content suitable for embedding-based retrieval and\\nprocessing by LLMs.\\n\\nLayer-Wise Technique The layer-wise technique is widely\\nused in the training and inference of large language models\\n(LLMs). LISA (Pan et al., 2024a) is a layer-wise sampling\\nmethod based on observations of the training dynamics of\\nLow-Rank Adaptation (LoRA)(Hu et al., 2022) across lay-\\n\\ners. LAMB(You et al., 2020)', 'in-\\nformation usually appear in a continuous sequence (Fang &\\nXie, 2022). Thus, we introduce a straightforward yet effec-\\ntive ChunkKV, grouping the tokens in a chunk as a basic\\ncompressing unit, which should be preserved or discarded\\nas a whole. Thus, it retains the most informative semantic\\nchunks from the original KV cache. As shown in Figure 1,\\npreserving a chunk helps to catch the subject, predicate,\\nand object. Furthermore, we investigate that the preserved\\n\\n1ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nQuestion: purple-crested turaco eats what food?\\n\\n| Discrete KV methods: 𝑆\" = 𝑓(𝑡\") | ChunkKV: 𝑆! = \\t ∑ #\" $% 𝑓 𝑡\" , \\twhere\\t𝑐\\t = \\t {𝑡%, … , 𝑡#} |\\n| --- | --- |\\n| Discrete KV methods with a low sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… Bird …… …… eat …… …… …… turacos, …… …… …… …… pulp …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… water …… …… …… …… …… …… …… leaves …… …… …… …… …… …… …… …… nuts …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a low sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… |\\n| Discrete KV methods with a high sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… …… …… eat …… …… …… turacos, …… …… …… …… …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a high sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… ………… …… …… …… |\\n\\n\\nFigure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The\\ndiscrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the\\nsubject of the words, maintaining more accurate semantic information. For the equation: S is the score function, and c is a\\nchunk of tokens.\\n\\nTable 1: Comparison of Methods on KV Cache Compression.\\n\\n| Method | KV Cache Compression | Dynamic Policy | Layer-Wise Policy | Semantic Information | Efficient Index Reuse |\\n| --- | --- | --- | --- | --- | --- |\\n| StreamingLLM (Xiao et al., 2024) | ✓ |  |  |  |  |\\n| H2O (Zhang et al., 2023) | ✓ | ✓ |  |  |  |\\n| SnapKV (Li et al., 2024) | ✓ | ✓ |  |  |  |\\n| PyramidInfer (Yang et al., 2024b) | ✓ | ✓ | ✓ |  |  |\\n| PyramidKV (Cai et al., 2024) | ✓ | ✓ | ✓ |  |  |\\n| ChunkKV(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |\\n\\n\\nKV cache indices by ChunkKV exhibit a higher similarity\\ncompared to previous methods. Consequently, we develop a\\ntechnique called layer-wise index reuse, which reduces the\\nadditional computational time introduced by the KV cache\\ncompression method. As outlined in Table 1, recent highly\\nrelevant KV cache compression methods lack the ability to\\nretain semantic information and efficiently reuse indices.\\n\\nTo evaluate ChunkKV’s performance, we conduct com-\\nprehensive experiments across multiple cutting-edge long-\\ncontext benchmarks: long-context tasks including Long-\\nBench (Bai et al., 2024) and Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023), in-context learning tasks such as\\nGSM8K (Cobbe et al., 2021', ', Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,\\nHan, X., Thai, Z. L., Wang, S., Liu, Z., et al. ∞-bench:\\nExtending long context evaluation beyond 100k tokens.\\nArXiv preprint, abs/2402.13718, 2024. URL https:\\n//arxiv.org/abs/2402.13718.\\n\\n13ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\\nR., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H2o:\\nHeavy-hitter oracle for efficient generative inference of\\nlarge language models. Advances in Neural Information\\nProcessing Systems, 36:34661–34710, 2023.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\\nvances in Neural Information Processing Systems, 36:\\n46595–46623, 2023.\\n\\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\\nAwadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and\\nRadev, D. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. In Toutanova, K.,\\nRumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Belt-\\nagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and\\nZhou, Y. (eds.), Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 5905–5921, Online, 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n472. URL https://aclanthology.org/2021.\\nnaacl-main.472.\\n\\nZhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y.,\\nCotterell, R., and Sachan, M. Recurrentgpt: Interactive\\ngeneration of (arbitrarily) long text, 2023.\\n\\nZhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., and Han, B.\\nCan language models perform robust reasoning in chain-\\nof-thought prompting with noisy rationales? In The\\nThirty-eighth Annual Conference on Neural Information\\nProcessing Systems, 2024.\\n\\n14ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nAppendix\\nA In-depth Analysis of ChunkKV vs. Discrete Token Methods 16\\nA.1 Quantitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.2 Hypothetical Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 Comparative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.4 Implications for Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB Additional Experiments 18\\nB.1 Layer-Wise Index Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB.2 LongBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nB.3 Needle-In-A-Haystack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nB.4 Chunk Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nB.5 Multi-Lingual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nC Theoretical Understanding 30\\nD Additional Related Work 33\\nE Statistics of Models 33\\nF Statistics of Datasets 34\\nG Prompt 34\\nH Limitations 35\\nI Licenses 35\\n\\n15ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# A. In-depth Analysis of ChunkKV vs. Discrete Token Methods\\n\\nA.1. Quantitative Analysis\\n\\nTo rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic\\nexperiments using a LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of\\nLongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine\\nsimilarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground\\ntruth. 2. Applied ChunkKV,', \" is a layer-wise adaptive learn-\\ning rate method that speeds up LLM training by stabilizing\\ntraining convergence with large batch sizes. DoLa (Chuang\\net al., 2023) employs layer-wise contrasting to reduce hallu-\\ncinations during LLM inference.\\n\\n# 3. ChunkKV\\n\\n3.1. Preliminary Study of KV Cache Compression\\n\\nWith the increasing long-context capabilities of LLMs, the\\nKV cache has become crucial for improving inference effi-\\nciency. However, it can consume significant GPU memory\\nwhen handling long input contexts. The GPU memory cost\\nof the KV cache for the decoding stage can be calculated as\\nfollows:\\n\\n$$M_{K V}=2\\\\times B\\\\times S\\\\times L\\\\times N\\\\times D\\\\times2\\\\qquad\\\\qquad(1)$$\\n\\nwhere B is the batch size, S is the sequence length of prompt\\nand decoded length, L is the number of layers, N is the num-\\nber of attention heads, D is the dimension of each attention\\nhead, and the first 2 accounts for the KV matrices, while\\nthe last 2 accounts for the precision when using float16.\\nTable E shows the configuration parameters for LLaMA-\\n3-8B-Instruct (Meta, 2024). With a batch size B = 1 and\\na sequence length of prompt S = 2048, the GPU mem-\\nory cost of the KV cache is nearly 1 GB. If the batch size\\nexceeds 24, the GPU memory cost of the KV cache will\\nexceed the capacity of an RTX 4090 GPU. To address this\\nissue, KV cache compression methods have been proposed,\\nwith the aim of retaining only a minimal amount of KV\\ncache while preserving as much information as possible.\\nFor more details on the LLM configuration parameters, re-\\nfer to Appendix E.\\n\\n3.2. Chunk Based KV Compression\\n\\nTo address the limitations of existing KV cache compression\\nmethods, we propose ChunkKV, a novel KV cache com-\\npression method that retains the most informative semantic\\nchunks. The key idea behind ChunkKV is to group tokens\\nin the KV cache into chunks that preserve more semantic\\ninformation, such as a chunk containing a subject, verb and\\nobject. As illustrated in Figure 1, ChunkKV preserves the\\nchunks of the KV cache that contain more semantic infor-\\nmation. First, we define a chunk as a group of tokens that\\ncontain related semantic information. By retaining the most\\ninformative chunks from the original KV cache, ChunkKV\\ncan effectively reduce the memory usage of the KV cache\\nwhile preserving essential information.\\n\\nThe Algorithm 1 shows the pseudocode outline of ChunkKV.\\nFirst, following H2O (Zhang et al., 2023) and SnapKV (Li\\net al., 2024), we set the observe window by computing the\\nobservation scores A ← QTq−w:Tq KT , where QTq−w:Tq\\n\\n3ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n| Algorithm 1 ChunkKV |\\n| --- |\\n| Input: Q E RTqxd K E RTkxd v E RTvxd observe , , window size w, chunk size c, compressed KV cache max length Lmax |\\n| Output: Compressed KV cache K', V' |\\n| Observe Window Calculation: |\\n| A ← QTq-w:Tq KT {Attention scores for the observe window} |\\n| C ← [ ] I {Calculate the number of chunks} |\\n| Chunk Attention Score Calculation: |\\n| for 2 = 1 to C do |\\n| Ai ← �j=(i-1)c+1 A:⌀ {Sum of observation scores for each chunk} |\\n| end for |\\n| Top-K Chunk Selection: |\\n| k ← Lmax c |\\n| Top K_ Indices ← indices of Top-k chunks based on Ai Compression: |\\n| K', V' ← index_select(K, V, Top_K_Indices) |\\n| Concatenation: |\\n| K' ← concat(Ko:Lmax -w' KTk-w:Tk) |\\n| v Tv-w:Tv ) V' ← concat(Vo:Lmax |\\n| -w' K', V' |\\n\\n\\nis the observe window, K is the Key matrix and the win-\\ndow size w is usually set to {4,8, 16, 32}. Next, the num-\\nber of chunks C is calculated as C = [ Te 1, where Tk is\\nthe length of the Key matrix and c is the chunk size. The\\nobservation scores for each chunk are then computed as\\nAi = �j=(i-1)c+1 A:⌀ for 2 = 1, 2, · · · , C. Referring to\\nprevious works (Zhang et al., 2023; Li et al., 2024; Yang\\net al., 2024b; Cai et al., 2024), we still use the top-k algo-\\nrithm as ChunkKV's sampling policy. For the top-k chunk\\nselection, the top-k chunks are selected based on their obser-\\nvation scores, where k = Lmax I, and Lmax is\", '# ChunkKV: Semantic-Preserving KV Cache Compression for\\nEfficient Long-Context LLM Inference\\n\\nXiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1\\n\\n# Abstract\\n\\nTo reduce memory costs in long-context inference\\nwith Large Language Models (LLMs), many re-\\ncent works focus on compressing the key-value\\n(KV) cache of different tokens. However, we\\nidentify that the previous KV cache compression\\nmethods measure token importance individually,\\nneglecting the dependency between different to-\\nkens in the real-world language characterics. In\\nlight of this, we introduce ChunkKV, grouping the\\ntokens in a chunk as a basic compressing unit, and\\nretaining the most informative semantic chunks\\nwhile discarding the less important ones. Further-\\nmore, observing that ChunkKV exhibits higher\\nsimilarity in the preserved indices across differ-\\nent layers, we propose layer-wise index reuse\\nto further reduce computational overhead. We\\nevaluated ChunkKV on cutting-edge long-context\\nbenchmarks including LongBench and Needle-\\nIn-A-HayStack, as well as the GSM8K and Jail-\\nbreakV in-context learning benchmark. Our ex-\\nperiments with instruction tuning and multi-step\\nreasoning (O1 and R1) LLMs, achieve up to 10%\\nperformance improvement under aggressive com-\\npression ratios compared to existing methods.\\n\\n# 1. Introduction\\n\\n2025\\nFeb\\n1\\n[cs.CL]\\narXiv:2502.00299v1\\n\\nLarge Language Models (LLMs) have become essential for\\naddressing various downstream tasks of natural language\\nprocessing (NLP), including summarization and question\\nanswering, which require the interpretation of a long con-\\ntext from sources such as books, reports, and documents,\\noften encompassing tens of thousands of tokens (Brown\\net al., 2020; Tay et al., 2022; Touvron et al., 2023). Re-\\ncent advances in long-context technology within the field of\\n\\n1The Hong Kong University of Science and Technol-\\nogy(Guangzhou), Guangzhou, China 2The Hong Kong University\\nof Science and Technology, Hong Kong, China. Correspondence\\nto: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu\\n<xwchu@hkust-gz.edu.cn>.\\n\\nmachine learning (ML) systems (Dao, 2024; Jacobs et al.,\\n2023; Xiao et al., 2024) have significantly enhanced com-\\nputational throughputs and reduced latency of LLMs to\\nprocess increasingly large input context lengths (Liu et al.,\\n2024b; Young et al., 2024) with saving historical KV cache\\n(key value attentions). However, the memory requirement\\nof the KV cache in serving super-long contexts becomes a\\nnew bottlneck (Zhang et al., 2023; Reid et al., 2024). For\\ninstance, the KV cache for a single token in a 7B-parameter\\nmodel requires approximately 0.5 MB of GPU memory, re-\\nsulting in a 10,000-token prompt consuming around 5 GB\\nof GPU memory.\\n\\nTo address the substantial GPU memory consumption\\ncaused by KV caching, recent studies consider compressing\\nthe KV cache by pruning non-important discrete parts from\\nthe prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge\\net al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al.,\\n2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang\\net al., 2023) and SnapKV (Li et al., 2024) have shown that\\nretaining less than 50% of the discrete KV cache can signif-\\nicantly reduce GPU memory usage with minimal impact on\\nperformance. However, we identify that the previous KV\\ncache compression methods (Zhang et al., 2023; Cai et al.,\\n2024) measure token importance isolatedly, neglecting the\\ndependency between different tokens in the real-world lan-\\nguage characterics. For example, as shown in Figure 1,\\nfocusing on token-level importance might excessively fo-\\ncus on words about subjects “turaco” in the question while\\nomitting crucial information about the objects (foods) in the\\ndocuments, resulting the loss of essential semantic informa-\\ntion. This motivates us to rethink the following question:\\n\\nHow to avoid isolated token importance measurement and\\npreserve the semantic information in KV cache?\\n\\nIn light of this, we observe that the complete semantic ', 'essing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\\nM., Petroni, F., and Liang, P. Lost in the middle: How\\nlanguage models use long contexts. Transactions of the\\nAssociation for Computational Linguistics, 12:157–173,\\n2024c. doi: 10.1162/tacl_a_00638. URL https://\\naclanthology.org/2024.tacl-1.9.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\\nrepository-level code auto-completion systems. In The\\nTwelfth International Conference on Learning Represen-\\ntations, 2024d. URL https://openreview.net/\\nforum?id=pPjZIOuQuF.\\n\\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyril-\\nlidis, A., and Shrivastava, A. Scissorhands: Exploiting\\nthe persistence of importance hypothesis for llm kv cache\\ncompression at test time. Advances in Neural Information\\nProcessing Systems, 36, 2024e.\\n\\nLuo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv:\\nA benchmark for assessing the robustness of multimodal\\nlarge language models against jailbreak attacks. In First\\nConference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=GC4mXVfquq.\\n\\nMeta. Introducing meta llama 3: The most capable openly\\navailable llm to date. https://ai.meta.com/\\nblog/meta-llama-3/, 2024. Accessed: 2024-06-\\n07.\\n\\nMohtas', 'king, the continuously chunk-\\nlevel KV cache preserves the whole examples (semantic\\ninformation) in ICL, thus reducing the requirement on dis-\\ntinguishability, i.e lower bound of KL divergence between\\nthe example and the question (Equation 4 in Condition 2).\\nThe complete analysis is provided in Appendix C.\\n\\n# 4. Experiment Results\\n\\nIn this section, we conduct experiments to evaluate the ef-\\nfectiveness of ChunkKV on KV cache compression in two\\nbenchmark fields, with a chunk size set to 10 even for vari-\\nous model architectures. The first is the In-Context Learn-\\ning benchmark, for which we select GSM8K (Cobbe et al.,\\n2021) and Jailbreakv (Luo et al., 2024) to evaluate the perfor-\\nmance of ChunkKV, furthermore we also include multi-step\\nreasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al.,\\n2025) to evaluate the performance of ChunkKV. The In-\\nContext Learning scenario is a crucial capability for LLMs\\nand has been adapted in many powerful technologies such as\\nChain-of-Thought (Wei et al., 2022; Diao et al., 2024; Pan\\net al., 2024b). The second is the Long-Context benchmark,\\nwhich includes LongBench (Bai et al., 2024) and Needle-In-\\nA-HayStack (NIAH) (Kamradt, 2023), both widely used for\\nassessing KV cache compression methods. All experiments\\nwere conducted three times, using the mean score to ensure\\nrobustness.\\n\\n4.1. In-Context Learning\\n\\nThe In-Context Learning (ICL) ability significantly en-\\nhances the impact of prompts on large language models\\n(LLMs). For example, the Chain-of-Thought approach (Wei\\n\\net al., 2022) increases the accuracy of the GSM8K of the\\nPaLM model (Chowdhery et al., 2022) from 18% to 57%\\nwithout additional training. In this section, we evaluate\\nthe performance of ChunkKV on the GSM8K, Many-Shot\\nGSM8K (Agarwal et al., 2024), and JailbreakV (Luo et al.,\\n2024) benchmarks.\\n\\nTable 3: GSM8K Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ |\\n| 10% | 51.6% | 55.6% | 57.6% | 62.6% | 65.7% |\\n| LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ |\\n| 30% | 70.5% | 72.2% | 76.1% | 77.1% | 77.3% |\\n| 20% | 63.8% | 64.0% | 68.8% | 71.4% | 77.6% |\\n| 10% | 47.8% | 45.0% | 50.3% | 48.2% | 65.7% |\\n| LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ |\\n| 30% | 70.6% | 73.6% | 70.2% | 68.2% | 74.6% |\\n| Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ |\\n| 30% | 70.8% | 61.2% | 70.8% | 64.7% | 73.5% |\\n\\n\\nGSM8K In the in-context learning scenario, we\\nevaluated multiple KV cache compression methods\\nfor GSM8K (Cobbe et al., 2021), which contains\\nmore than 1,000 arithmetic questions on LLaMA-3-8B-\\nInstruct, LLaMA-3.1-8B-Instruct (Meta, 2024), Qwen2-\\n7B-Instruct (Yang et al., 2024a) and DeepSeek-R1-Distill-\\nLlama-8B (Guo et al., 2025). Follow the Agarwal et al.\\n(2024), we consider many-shot GSM8K as a long-context\\nreasoning scenario, which is a more challenging task than\\nLongBench (Bai et al., 2024). The CoT prompt settings\\nfor this experiment are the same as those used by Wei et al.\\n(2022), for many-shot GSM8K we set the number of shots\\nto 50, which the prompt length is more than 4k tokens.\\n\\n5ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-', 'suring Efficiency. We evaluated the latency and\\nthroughput of ChunkKV compared to FullKV using\\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\\nwere conducted with reuse layer is 2, batch size set to 1\\nand inference was performed using Flash Attention 2, each\\nexperiment was repeated 10 times and the average latency\\nand throughput were reported.\\n\\nTable 8: Latency and throughput comparison between\\nChunkKV and FullKV under different input-output configu-\\nrations. Percentages in parentheses indicate improvements\\nover FullKV baseline.\\n\\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\\n| --- | --- | --- | --- | --- |\\n| Method | Input | Output | Latency(s) ↓ | Throughput(T/S) ↑ |\\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\\n\\n\\nThe results in Table 8 shows that the layer-wise index reuse\\nstrategy (ChunkKV_reuse) further boosts performance,\\nachieving up to a 20.7% reduction in latency, and through-\\nput improvements are particularly notable for longer input\\nsequences, with ChunkKV_reuse delivering up to a 26.5%\\nimprovement over FullKV.\\n\\nIndex Reuse Performance on LongBench\\n\\n![image](/image/placeholder)\\n- Chart Title: Index Reuse Performance on LongBench\\n- X-Axis: Number of Index Reuse Layers\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\\n\\n\\nFigure 4: Comparison with different index reuse layers on\\nLongBench.\\n\\nMeasuring Task Performance. This experiment evaluates\\nthe performance of the layer-wise index reuse approach by\\nmeasuring the performance of the LongBench (Bai et al.,\\n2024), the experiment settings are the same as LongBench\\nin 4.2. And the number of index reuse layers is set from 1\\nto the number of layers in the model, where an index reuse\\nlayer of 1 corresponds to the normal ChunkKV without\\nindex reuse, and our method set reuse layer to 2.\\n\\nFigure 4 illustrates the performance of ChunkKV with vary-\\ning index reuse layers on the LongBench benchmark. Gen-\\nerally, reuse layer set to 2 can achieve the minimal perfor-\\nmance degradation across all models. For more experiments\\n\\n7ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LongBench Performance vs Chunk Size\\n- X-Axis: Chunk Size\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | ILaMA-3-BB | Mistral-7B | Owen2-7B | LLaMA-3-BB Full kV | Mistral-7B Full kV | Qwen2-7B Full KV |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 38 | 46 | 46 | 46 | 46 | 40 |\\n\\n\\nFigure 5: LongBench Performance Comparison with differ-\\nent chunk size under 10% compression rate.\\n\\non index reuse, please refer to the APPENDIX B.1.3.\\n\\nOverall, these findings on efficiency and performance sug-\\ngest that layer-wise index reuse can be an effective technique\\nfor optimizing the efficiency-performance trade-off in KV\\ncache compression, with the potential for model-specific\\ntuning to maximize benefits.\\n\\n# 5. Ablation study\\n\\n# 5.1. Chunk Size\\n\\nThis section aims to investigate the impact of chunk size on\\nthe performance of ChunkKV. Different chunk sizes will\\nlead to varying degrees of compression on the semantic\\ninformation of the data. We set the experiemnt setting the']\n",
            "Based on the retrieved content, answer the user question.\n",
            "\n",
            "Assistant: Below is the retrieved content of the Paper.\n",
            "-----------------------------------\n",
            "\n",
            "0: ['\\nsame as in LongBench in Section 4.2. The chunk size is\\nset from the range {1, 3, 5, 10, 20, 30}. Figure 5 shows the\\nperformance of the ChunkKV with different chunk size on\\nthe LongBench and NIAH benchmarks. The three colorful\\ncurves represent three LLMs with different chunk sizes, and\\nthe colorful dashed line is the corresponding FullKV perfor-\\nmance. For more experiments on the size of the chunks with\\ndifferent compression ratios, refer to the Appendix B.4.\\n\\nTable 9: LongBench Performance with Different Chunk\\nSizes and Compression Ratios for LLaMA-3-8B-Instruct\\n\\n| Compression | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Rate | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 10% | 37.32 | 40.49 | 40.47 | 40.51 | 40.21 | 40.05 | 39.57 |\\n| 20% | 38.80 | 40.66 | 40.57 | 40.74 | 40.53 | 40.46 | 40.04 |\\n| 30% | 39.23 | 41.02 | 41.29 | 41.59 | 41.38 | 41.33 | 41.02 |\\n\\n\\nFrom Figure 5, we can observe that the LongBench per-\\nformance of ChunkKV is not significantly affected by the\\nchunk size, with performance variations less than 1%. The\\nthree curves are closely aligned, indicating that chunk sizes\\nin the range of {10, 20} exhibit better performance.\\n\\nTable 9 and 10 show the performance of ChunkKV with\\ndifferent comperession ratios and different chunk sizes on\\nthe LongBench and NIAH. We conducted extensive exper-\\niments across different compression ratios and KV cache\\nsizes to shows the effectiveness of ChunkKV and the chunk\\nsize is robust.\\n\\nTable 10: NIAH Performance with Different Chunk Sizes\\nand KV Cache Sizes for LLaMA-3-8B-Instruct\\n\\n| KV Cache | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Size | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 96 | 41.0 | 63.2 | 65.2 | 70.3 | 67.2 | 65.3 | 53.1 |\\n| 128 | 47.9 | 65.6 | 69.1 | 73.8 | 72.3 | 72.0 | 71.2 |\\n| 256 | 61.7 | 70.3 | 71.2 | 74.1 | 73.2 | 72.3 | 71.1 |\\n| 512 | 68.6 | 72.6 | 72.5 | 74.5 | 74.3 | 74.0 | 72.6 |\\n\\n\\nFrom the chunk size ablation study, we can observe that\\nacross different tasks (LongBench and NIAH) and vari-\\nous compression settings, a chunk size of 10 consistently\\ndelivers optimal or near-optimal performance. This em-\\npirical finding suggests that a chunk size of 10 strikes a\\ngood balance between preserving semantic information and\\ncompression efficiency, making it a robust default choice\\nfor ChunkKV. Therefore, we adopt this chunk size setting\\nthroughout our experiments.\\n\\n# 6. Conclusion\\n\\nWe introduced ChunkKV, a novel KV cache compression\\nmethod that preserves semantic information by retaining\\nmore informative chunks. Through extensive experiments\\nacross multiple state-of-the-art LLMs (including DeepSeek-\\nR1, LLaMA-3, Qwen2, and Mistral) and diverse bench-\\nmarks (GSM8K, LongBench, NIAH, and JailbreakV), we\\ndemonstrate that ChunkKV consistently outperforms ex-\\nisting methods while using only a fraction of the memory.\\nOur comprehensive analysis shows that ChunkKV’s chunk-\\nbased approach maintains crucial contextual information,\\nleading to superior performance in complex reasoning tasks,\\nlong-context understanding, and safety evaluations. The\\nmethod’s effectiveness is particularly evident in challenging\\nscenarios like many-shot GSM8K and multi-document QA\\ntasks, where semantic coherence is crucial. Furthermore,\\nour proposed layer-wise index reuse technique provides\\nsignificant computational efficiency gains with minimal per-\\nformance impact, achieving up to 20.7% latency reduction\\nand 26.5% throughput improvement. These findings, sup-\\nported by detailed quantitative analysis and ablation stud-\\nies, establish ChunkKV as a significant advancement in KV\\ncache compression technology, offering an effective solution\\nfor deploying LLMs in resource-constrained environments\\nwhile maintaining high-quality outputs.\\n\\n8ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n', 'hami, A. and Jaggi, M. Landmark attention:\\nRandom-access infinite context length for transformers.\\nArXiv preprint, abs/2305.16300, 2023. URL https:\\n//arxiv.org/abs/2305.16300.\\n\\nOpenAI. Gpt-4o-mini: Advancing cost-efficient intelli-\\ngence, 2023. Accessed: 2023-12-14.\\n\\nPan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C.,\\nand Zhang, T. Lisa: Layerwise importance sampling\\nfor memory-efficient large language model fine-tuning.\\nArXiv preprint, abs/2403.17919, 2024a. URL https:\\n//arxiv.org/abs/2403.17919.\\n\\nPan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K.,\\nZhang, J., Pi, R., and Zhang, T. Plum: Prompt learning us-\\ning metaheuristics. In Ku, L.-W., Martins, A., and Sriku-\\nmar, V. (eds.), Findings of the Association for Computa-\\ntional Linguistics ACL 2024, pp. 2177–2197, Bangkok,\\nThailand and virtual meeting, August 2024b. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2024.\\nfindings-acl.129. URL https://aclanthology.\\norg/2024.findings-acl.129.\\n\\nPires, B. Á. and Szepesvári, C. Multiclass classification\\ncalibration functions. arXiv preprint arXiv:1609.06385,\\n2016.\\n\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lill-\\nicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Fi-\\nrat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of\\ncontext. ArXiv preprint, abs/2403.05530, 2024. URL\\nhttps://arxiv.org/abs/2403.05530.\\n\\nShaham, U., Ivgi, M., Efrat, A., Berant, J., and\\nLevy, O. ZeroSCROLLS: A zero-shot benchmark for\\nlong text understanding. In Bouamor, H., Pino, J.,\\nand Bali, K. (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023, pp. 7977–\\n7989, Singapore, 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.\\n536. URL https://aclanthology.org/2023.\\nfindings-emnlp.536.\\n\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V.,\\nSmith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis,\\nM. In-context pretraining: Language modeling beyond\\ndocument boundaries. In The Twelfth International Con-\\nference on Learning Representations.\\n\\nSmith, B. and Troynikov, A. Evaluating chunking\\nstrategies for retrieval. Technical report, Chroma,\\n2024. URL https://research.trychroma.\\ncom/evaluating-chunking.\\n\\nSteinwart, I. How to compare different loss functions and\\ntheir risks. Constructive Approximation, 26:225–287,\\n2007. URL https://api.semanticscholar.\\norg/CorpusID:16660598.\\n\\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma,\\nS., Zhang, Q., Wang, J., and Wei, F. You only cache\\nonce: Decoder-decoder architectures for language models.\\narXiv preprint arXiv:2405.05254, 2024.\\n\\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\\nS. Quest: Query-aware sparsity for efficient long-context\\nllm inference. ArXiv preprint, abs/2406.10774, 2024.\\nURL https://arxiv.org/abs/2406.10774.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,\\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\\nD. Long range arena : A benchmark for efficient trans-\\nformers. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=qVyeW-grC2k.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\\nUnifying language learning paradigms. ArXiv preprint,\\nabs/2205.05131, 2022. URL https://arxiv.org/\\nabs/2205.05131.\\n\\n12ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundatio', ') and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nreasoning (O1 and R1) LLMs, achieving state-of-the-art\\nperformance.\\n\\n# 2. Related Work\\n\\nKV Cache Compression. KV cache compression technol-\\nogy has developed rapidly in the era of LLM, with meth-\\nods mainly focused on evicting unimportant tokens. The\\ncompression process occurs before the attention blocks, op-\\ntimizing both the prefilling time and GPU memory. Xiao\\net al. (2024) and Han et al. (2024) propose that initial and re-\\ncent tokens consistently have high attention scores between\\ndifferent layers and attention heads. As a result, retaining\\nthese tokens in the KV cache is more likely to preserve im-\\nportant information. Furthermore, FastGen (Ge et al., 2023)\\nevicts tokens based on observed patterns. H2O (Zhang et al.,\\n2023) and SnapKV (Li et al., 2024) employ dynamic KV\\ncache compression methods, evaluating the importance of\\ntokens based on attention scores and then evicting the less\\nimportant ones. As inference scenarios become increas-\\ningly complex, dynamic KV cache compression methods\\ndemonstrate powerful performance. Recently, Yang et al.\\n(2024b) and Cai et al. (2024) have closely examined the dis-\\ntributions of attention scores during the pre-filling stage of\\nthe Retrieval-Augmented Generation (RAG) task, discover-\\ning a pyramidal KV cache compression pattern in different\\ntransformer layers.\\n\\nAlthough these KV cache compression methods have ex-\\nplored efficient GPU memory management while maintain-\\ning original performance, our study focuses more on the\\nsemantic information of the prompt. We find that chunks\\nof the original KV cache are more important than discrete\\ntokens.\\n\\nChunking Method. The chunking methodology is widely\\nused in the field of NLP due to its simplicity and effective-\\nness (Tjong Kim Sang & Veenstra, 1999). In the era of\\nLLMs, chunking is primarily applied in data pre-processing.\\nFor example, Shi et al. suggest grouping related train-\\ning data into chunks to achieve better convergence curves\\nto pre-train LLMs. Fei et al. (2024) apply a topic-based\\nchunking method to improve the semantic compression of\\nprompts. Furthermore, chunking plays an important role in\\nthe Retrieval-Augmented Generation (RAG) field (Yepes\\net al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It\\nserves to divide documents into units of information with\\nsemantic content suitable for embedding-based retrieval and\\nprocessing by LLMs.\\n\\nLayer-Wise Technique The layer-wise technique is widely\\nused in the training and inference of large language models\\n(LLMs). LISA (Pan et al., 2024a) is a layer-wise sampling\\nmethod based on observations of the training dynamics of\\nLow-Rank Adaptation (LoRA)(Hu et al., 2022) across lay-\\n\\ners. LAMB(You et al., 2020)', 'in-\\nformation usually appear in a continuous sequence (Fang &\\nXie, 2022). Thus, we introduce a straightforward yet effec-\\ntive ChunkKV, grouping the tokens in a chunk as a basic\\ncompressing unit, which should be preserved or discarded\\nas a whole. Thus, it retains the most informative semantic\\nchunks from the original KV cache. As shown in Figure 1,\\npreserving a chunk helps to catch the subject, predicate,\\nand object. Furthermore, we investigate that the preserved\\n\\n1ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nQuestion: purple-crested turaco eats what food?\\n\\n| Discrete KV methods: 𝑆\" = 𝑓(𝑡\") | ChunkKV: 𝑆! = \\t ∑ #\" $% 𝑓 𝑡\" , \\twhere\\t𝑐\\t = \\t {𝑡%, … , 𝑡#} |\\n| --- | --- |\\n| Discrete KV methods with a low sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… Bird …… …… eat …… …… …… turacos, …… …… …… …… pulp …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… water …… …… …… …… …… …… …… leaves …… …… …… …… …… …… …… …… nuts …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a low sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… |\\n| Discrete KV methods with a high sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… …… …… eat …… …… …… turacos, …… …… …… …… …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a high sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… ………… …… …… …… |\\n\\n\\nFigure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The\\ndiscrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the\\nsubject of the words, maintaining more accurate semantic information. For the equation: S is the score function, and c is a\\nchunk of tokens.\\n\\nTable 1: Comparison of Methods on KV Cache Compression.\\n\\n| Method | KV Cache Compression | Dynamic Policy | Layer-Wise Policy | Semantic Information | Efficient Index Reuse |\\n| --- | --- | --- | --- | --- | --- |\\n| StreamingLLM (Xiao et al., 2024) | ✓ |  |  |  |  |\\n| H2O (Zhang et al., 2023) | ✓ | ✓ |  |  |  |\\n| SnapKV (Li et al., 2024) | ✓ | ✓ |  |  |  |\\n| PyramidInfer (Yang et al., 2024b) | ✓ | ✓ | ✓ |  |  |\\n| PyramidKV (Cai et al., 2024) | ✓ | ✓ | ✓ |  |  |\\n| ChunkKV(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |\\n\\n\\nKV cache indices by ChunkKV exhibit a higher similarity\\ncompared to previous methods. Consequently, we develop a\\ntechnique called layer-wise index reuse, which reduces the\\nadditional computational time introduced by the KV cache\\ncompression method. As outlined in Table 1, recent highly\\nrelevant KV cache compression methods lack the ability to\\nretain semantic information and efficiently reuse indices.\\n\\nTo evaluate ChunkKV’s performance, we conduct com-\\nprehensive experiments across multiple cutting-edge long-\\ncontext benchmarks: long-context tasks including Long-\\nBench (Bai et al., 2024) and Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023), in-context learning tasks such as\\nGSM8K (Cobbe et al., 2021', ', Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,\\nHan, X., Thai, Z. L., Wang, S., Liu, Z., et al. ∞-bench:\\nExtending long context evaluation beyond 100k tokens.\\nArXiv preprint, abs/2402.13718, 2024. URL https:\\n//arxiv.org/abs/2402.13718.\\n\\n13ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\\nR., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H2o:\\nHeavy-hitter oracle for efficient generative inference of\\nlarge language models. Advances in Neural Information\\nProcessing Systems, 36:34661–34710, 2023.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\\nvances in Neural Information Processing Systems, 36:\\n46595–46623, 2023.\\n\\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\\nAwadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and\\nRadev, D. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. In Toutanova, K.,\\nRumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Belt-\\nagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and\\nZhou, Y. (eds.), Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 5905–5921, Online, 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n472. URL https://aclanthology.org/2021.\\nnaacl-main.472.\\n\\nZhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y.,\\nCotterell, R., and Sachan, M. Recurrentgpt: Interactive\\ngeneration of (arbitrarily) long text, 2023.\\n\\nZhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., and Han, B.\\nCan language models perform robust reasoning in chain-\\nof-thought prompting with noisy rationales? In The\\nThirty-eighth Annual Conference on Neural Information\\nProcessing Systems, 2024.\\n\\n14ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nAppendix\\nA In-depth Analysis of ChunkKV vs. Discrete Token Methods 16\\nA.1 Quantitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.2 Hypothetical Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 Comparative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.4 Implications for Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB Additional Experiments 18\\nB.1 Layer-Wise Index Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB.2 LongBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nB.3 Needle-In-A-Haystack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nB.4 Chunk Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nB.5 Multi-Lingual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nC Theoretical Understanding 30\\nD Additional Related Work 33\\nE Statistics of Models 33\\nF Statistics of Datasets 34\\nG Prompt 34\\nH Limitations 35\\nI Licenses 35\\n\\n15ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# A. In-depth Analysis of ChunkKV vs. Discrete Token Methods\\n\\nA.1. Quantitative Analysis\\n\\nTo rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic\\nexperiments using a LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of\\nLongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine\\nsimilarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground\\ntruth. 2. Applied ChunkKV,', \" is a layer-wise adaptive learn-\\ning rate method that speeds up LLM training by stabilizing\\ntraining convergence with large batch sizes. DoLa (Chuang\\net al., 2023) employs layer-wise contrasting to reduce hallu-\\ncinations during LLM inference.\\n\\n# 3. ChunkKV\\n\\n3.1. Preliminary Study of KV Cache Compression\\n\\nWith the increasing long-context capabilities of LLMs, the\\nKV cache has become crucial for improving inference effi-\\nciency. However, it can consume significant GPU memory\\nwhen handling long input contexts. The GPU memory cost\\nof the KV cache for the decoding stage can be calculated as\\nfollows:\\n\\n$$M_{K V}=2\\\\times B\\\\times S\\\\times L\\\\times N\\\\times D\\\\times2\\\\qquad\\\\qquad(1)$$\\n\\nwhere B is the batch size, S is the sequence length of prompt\\nand decoded length, L is the number of layers, N is the num-\\nber of attention heads, D is the dimension of each attention\\nhead, and the first 2 accounts for the KV matrices, while\\nthe last 2 accounts for the precision when using float16.\\nTable E shows the configuration parameters for LLaMA-\\n3-8B-Instruct (Meta, 2024). With a batch size B = 1 and\\na sequence length of prompt S = 2048, the GPU mem-\\nory cost of the KV cache is nearly 1 GB. If the batch size\\nexceeds 24, the GPU memory cost of the KV cache will\\nexceed the capacity of an RTX 4090 GPU. To address this\\nissue, KV cache compression methods have been proposed,\\nwith the aim of retaining only a minimal amount of KV\\ncache while preserving as much information as possible.\\nFor more details on the LLM configuration parameters, re-\\nfer to Appendix E.\\n\\n3.2. Chunk Based KV Compression\\n\\nTo address the limitations of existing KV cache compression\\nmethods, we propose ChunkKV, a novel KV cache com-\\npression method that retains the most informative semantic\\nchunks. The key idea behind ChunkKV is to group tokens\\nin the KV cache into chunks that preserve more semantic\\ninformation, such as a chunk containing a subject, verb and\\nobject. As illustrated in Figure 1, ChunkKV preserves the\\nchunks of the KV cache that contain more semantic infor-\\nmation. First, we define a chunk as a group of tokens that\\ncontain related semantic information. By retaining the most\\ninformative chunks from the original KV cache, ChunkKV\\ncan effectively reduce the memory usage of the KV cache\\nwhile preserving essential information.\\n\\nThe Algorithm 1 shows the pseudocode outline of ChunkKV.\\nFirst, following H2O (Zhang et al., 2023) and SnapKV (Li\\net al., 2024), we set the observe window by computing the\\nobservation scores A ← QTq−w:Tq KT , where QTq−w:Tq\\n\\n3ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n| Algorithm 1 ChunkKV |\\n| --- |\\n| Input: Q E RTqxd K E RTkxd v E RTvxd observe , , window size w, chunk size c, compressed KV cache max length Lmax |\\n| Output: Compressed KV cache K', V' |\\n| Observe Window Calculation: |\\n| A ← QTq-w:Tq KT {Attention scores for the observe window} |\\n| C ← [ ] I {Calculate the number of chunks} |\\n| Chunk Attention Score Calculation: |\\n| for 2 = 1 to C do |\\n| Ai ← �j=(i-1)c+1 A:⌀ {Sum of observation scores for each chunk} |\\n| end for |\\n| Top-K Chunk Selection: |\\n| k ← Lmax c |\\n| Top K_ Indices ← indices of Top-k chunks based on Ai Compression: |\\n| K', V' ← index_select(K, V, Top_K_Indices) |\\n| Concatenation: |\\n| K' ← concat(Ko:Lmax -w' KTk-w:Tk) |\\n| v Tv-w:Tv ) V' ← concat(Vo:Lmax |\\n| -w' K', V' |\\n\\n\\nis the observe window, K is the Key matrix and the win-\\ndow size w is usually set to {4,8, 16, 32}. Next, the num-\\nber of chunks C is calculated as C = [ Te 1, where Tk is\\nthe length of the Key matrix and c is the chunk size. The\\nobservation scores for each chunk are then computed as\\nAi = �j=(i-1)c+1 A:⌀ for 2 = 1, 2, · · · , C. Referring to\\nprevious works (Zhang et al., 2023; Li et al., 2024; Yang\\net al., 2024b; Cai et al., 2024), we still use the top-k algo-\\nrithm as ChunkKV's sampling policy. For the top-k chunk\\nselection, the top-k chunks are selected based on their obser-\\nvation scores, where k = Lmax I, and Lmax is\", '# ChunkKV: Semantic-Preserving KV Cache Compression for\\nEfficient Long-Context LLM Inference\\n\\nXiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1\\n\\n# Abstract\\n\\nTo reduce memory costs in long-context inference\\nwith Large Language Models (LLMs), many re-\\ncent works focus on compressing the key-value\\n(KV) cache of different tokens. However, we\\nidentify that the previous KV cache compression\\nmethods measure token importance individually,\\nneglecting the dependency between different to-\\nkens in the real-world language characterics. In\\nlight of this, we introduce ChunkKV, grouping the\\ntokens in a chunk as a basic compressing unit, and\\nretaining the most informative semantic chunks\\nwhile discarding the less important ones. Further-\\nmore, observing that ChunkKV exhibits higher\\nsimilarity in the preserved indices across differ-\\nent layers, we propose layer-wise index reuse\\nto further reduce computational overhead. We\\nevaluated ChunkKV on cutting-edge long-context\\nbenchmarks including LongBench and Needle-\\nIn-A-HayStack, as well as the GSM8K and Jail-\\nbreakV in-context learning benchmark. Our ex-\\nperiments with instruction tuning and multi-step\\nreasoning (O1 and R1) LLMs, achieve up to 10%\\nperformance improvement under aggressive com-\\npression ratios compared to existing methods.\\n\\n# 1. Introduction\\n\\n2025\\nFeb\\n1\\n[cs.CL]\\narXiv:2502.00299v1\\n\\nLarge Language Models (LLMs) have become essential for\\naddressing various downstream tasks of natural language\\nprocessing (NLP), including summarization and question\\nanswering, which require the interpretation of a long con-\\ntext from sources such as books, reports, and documents,\\noften encompassing tens of thousands of tokens (Brown\\net al., 2020; Tay et al., 2022; Touvron et al., 2023). Re-\\ncent advances in long-context technology within the field of\\n\\n1The Hong Kong University of Science and Technol-\\nogy(Guangzhou), Guangzhou, China 2The Hong Kong University\\nof Science and Technology, Hong Kong, China. Correspondence\\nto: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu\\n<xwchu@hkust-gz.edu.cn>.\\n\\nmachine learning (ML) systems (Dao, 2024; Jacobs et al.,\\n2023; Xiao et al., 2024) have significantly enhanced com-\\nputational throughputs and reduced latency of LLMs to\\nprocess increasingly large input context lengths (Liu et al.,\\n2024b; Young et al., 2024) with saving historical KV cache\\n(key value attentions). However, the memory requirement\\nof the KV cache in serving super-long contexts becomes a\\nnew bottlneck (Zhang et al., 2023; Reid et al., 2024). For\\ninstance, the KV cache for a single token in a 7B-parameter\\nmodel requires approximately 0.5 MB of GPU memory, re-\\nsulting in a 10,000-token prompt consuming around 5 GB\\nof GPU memory.\\n\\nTo address the substantial GPU memory consumption\\ncaused by KV caching, recent studies consider compressing\\nthe KV cache by pruning non-important discrete parts from\\nthe prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge\\net al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al.,\\n2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang\\net al., 2023) and SnapKV (Li et al., 2024) have shown that\\nretaining less than 50% of the discrete KV cache can signif-\\nicantly reduce GPU memory usage with minimal impact on\\nperformance. However, we identify that the previous KV\\ncache compression methods (Zhang et al., 2023; Cai et al.,\\n2024) measure token importance isolatedly, neglecting the\\ndependency between different tokens in the real-world lan-\\nguage characterics. For example, as shown in Figure 1,\\nfocusing on token-level importance might excessively fo-\\ncus on words about subjects “turaco” in the question while\\nomitting crucial information about the objects (foods) in the\\ndocuments, resulting the loss of essential semantic informa-\\ntion. This motivates us to rethink the following question:\\n\\nHow to avoid isolated token importance measurement and\\npreserve the semantic information in KV cache?\\n\\nIn light of this, we observe that the complete semantic ', 'essing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\\nM., Petroni, F., and Liang, P. Lost in the middle: How\\nlanguage models use long contexts. Transactions of the\\nAssociation for Computational Linguistics, 12:157–173,\\n2024c. doi: 10.1162/tacl_a_00638. URL https://\\naclanthology.org/2024.tacl-1.9.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\\nrepository-level code auto-completion systems. In The\\nTwelfth International Conference on Learning Represen-\\ntations, 2024d. URL https://openreview.net/\\nforum?id=pPjZIOuQuF.\\n\\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyril-\\nlidis, A., and Shrivastava, A. Scissorhands: Exploiting\\nthe persistence of importance hypothesis for llm kv cache\\ncompression at test time. Advances in Neural Information\\nProcessing Systems, 36, 2024e.\\n\\nLuo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv:\\nA benchmark for assessing the robustness of multimodal\\nlarge language models against jailbreak attacks. In First\\nConference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=GC4mXVfquq.\\n\\nMeta. Introducing meta llama 3: The most capable openly\\navailable llm to date. https://ai.meta.com/\\nblog/meta-llama-3/, 2024. Accessed: 2024-06-\\n07.\\n\\nMohtas', 'king, the continuously chunk-\\nlevel KV cache preserves the whole examples (semantic\\ninformation) in ICL, thus reducing the requirement on dis-\\ntinguishability, i.e lower bound of KL divergence between\\nthe example and the question (Equation 4 in Condition 2).\\nThe complete analysis is provided in Appendix C.\\n\\n# 4. Experiment Results\\n\\nIn this section, we conduct experiments to evaluate the ef-\\nfectiveness of ChunkKV on KV cache compression in two\\nbenchmark fields, with a chunk size set to 10 even for vari-\\nous model architectures. The first is the In-Context Learn-\\ning benchmark, for which we select GSM8K (Cobbe et al.,\\n2021) and Jailbreakv (Luo et al., 2024) to evaluate the perfor-\\nmance of ChunkKV, furthermore we also include multi-step\\nreasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al.,\\n2025) to evaluate the performance of ChunkKV. The In-\\nContext Learning scenario is a crucial capability for LLMs\\nand has been adapted in many powerful technologies such as\\nChain-of-Thought (Wei et al., 2022; Diao et al., 2024; Pan\\net al., 2024b). The second is the Long-Context benchmark,\\nwhich includes LongBench (Bai et al., 2024) and Needle-In-\\nA-HayStack (NIAH) (Kamradt, 2023), both widely used for\\nassessing KV cache compression methods. All experiments\\nwere conducted three times, using the mean score to ensure\\nrobustness.\\n\\n4.1. In-Context Learning\\n\\nThe In-Context Learning (ICL) ability significantly en-\\nhances the impact of prompts on large language models\\n(LLMs). For example, the Chain-of-Thought approach (Wei\\n\\net al., 2022) increases the accuracy of the GSM8K of the\\nPaLM model (Chowdhery et al., 2022) from 18% to 57%\\nwithout additional training. In this section, we evaluate\\nthe performance of ChunkKV on the GSM8K, Many-Shot\\nGSM8K (Agarwal et al., 2024), and JailbreakV (Luo et al.,\\n2024) benchmarks.\\n\\nTable 3: GSM8K Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ |\\n| 10% | 51.6% | 55.6% | 57.6% | 62.6% | 65.7% |\\n| LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ |\\n| 30% | 70.5% | 72.2% | 76.1% | 77.1% | 77.3% |\\n| 20% | 63.8% | 64.0% | 68.8% | 71.4% | 77.6% |\\n| 10% | 47.8% | 45.0% | 50.3% | 48.2% | 65.7% |\\n| LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ |\\n| 30% | 70.6% | 73.6% | 70.2% | 68.2% | 74.6% |\\n| Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ |\\n| 30% | 70.8% | 61.2% | 70.8% | 64.7% | 73.5% |\\n\\n\\nGSM8K In the in-context learning scenario, we\\nevaluated multiple KV cache compression methods\\nfor GSM8K (Cobbe et al., 2021), which contains\\nmore than 1,000 arithmetic questions on LLaMA-3-8B-\\nInstruct, LLaMA-3.1-8B-Instruct (Meta, 2024), Qwen2-\\n7B-Instruct (Yang et al., 2024a) and DeepSeek-R1-Distill-\\nLlama-8B (Guo et al., 2025). Follow the Agarwal et al.\\n(2024), we consider many-shot GSM8K as a long-context\\nreasoning scenario, which is a more challenging task than\\nLongBench (Bai et al., 2024). The CoT prompt settings\\nfor this experiment are the same as those used by Wei et al.\\n(2022), for many-shot GSM8K we set the number of shots\\nto 50, which the prompt length is more than 4k tokens.\\n\\n5ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-', 'suring Efficiency. We evaluated the latency and\\nthroughput of ChunkKV compared to FullKV using\\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\\nwere conducted with reuse layer is 2, batch size set to 1\\nand inference was performed using Flash Attention 2, each\\nexperiment was repeated 10 times and the average latency\\nand throughput were reported.\\n\\nTable 8: Latency and throughput comparison between\\nChunkKV and FullKV under different input-output configu-\\nrations. Percentages in parentheses indicate improvements\\nover FullKV baseline.\\n\\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\\n| --- | --- | --- | --- | --- |\\n| Method | Input | Output | Latency(s) ↓ | Throughput(T/S) ↑ |\\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\\n\\n\\nThe results in Table 8 shows that the layer-wise index reuse\\nstrategy (ChunkKV_reuse) further boosts performance,\\nachieving up to a 20.7% reduction in latency, and through-\\nput improvements are particularly notable for longer input\\nsequences, with ChunkKV_reuse delivering up to a 26.5%\\nimprovement over FullKV.\\n\\nIndex Reuse Performance on LongBench\\n\\n![image](/image/placeholder)\\n- Chart Title: Index Reuse Performance on LongBench\\n- X-Axis: Number of Index Reuse Layers\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\\n\\n\\nFigure 4: Comparison with different index reuse layers on\\nLongBench.\\n\\nMeasuring Task Performance. This experiment evaluates\\nthe performance of the layer-wise index reuse approach by\\nmeasuring the performance of the LongBench (Bai et al.,\\n2024), the experiment settings are the same as LongBench\\nin 4.2. And the number of index reuse layers is set from 1\\nto the number of layers in the model, where an index reuse\\nlayer of 1 corresponds to the normal ChunkKV without\\nindex reuse, and our method set reuse layer to 2.\\n\\nFigure 4 illustrates the performance of ChunkKV with vary-\\ning index reuse layers on the LongBench benchmark. Gen-\\nerally, reuse layer set to 2 can achieve the minimal perfor-\\nmance degradation across all models. For more experiments\\n\\n7ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LongBench Performance vs Chunk Size\\n- X-Axis: Chunk Size\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | ILaMA-3-BB | Mistral-7B | Owen2-7B | LLaMA-3-BB Full kV | Mistral-7B Full kV | Qwen2-7B Full KV |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 38 | 46 | 46 | 46 | 46 | 40 |\\n\\n\\nFigure 5: LongBench Performance Comparison with differ-\\nent chunk size under 10% compression rate.\\n\\non index reuse, please refer to the APPENDIX B.1.3.\\n\\nOverall, these findings on efficiency and performance sug-\\ngest that layer-wise index reuse can be an effective technique\\nfor optimizing the efficiency-performance trade-off in KV\\ncache compression, with the potential for model-specific\\ntuning to maximize benefits.\\n\\n# 5. Ablation study\\n\\n# 5.1. Chunk Size\\n\\nThis section aims to investigate the impact of chunk size on\\nthe performance of ChunkKV. Different chunk sizes will\\nlead to varying degrees of compression on the semantic\\ninformation of the data. We set the experiemnt setting the']\n",
            "Based on the retrieved content, answer the user question.\n",
            "\n",
            "Assistant: Below is the retrieved content of the Paper.\n",
            "-----------------------------------\n",
            "\n",
            "0: ['\\nsame as in LongBench in Section 4.2. The chunk size is\\nset from the range {1, 3, 5, 10, 20, 30}. Figure 5 shows the\\nperformance of the ChunkKV with different chunk size on\\nthe LongBench and NIAH benchmarks. The three colorful\\ncurves represent three LLMs with different chunk sizes, and\\nthe colorful dashed line is the corresponding FullKV perfor-\\nmance. For more experiments on the size of the chunks with\\ndifferent compression ratios, refer to the Appendix B.4.\\n\\nTable 9: LongBench Performance with Different Chunk\\nSizes and Compression Ratios for LLaMA-3-8B-Instruct\\n\\n| Compression | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Rate | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 10% | 37.32 | 40.49 | 40.47 | 40.51 | 40.21 | 40.05 | 39.57 |\\n| 20% | 38.80 | 40.66 | 40.57 | 40.74 | 40.53 | 40.46 | 40.04 |\\n| 30% | 39.23 | 41.02 | 41.29 | 41.59 | 41.38 | 41.33 | 41.02 |\\n\\n\\nFrom Figure 5, we can observe that the LongBench per-\\nformance of ChunkKV is not significantly affected by the\\nchunk size, with performance variations less than 1%. The\\nthree curves are closely aligned, indicating that chunk sizes\\nin the range of {10, 20} exhibit better performance.\\n\\nTable 9 and 10 show the performance of ChunkKV with\\ndifferent comperession ratios and different chunk sizes on\\nthe LongBench and NIAH. We conducted extensive exper-\\niments across different compression ratios and KV cache\\nsizes to shows the effectiveness of ChunkKV and the chunk\\nsize is robust.\\n\\nTable 10: NIAH Performance with Different Chunk Sizes\\nand KV Cache Sizes for LLaMA-3-8B-Instruct\\n\\n| KV Cache | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Size | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 96 | 41.0 | 63.2 | 65.2 | 70.3 | 67.2 | 65.3 | 53.1 |\\n| 128 | 47.9 | 65.6 | 69.1 | 73.8 | 72.3 | 72.0 | 71.2 |\\n| 256 | 61.7 | 70.3 | 71.2 | 74.1 | 73.2 | 72.3 | 71.1 |\\n| 512 | 68.6 | 72.6 | 72.5 | 74.5 | 74.3 | 74.0 | 72.6 |\\n\\n\\nFrom the chunk size ablation study, we can observe that\\nacross different tasks (LongBench and NIAH) and vari-\\nous compression settings, a chunk size of 10 consistently\\ndelivers optimal or near-optimal performance. This em-\\npirical finding suggests that a chunk size of 10 strikes a\\ngood balance between preserving semantic information and\\ncompression efficiency, making it a robust default choice\\nfor ChunkKV. Therefore, we adopt this chunk size setting\\nthroughout our experiments.\\n\\n# 6. Conclusion\\n\\nWe introduced ChunkKV, a novel KV cache compression\\nmethod that preserves semantic information by retaining\\nmore informative chunks. Through extensive experiments\\nacross multiple state-of-the-art LLMs (including DeepSeek-\\nR1, LLaMA-3, Qwen2, and Mistral) and diverse bench-\\nmarks (GSM8K, LongBench, NIAH, and JailbreakV), we\\ndemonstrate that ChunkKV consistently outperforms ex-\\nisting methods while using only a fraction of the memory.\\nOur comprehensive analysis shows that ChunkKV’s chunk-\\nbased approach maintains crucial contextual information,\\nleading to superior performance in complex reasoning tasks,\\nlong-context understanding, and safety evaluations. The\\nmethod’s effectiveness is particularly evident in challenging\\nscenarios like many-shot GSM8K and multi-document QA\\ntasks, where semantic coherence is crucial. Furthermore,\\nour proposed layer-wise index reuse technique provides\\nsignificant computational efficiency gains with minimal per-\\nformance impact, achieving up to 20.7% latency reduction\\nand 26.5% throughput improvement. These findings, sup-\\nported by detailed quantitative analysis and ablation stud-\\nies, establish ChunkKV as a significant advancement in KV\\ncache compression technology, offering an effective solution\\nfor deploying LLMs in resource-constrained environments\\nwhile maintaining high-quality outputs.\\n\\n8ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n', 'hami, A. and Jaggi, M. Landmark attention:\\nRandom-access infinite context length for transformers.\\nArXiv preprint, abs/2305.16300, 2023. URL https:\\n//arxiv.org/abs/2305.16300.\\n\\nOpenAI. Gpt-4o-mini: Advancing cost-efficient intelli-\\ngence, 2023. Accessed: 2023-12-14.\\n\\nPan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C.,\\nand Zhang, T. Lisa: Layerwise importance sampling\\nfor memory-efficient large language model fine-tuning.\\nArXiv preprint, abs/2403.17919, 2024a. URL https:\\n//arxiv.org/abs/2403.17919.\\n\\nPan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K.,\\nZhang, J., Pi, R., and Zhang, T. Plum: Prompt learning us-\\ning metaheuristics. In Ku, L.-W., Martins, A., and Sriku-\\nmar, V. (eds.), Findings of the Association for Computa-\\ntional Linguistics ACL 2024, pp. 2177–2197, Bangkok,\\nThailand and virtual meeting, August 2024b. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2024.\\nfindings-acl.129. URL https://aclanthology.\\norg/2024.findings-acl.129.\\n\\nPires, B. Á. and Szepesvári, C. Multiclass classification\\ncalibration functions. arXiv preprint arXiv:1609.06385,\\n2016.\\n\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lill-\\nicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Fi-\\nrat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of\\ncontext. ArXiv preprint, abs/2403.05530, 2024. URL\\nhttps://arxiv.org/abs/2403.05530.\\n\\nShaham, U., Ivgi, M., Efrat, A., Berant, J., and\\nLevy, O. ZeroSCROLLS: A zero-shot benchmark for\\nlong text understanding. In Bouamor, H., Pino, J.,\\nand Bali, K. (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023, pp. 7977–\\n7989, Singapore, 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.\\n536. URL https://aclanthology.org/2023.\\nfindings-emnlp.536.\\n\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V.,\\nSmith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis,\\nM. In-context pretraining: Language modeling beyond\\ndocument boundaries. In The Twelfth International Con-\\nference on Learning Representations.\\n\\nSmith, B. and Troynikov, A. Evaluating chunking\\nstrategies for retrieval. Technical report, Chroma,\\n2024. URL https://research.trychroma.\\ncom/evaluating-chunking.\\n\\nSteinwart, I. How to compare different loss functions and\\ntheir risks. Constructive Approximation, 26:225–287,\\n2007. URL https://api.semanticscholar.\\norg/CorpusID:16660598.\\n\\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma,\\nS., Zhang, Q., Wang, J., and Wei, F. You only cache\\nonce: Decoder-decoder architectures for language models.\\narXiv preprint arXiv:2405.05254, 2024.\\n\\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\\nS. Quest: Query-aware sparsity for efficient long-context\\nllm inference. ArXiv preprint, abs/2406.10774, 2024.\\nURL https://arxiv.org/abs/2406.10774.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,\\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\\nD. Long range arena : A benchmark for efficient trans-\\nformers. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=qVyeW-grC2k.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\\nUnifying language learning paradigms. ArXiv preprint,\\nabs/2205.05131, 2022. URL https://arxiv.org/\\nabs/2205.05131.\\n\\n12ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundatio', ') and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nreasoning (O1 and R1) LLMs, achieving state-of-the-art\\nperformance.\\n\\n# 2. Related Work\\n\\nKV Cache Compression. KV cache compression technol-\\nogy has developed rapidly in the era of LLM, with meth-\\nods mainly focused on evicting unimportant tokens. The\\ncompression process occurs before the attention blocks, op-\\ntimizing both the prefilling time and GPU memory. Xiao\\net al. (2024) and Han et al. (2024) propose that initial and re-\\ncent tokens consistently have high attention scores between\\ndifferent layers and attention heads. As a result, retaining\\nthese tokens in the KV cache is more likely to preserve im-\\nportant information. Furthermore, FastGen (Ge et al., 2023)\\nevicts tokens based on observed patterns. H2O (Zhang et al.,\\n2023) and SnapKV (Li et al., 2024) employ dynamic KV\\ncache compression methods, evaluating the importance of\\ntokens based on attention scores and then evicting the less\\nimportant ones. As inference scenarios become increas-\\ningly complex, dynamic KV cache compression methods\\ndemonstrate powerful performance. Recently, Yang et al.\\n(2024b) and Cai et al. (2024) have closely examined the dis-\\ntributions of attention scores during the pre-filling stage of\\nthe Retrieval-Augmented Generation (RAG) task, discover-\\ning a pyramidal KV cache compression pattern in different\\ntransformer layers.\\n\\nAlthough these KV cache compression methods have ex-\\nplored efficient GPU memory management while maintain-\\ning original performance, our study focuses more on the\\nsemantic information of the prompt. We find that chunks\\nof the original KV cache are more important than discrete\\ntokens.\\n\\nChunking Method. The chunking methodology is widely\\nused in the field of NLP due to its simplicity and effective-\\nness (Tjong Kim Sang & Veenstra, 1999). In the era of\\nLLMs, chunking is primarily applied in data pre-processing.\\nFor example, Shi et al. suggest grouping related train-\\ning data into chunks to achieve better convergence curves\\nto pre-train LLMs. Fei et al. (2024) apply a topic-based\\nchunking method to improve the semantic compression of\\nprompts. Furthermore, chunking plays an important role in\\nthe Retrieval-Augmented Generation (RAG) field (Yepes\\net al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It\\nserves to divide documents into units of information with\\nsemantic content suitable for embedding-based retrieval and\\nprocessing by LLMs.\\n\\nLayer-Wise Technique The layer-wise technique is widely\\nused in the training and inference of large language models\\n(LLMs). LISA (Pan et al., 2024a) is a layer-wise sampling\\nmethod based on observations of the training dynamics of\\nLow-Rank Adaptation (LoRA)(Hu et al., 2022) across lay-\\n\\ners. LAMB(You et al., 2020)', 'in-\\nformation usually appear in a continuous sequence (Fang &\\nXie, 2022). Thus, we introduce a straightforward yet effec-\\ntive ChunkKV, grouping the tokens in a chunk as a basic\\ncompressing unit, which should be preserved or discarded\\nas a whole. Thus, it retains the most informative semantic\\nchunks from the original KV cache. As shown in Figure 1,\\npreserving a chunk helps to catch the subject, predicate,\\nand object. Furthermore, we investigate that the preserved\\n\\n1ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nQuestion: purple-crested turaco eats what food?\\n\\n| Discrete KV methods: 𝑆\" = 𝑓(𝑡\") | ChunkKV: 𝑆! = \\t ∑ #\" $% 𝑓 𝑡\" , \\twhere\\t𝑐\\t = \\t {𝑡%, … , 𝑡#} |\\n| --- | --- |\\n| Discrete KV methods with a low sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… Bird …… …… eat …… …… …… turacos, …… …… …… …… pulp …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… water …… …… …… …… …… …… …… leaves …… …… …… …… …… …… …… …… nuts …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a low sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… |\\n| Discrete KV methods with a high sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… …… …… eat …… …… …… turacos, …… …… …… …… …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a high sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… ………… …… …… …… |\\n\\n\\nFigure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The\\ndiscrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the\\nsubject of the words, maintaining more accurate semantic information. For the equation: S is the score function, and c is a\\nchunk of tokens.\\n\\nTable 1: Comparison of Methods on KV Cache Compression.\\n\\n| Method | KV Cache Compression | Dynamic Policy | Layer-Wise Policy | Semantic Information | Efficient Index Reuse |\\n| --- | --- | --- | --- | --- | --- |\\n| StreamingLLM (Xiao et al., 2024) | ✓ |  |  |  |  |\\n| H2O (Zhang et al., 2023) | ✓ | ✓ |  |  |  |\\n| SnapKV (Li et al., 2024) | ✓ | ✓ |  |  |  |\\n| PyramidInfer (Yang et al., 2024b) | ✓ | ✓ | ✓ |  |  |\\n| PyramidKV (Cai et al., 2024) | ✓ | ✓ | ✓ |  |  |\\n| ChunkKV(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |\\n\\n\\nKV cache indices by ChunkKV exhibit a higher similarity\\ncompared to previous methods. Consequently, we develop a\\ntechnique called layer-wise index reuse, which reduces the\\nadditional computational time introduced by the KV cache\\ncompression method. As outlined in Table 1, recent highly\\nrelevant KV cache compression methods lack the ability to\\nretain semantic information and efficiently reuse indices.\\n\\nTo evaluate ChunkKV’s performance, we conduct com-\\nprehensive experiments across multiple cutting-edge long-\\ncontext benchmarks: long-context tasks including Long-\\nBench (Bai et al., 2024) and Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023), in-context learning tasks such as\\nGSM8K (Cobbe et al., 2021', ', Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,\\nHan, X., Thai, Z. L., Wang, S., Liu, Z., et al. ∞-bench:\\nExtending long context evaluation beyond 100k tokens.\\nArXiv preprint, abs/2402.13718, 2024. URL https:\\n//arxiv.org/abs/2402.13718.\\n\\n13ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\\nR., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H2o:\\nHeavy-hitter oracle for efficient generative inference of\\nlarge language models. Advances in Neural Information\\nProcessing Systems, 36:34661–34710, 2023.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\\nvances in Neural Information Processing Systems, 36:\\n46595–46623, 2023.\\n\\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\\nAwadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and\\nRadev, D. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. In Toutanova, K.,\\nRumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Belt-\\nagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and\\nZhou, Y. (eds.), Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 5905–5921, Online, 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n472. URL https://aclanthology.org/2021.\\nnaacl-main.472.\\n\\nZhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y.,\\nCotterell, R., and Sachan, M. Recurrentgpt: Interactive\\ngeneration of (arbitrarily) long text, 2023.\\n\\nZhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., and Han, B.\\nCan language models perform robust reasoning in chain-\\nof-thought prompting with noisy rationales? In The\\nThirty-eighth Annual Conference on Neural Information\\nProcessing Systems, 2024.\\n\\n14ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nAppendix\\nA In-depth Analysis of ChunkKV vs. Discrete Token Methods 16\\nA.1 Quantitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.2 Hypothetical Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 Comparative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.4 Implications for Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB Additional Experiments 18\\nB.1 Layer-Wise Index Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB.2 LongBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nB.3 Needle-In-A-Haystack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nB.4 Chunk Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nB.5 Multi-Lingual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nC Theoretical Understanding 30\\nD Additional Related Work 33\\nE Statistics of Models 33\\nF Statistics of Datasets 34\\nG Prompt 34\\nH Limitations 35\\nI Licenses 35\\n\\n15ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# A. In-depth Analysis of ChunkKV vs. Discrete Token Methods\\n\\nA.1. Quantitative Analysis\\n\\nTo rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic\\nexperiments using a LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of\\nLongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine\\nsimilarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground\\ntruth. 2. Applied ChunkKV,', \" is a layer-wise adaptive learn-\\ning rate method that speeds up LLM training by stabilizing\\ntraining convergence with large batch sizes. DoLa (Chuang\\net al., 2023) employs layer-wise contrasting to reduce hallu-\\ncinations during LLM inference.\\n\\n# 3. ChunkKV\\n\\n3.1. Preliminary Study of KV Cache Compression\\n\\nWith the increasing long-context capabilities of LLMs, the\\nKV cache has become crucial for improving inference effi-\\nciency. However, it can consume significant GPU memory\\nwhen handling long input contexts. The GPU memory cost\\nof the KV cache for the decoding stage can be calculated as\\nfollows:\\n\\n$$M_{K V}=2\\\\times B\\\\times S\\\\times L\\\\times N\\\\times D\\\\times2\\\\qquad\\\\qquad(1)$$\\n\\nwhere B is the batch size, S is the sequence length of prompt\\nand decoded length, L is the number of layers, N is the num-\\nber of attention heads, D is the dimension of each attention\\nhead, and the first 2 accounts for the KV matrices, while\\nthe last 2 accounts for the precision when using float16.\\nTable E shows the configuration parameters for LLaMA-\\n3-8B-Instruct (Meta, 2024). With a batch size B = 1 and\\na sequence length of prompt S = 2048, the GPU mem-\\nory cost of the KV cache is nearly 1 GB. If the batch size\\nexceeds 24, the GPU memory cost of the KV cache will\\nexceed the capacity of an RTX 4090 GPU. To address this\\nissue, KV cache compression methods have been proposed,\\nwith the aim of retaining only a minimal amount of KV\\ncache while preserving as much information as possible.\\nFor more details on the LLM configuration parameters, re-\\nfer to Appendix E.\\n\\n3.2. Chunk Based KV Compression\\n\\nTo address the limitations of existing KV cache compression\\nmethods, we propose ChunkKV, a novel KV cache com-\\npression method that retains the most informative semantic\\nchunks. The key idea behind ChunkKV is to group tokens\\nin the KV cache into chunks that preserve more semantic\\ninformation, such as a chunk containing a subject, verb and\\nobject. As illustrated in Figure 1, ChunkKV preserves the\\nchunks of the KV cache that contain more semantic infor-\\nmation. First, we define a chunk as a group of tokens that\\ncontain related semantic information. By retaining the most\\ninformative chunks from the original KV cache, ChunkKV\\ncan effectively reduce the memory usage of the KV cache\\nwhile preserving essential information.\\n\\nThe Algorithm 1 shows the pseudocode outline of ChunkKV.\\nFirst, following H2O (Zhang et al., 2023) and SnapKV (Li\\net al., 2024), we set the observe window by computing the\\nobservation scores A ← QTq−w:Tq KT , where QTq−w:Tq\\n\\n3ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n| Algorithm 1 ChunkKV |\\n| --- |\\n| Input: Q E RTqxd K E RTkxd v E RTvxd observe , , window size w, chunk size c, compressed KV cache max length Lmax |\\n| Output: Compressed KV cache K', V' |\\n| Observe Window Calculation: |\\n| A ← QTq-w:Tq KT {Attention scores for the observe window} |\\n| C ← [ ] I {Calculate the number of chunks} |\\n| Chunk Attention Score Calculation: |\\n| for 2 = 1 to C do |\\n| Ai ← �j=(i-1)c+1 A:⌀ {Sum of observation scores for each chunk} |\\n| end for |\\n| Top-K Chunk Selection: |\\n| k ← Lmax c |\\n| Top K_ Indices ← indices of Top-k chunks based on Ai Compression: |\\n| K', V' ← index_select(K, V, Top_K_Indices) |\\n| Concatenation: |\\n| K' ← concat(Ko:Lmax -w' KTk-w:Tk) |\\n| v Tv-w:Tv ) V' ← concat(Vo:Lmax |\\n| -w' K', V' |\\n\\n\\nis the observe window, K is the Key matrix and the win-\\ndow size w is usually set to {4,8, 16, 32}. Next, the num-\\nber of chunks C is calculated as C = [ Te 1, where Tk is\\nthe length of the Key matrix and c is the chunk size. The\\nobservation scores for each chunk are then computed as\\nAi = �j=(i-1)c+1 A:⌀ for 2 = 1, 2, · · · , C. Referring to\\nprevious works (Zhang et al., 2023; Li et al., 2024; Yang\\net al., 2024b; Cai et al., 2024), we still use the top-k algo-\\nrithm as ChunkKV's sampling policy. For the top-k chunk\\nselection, the top-k chunks are selected based on their obser-\\nvation scores, where k = Lmax I, and Lmax is\", '# ChunkKV: Semantic-Preserving KV Cache Compression for\\nEfficient Long-Context LLM Inference\\n\\nXiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1\\n\\n# Abstract\\n\\nTo reduce memory costs in long-context inference\\nwith Large Language Models (LLMs), many re-\\ncent works focus on compressing the key-value\\n(KV) cache of different tokens. However, we\\nidentify that the previous KV cache compression\\nmethods measure token importance individually,\\nneglecting the dependency between different to-\\nkens in the real-world language characterics. In\\nlight of this, we introduce ChunkKV, grouping the\\ntokens in a chunk as a basic compressing unit, and\\nretaining the most informative semantic chunks\\nwhile discarding the less important ones. Further-\\nmore, observing that ChunkKV exhibits higher\\nsimilarity in the preserved indices across differ-\\nent layers, we propose layer-wise index reuse\\nto further reduce computational overhead. We\\nevaluated ChunkKV on cutting-edge long-context\\nbenchmarks including LongBench and Needle-\\nIn-A-HayStack, as well as the GSM8K and Jail-\\nbreakV in-context learning benchmark. Our ex-\\nperiments with instruction tuning and multi-step\\nreasoning (O1 and R1) LLMs, achieve up to 10%\\nperformance improvement under aggressive com-\\npression ratios compared to existing methods.\\n\\n# 1. Introduction\\n\\n2025\\nFeb\\n1\\n[cs.CL]\\narXiv:2502.00299v1\\n\\nLarge Language Models (LLMs) have become essential for\\naddressing various downstream tasks of natural language\\nprocessing (NLP), including summarization and question\\nanswering, which require the interpretation of a long con-\\ntext from sources such as books, reports, and documents,\\noften encompassing tens of thousands of tokens (Brown\\net al., 2020; Tay et al., 2022; Touvron et al., 2023). Re-\\ncent advances in long-context technology within the field of\\n\\n1The Hong Kong University of Science and Technol-\\nogy(Guangzhou), Guangzhou, China 2The Hong Kong University\\nof Science and Technology, Hong Kong, China. Correspondence\\nto: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu\\n<xwchu@hkust-gz.edu.cn>.\\n\\nmachine learning (ML) systems (Dao, 2024; Jacobs et al.,\\n2023; Xiao et al., 2024) have significantly enhanced com-\\nputational throughputs and reduced latency of LLMs to\\nprocess increasingly large input context lengths (Liu et al.,\\n2024b; Young et al., 2024) with saving historical KV cache\\n(key value attentions). However, the memory requirement\\nof the KV cache in serving super-long contexts becomes a\\nnew bottlneck (Zhang et al., 2023; Reid et al., 2024). For\\ninstance, the KV cache for a single token in a 7B-parameter\\nmodel requires approximately 0.5 MB of GPU memory, re-\\nsulting in a 10,000-token prompt consuming around 5 GB\\nof GPU memory.\\n\\nTo address the substantial GPU memory consumption\\ncaused by KV caching, recent studies consider compressing\\nthe KV cache by pruning non-important discrete parts from\\nthe prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge\\net al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al.,\\n2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang\\net al., 2023) and SnapKV (Li et al., 2024) have shown that\\nretaining less than 50% of the discrete KV cache can signif-\\nicantly reduce GPU memory usage with minimal impact on\\nperformance. However, we identify that the previous KV\\ncache compression methods (Zhang et al., 2023; Cai et al.,\\n2024) measure token importance isolatedly, neglecting the\\ndependency between different tokens in the real-world lan-\\nguage characterics. For example, as shown in Figure 1,\\nfocusing on token-level importance might excessively fo-\\ncus on words about subjects “turaco” in the question while\\nomitting crucial information about the objects (foods) in the\\ndocuments, resulting the loss of essential semantic informa-\\ntion. This motivates us to rethink the following question:\\n\\nHow to avoid isolated token importance measurement and\\npreserve the semantic information in KV cache?\\n\\nIn light of this, we observe that the complete semantic ', 'essing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\\nM., Petroni, F., and Liang, P. Lost in the middle: How\\nlanguage models use long contexts. Transactions of the\\nAssociation for Computational Linguistics, 12:157–173,\\n2024c. doi: 10.1162/tacl_a_00638. URL https://\\naclanthology.org/2024.tacl-1.9.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\\nrepository-level code auto-completion systems. In The\\nTwelfth International Conference on Learning Represen-\\ntations, 2024d. URL https://openreview.net/\\nforum?id=pPjZIOuQuF.\\n\\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyril-\\nlidis, A., and Shrivastava, A. Scissorhands: Exploiting\\nthe persistence of importance hypothesis for llm kv cache\\ncompression at test time. Advances in Neural Information\\nProcessing Systems, 36, 2024e.\\n\\nLuo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv:\\nA benchmark for assessing the robustness of multimodal\\nlarge language models against jailbreak attacks. In First\\nConference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=GC4mXVfquq.\\n\\nMeta. Introducing meta llama 3: The most capable openly\\navailable llm to date. https://ai.meta.com/\\nblog/meta-llama-3/, 2024. Accessed: 2024-06-\\n07.\\n\\nMohtas', 'king, the continuously chunk-\\nlevel KV cache preserves the whole examples (semantic\\ninformation) in ICL, thus reducing the requirement on dis-\\ntinguishability, i.e lower bound of KL divergence between\\nthe example and the question (Equation 4 in Condition 2).\\nThe complete analysis is provided in Appendix C.\\n\\n# 4. Experiment Results\\n\\nIn this section, we conduct experiments to evaluate the ef-\\nfectiveness of ChunkKV on KV cache compression in two\\nbenchmark fields, with a chunk size set to 10 even for vari-\\nous model architectures. The first is the In-Context Learn-\\ning benchmark, for which we select GSM8K (Cobbe et al.,\\n2021) and Jailbreakv (Luo et al., 2024) to evaluate the perfor-\\nmance of ChunkKV, furthermore we also include multi-step\\nreasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al.,\\n2025) to evaluate the performance of ChunkKV. The In-\\nContext Learning scenario is a crucial capability for LLMs\\nand has been adapted in many powerful technologies such as\\nChain-of-Thought (Wei et al., 2022; Diao et al., 2024; Pan\\net al., 2024b). The second is the Long-Context benchmark,\\nwhich includes LongBench (Bai et al., 2024) and Needle-In-\\nA-HayStack (NIAH) (Kamradt, 2023), both widely used for\\nassessing KV cache compression methods. All experiments\\nwere conducted three times, using the mean score to ensure\\nrobustness.\\n\\n4.1. In-Context Learning\\n\\nThe In-Context Learning (ICL) ability significantly en-\\nhances the impact of prompts on large language models\\n(LLMs). For example, the Chain-of-Thought approach (Wei\\n\\net al., 2022) increases the accuracy of the GSM8K of the\\nPaLM model (Chowdhery et al., 2022) from 18% to 57%\\nwithout additional training. In this section, we evaluate\\nthe performance of ChunkKV on the GSM8K, Many-Shot\\nGSM8K (Agarwal et al., 2024), and JailbreakV (Luo et al.,\\n2024) benchmarks.\\n\\nTable 3: GSM8K Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ |\\n| 10% | 51.6% | 55.6% | 57.6% | 62.6% | 65.7% |\\n| LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ |\\n| 30% | 70.5% | 72.2% | 76.1% | 77.1% | 77.3% |\\n| 20% | 63.8% | 64.0% | 68.8% | 71.4% | 77.6% |\\n| 10% | 47.8% | 45.0% | 50.3% | 48.2% | 65.7% |\\n| LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ |\\n| 30% | 70.6% | 73.6% | 70.2% | 68.2% | 74.6% |\\n| Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ |\\n| 30% | 70.8% | 61.2% | 70.8% | 64.7% | 73.5% |\\n\\n\\nGSM8K In the in-context learning scenario, we\\nevaluated multiple KV cache compression methods\\nfor GSM8K (Cobbe et al., 2021), which contains\\nmore than 1,000 arithmetic questions on LLaMA-3-8B-\\nInstruct, LLaMA-3.1-8B-Instruct (Meta, 2024), Qwen2-\\n7B-Instruct (Yang et al., 2024a) and DeepSeek-R1-Distill-\\nLlama-8B (Guo et al., 2025). Follow the Agarwal et al.\\n(2024), we consider many-shot GSM8K as a long-context\\nreasoning scenario, which is a more challenging task than\\nLongBench (Bai et al., 2024). The CoT prompt settings\\nfor this experiment are the same as those used by Wei et al.\\n(2022), for many-shot GSM8K we set the number of shots\\nto 50, which the prompt length is more than 4k tokens.\\n\\n5ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-', 'suring Efficiency. We evaluated the latency and\\nthroughput of ChunkKV compared to FullKV using\\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\\nwere conducted with reuse layer is 2, batch size set to 1\\nand inference was performed using Flash Attention 2, each\\nexperiment was repeated 10 times and the average latency\\nand throughput were reported.\\n\\nTable 8: Latency and throughput comparison between\\nChunkKV and FullKV under different input-output configu-\\nrations. Percentages in parentheses indicate improvements\\nover FullKV baseline.\\n\\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\\n| --- | --- | --- | --- | --- |\\n| Method | Input | Output | Latency(s) ↓ | Throughput(T/S) ↑ |\\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\\n\\n\\nThe results in Table 8 shows that the layer-wise index reuse\\nstrategy (ChunkKV_reuse) further boosts performance,\\nachieving up to a 20.7% reduction in latency, and through-\\nput improvements are particularly notable for longer input\\nsequences, with ChunkKV_reuse delivering up to a 26.5%\\nimprovement over FullKV.\\n\\nIndex Reuse Performance on LongBench\\n\\n![image](/image/placeholder)\\n- Chart Title: Index Reuse Performance on LongBench\\n- X-Axis: Number of Index Reuse Layers\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\\n\\n\\nFigure 4: Comparison with different index reuse layers on\\nLongBench.\\n\\nMeasuring Task Performance. This experiment evaluates\\nthe performance of the layer-wise index reuse approach by\\nmeasuring the performance of the LongBench (Bai et al.,\\n2024), the experiment settings are the same as LongBench\\nin 4.2. And the number of index reuse layers is set from 1\\nto the number of layers in the model, where an index reuse\\nlayer of 1 corresponds to the normal ChunkKV without\\nindex reuse, and our method set reuse layer to 2.\\n\\nFigure 4 illustrates the performance of ChunkKV with vary-\\ning index reuse layers on the LongBench benchmark. Gen-\\nerally, reuse layer set to 2 can achieve the minimal perfor-\\nmance degradation across all models. For more experiments\\n\\n7ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LongBench Performance vs Chunk Size\\n- X-Axis: Chunk Size\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | ILaMA-3-BB | Mistral-7B | Owen2-7B | LLaMA-3-BB Full kV | Mistral-7B Full kV | Qwen2-7B Full KV |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 38 | 46 | 46 | 46 | 46 | 40 |\\n\\n\\nFigure 5: LongBench Performance Comparison with differ-\\nent chunk size under 10% compression rate.\\n\\non index reuse, please refer to the APPENDIX B.1.3.\\n\\nOverall, these findings on efficiency and performance sug-\\ngest that layer-wise index reuse can be an effective technique\\nfor optimizing the efficiency-performance trade-off in KV\\ncache compression, with the potential for model-specific\\ntuning to maximize benefits.\\n\\n# 5. Ablation study\\n\\n# 5.1. Chunk Size\\n\\nThis section aims to investigate the impact of chunk size on\\nthe performance of ChunkKV. Different chunk sizes will\\nlead to varying degrees of compression on the semantic\\ninformation of the data. We set the experiemnt setting the']\n",
            "Based on the retrieved content, answer the user question.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            if value[\"messages\"][-1].content:\n",
        "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "                print()\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxNbdmWOAuEX",
        "outputId": "1b879582-17a4-49e5-c44f-3106ee28f5d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv. \n",
            "Assistant: URL to download 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference': https://arxiv.org/pdf/2502.00299\n",
            "\n",
            "Found cached markdown for 2502.00299\n",
            "Assistant: we already have the paper content stored in our database in the id of 2502.00299\n",
            "\n",
            "Assistant: Below is the retrieved content of the Paper.\n",
            "-----------------------------------\n",
            "\n",
            "0: ['varian, M., Chen, M., Jun, H.,\\nKaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano,\\nR., et al. Training verifiers to solve math word problems.\\nArXiv preprint, abs/2110.14168, 2021. URL https:\\n//arxiv.org/abs/2110.14168.\\n\\nDao, T. FlashAttention-2: Faster attention with better paral-\\nlelism and work partitioning. In International Conference\\non Learning Representations (ICLR), 2024.\\n\\nDasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N. A., and\\nGardner, M. A dataset of information-seeking questions\\nand answers anchored in research papers. In Toutanova,\\nK., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D.,\\nBeltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and\\nZhou, Y. (eds.), Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 4599–4610, Online, 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n\\n9ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n365. URL https://aclanthology.org/2021.\\nnaacl-main.365.\\n\\nDiao, S., Wang, P., Lin, Y., Pan, R., Liu, X., and Zhang,\\nT. Active prompting with chain-of-thought for large lan-\\nguage models. In Ku, L.-W., Martins, A., and Srikumar,\\nV . (eds.), Proceedings of the 62nd Annual Meeting of\\nthe Association for Computational Linguistics (Volume\\n1: Long Papers), pp. 1330–1350, Bangkok, Thailand,\\nAugust 2024. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2024.acl-long.73. URL https:\\n//aclanthology.org/2024.acl-long.73.\\n\\nFabbri, A., Li, I., She, T., Li, S., and Radev, D. Multi-news:\\nA large-scale multi-document summarization dataset and\\nabstractive hierarchical model. In Korhonen, A., Traum,\\nD., and Màrquez, L. (eds.), Proceedings of the 57th An-\\nnual Meeting of the Association for Computational Lin-\\nguistics, pp. 1074–1084, Florence, Italy, 2019. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/\\nP19-1102. URL https://aclanthology.org/\\nP19-1102.\\n\\nFang, H. and Xie, P. An end-to-end contrastive self-\\nsupervised learning framework for language understand-\\ning. Transactions of the Association for Computational\\nLinguistics, 10:1324–1340, 2022. doi: 10.1162/tacl_\\na_00521. URL https://aclanthology.org/\\n2022.tacl-1.76/.\\n\\nFei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng,\\nL., and Han, W. Extending context window of large\\nlanguage models via semantic compression. In Ku,\\nL.-W., Martins, A., and Srikumar, V. (eds.), Find-\\nings of the Association for Computational Linguistics\\nACL 2024, pp. 5169–5181, Bangkok, Thailand and vir-\\ntual meeting, August 2024. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2024.findings-acl.\\n306. URL https://aclanthology.org/2024.\\nfindings-acl.306.\\n\\nFu, Q., Cho, M., Merth, T., Mehta, S., Rastegari, M., and\\nNajibi, M. LazyLLM: Dynamic token pruning for ef-\\nficient long context LLM inference. In Workshop on\\nEfficient Systems for Foundation Models II @ ICML2024,\\n2024a. URL https://openreview.net/forum?\\nid=gGZD1dsJqZ.\\n\\nFu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the se-\\nquential dependency of llm inference using lookahead\\ndecoding. arXiv preprint arXiv:2402.02057, 2024b.\\n\\nGe, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J.\\nModel tells you what to discard: Adaptive kv cache com-\\npression for llms. ArXiv preprint, abs/2310.01801, 2023.\\nURL https://arxiv.org/abs/2310.01801.\\n\\nGliwa, B., Mochol, I., Biesek, M., and Wawer, A. SAM-\\nSum corpus: A human-annotated dialogue dataset for\\nabstractive summarization. In Wang, L., Cheung, J. C. K.,\\nCarenini, G., and Liu, F. (eds.), Proceedings of the 2nd\\nWorkshop on New Frontiers in Summarization, pp. 70–\\n79, Hong Kong, China, 2019. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/D19-5409. URL\\nhttps://aclanthology.org/D19-5409.\\n\\nGuo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. J.\\nLongcoder: A long-range pre-trained language model for\\ncode completion. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, ', '# Impact Statement\\n\\nOur study does not involve human subjects, data collection\\nfrom individuals, or experiments on protected groups. The\\nmodels and datasets used in this work are publicly avail-\\nable and widely used in the research community. We have\\nmade efforts to ensure our experimental design and report-\\ning of results are fair, unbiased, and do not misrepresent the\\ncapabilities or limitations of the methods presented.\\n\\nIn our work on KV cache compression for large language\\nmodels, we acknowledge the potential broader impacts of\\nimproving efficiency in AI systems. While our method aims\\nto reduce computational resources and potentially increase\\naccessibility of these models, we recognize that more effi-\\ncient language models could also lead to increased deploy-\\nment and usage, which may have both positive and negative\\nsocietal implications. We encourage further research and\\ndiscussion on the responsible development and application\\nof such technologies.\\n\\nWe declare no conflicts of interest that could inappropriately\\ninfluence our work. All experiments were conducted using\\npublicly available resources, and our code will be made\\navailable to ensure reproducibility. We have made every\\neffort to cite relevant prior work appropriately and to accu-\\nrately represent our contributions in the context of existing\\nresearch.\\n\\n# References\\n\\nAgarwal, R., Singh, A., Zhang, L. M., Bohnet, B., Rosias,\\nL., Chan, S., Zhang, B., Anand, A., Abbas, Z., Nova,\\nA., et al. Many-shot in-context learning. arXiv preprint\\narXiv:2404.11018, 2024.\\n\\nAn, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong,\\nL., and Qiu, X. L-eval: Instituting standardized evalua-\\ntion for long context language models. ArXiv preprint,\\nabs/2307.11088, 2023. URL https://arxiv.org/\\nabs/2307.11088.\\n\\nAnthropic. Introducing contextual retrieval, 2024.\\nURL https://www.anthropic.com/news/\\ncontextual-retrieval.\\n\\nBai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du,\\nZ., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li,\\nJ. LongBench: A bilingual, multitask benchmark for long\\ncontext understanding. In Ku, L.-W., Martins, A., and\\nSrikumar, V. (eds.), Proceedings of the 62nd Annual Meet-\\ning of the Association for Computational Linguistics (Vol-\\nume 1: Long Papers), pp. 3119–3137, Bangkok, Thailand,\\nAugust 2024. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/2024.acl-long.172. URL https:\\n//aclanthology.org/2024.acl-long.172.\\n\\nBrandon, W., Mishra, M., Nrusimha, A., Panda, R.,\\nand Kelly, J. R. Reducing transformer key-value\\ncache size with cross-layer attention. arXiv preprint\\narXiv:2405.12981, 2024.\\n\\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\\nAskell, A., et al. Language models are few-shot learners.\\nAdvances in neural information processing systems, 33:\\n1877–1901, 2020.\\n\\nCai, Z., Zhang, Y., Gao, B., Liu, Y., Liu, T., Lu, K., Xiong,\\nW., Dong, Y., Chang, B., Hu, J., et al. Pyramidkv: Dy-\\nnamic kv cache compression based on pyramidal informa-\\ntion funneling. arXiv preprint arXiv:2406.02069, 2024.\\n\\nChevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting\\nlanguage models to compress contexts. In Bouamor,\\nH., Pino, J., and Bali, K. (eds.), Proceedings of the\\n2023 Conference on Empirical Methods in Natural Lan-\\nguage Processing, pp. 3829–3846, Singapore, December\\n2023. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.232. URL https://\\naclanthology.org/2023.emnlp-main.232.\\n\\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\\nGehrmann, S., et al. Palm: Scaling language modeling\\nwith pathways. ArXiv preprint, abs/2204.02311, 2022.\\nURL https://arxiv.org/abs/2204.02311.\\n\\nChuang, Y.-S., Xie, Y., Luo, H., Kim, Y., Glass, J., and\\nHe, P. Dola: Decoding by contrasting layers improves\\nfactuality in large language models. ArXiv preprint,\\nabs/2309.03883, 2023. URL https://arxiv.org/\\nabs/2309.03883.\\n\\nCobbe, K., Kosaraju, V., Ba', 'n and fine-\\ntuned chat models. ArXiv preprint, abs/2307.09288, 2023.\\nURL https://arxiv.org/abs/2307.09288.\\n\\nTrivedi, H., Balasubramanian, N., Khot, T., and Sabharwal,\\nA. MuSiQue: Multihop questions via single-hop ques-\\ntion composition. Transactions of the Association for\\nComputational Linguistics, 10:539–554, 2022. doi: 10.\\n1162/tacl_a_00475. URL https://aclanthology.\\norg/2022.tacl-1.31.\\n\\nWang, Q., Ding, L., Cao, Y., Tian, Z., Wang, S., Tao, D., and\\nGuo, L. Recursively summarizing enables long-term dia-\\nlogue memory in large language models. arXiv preprint\\narXiv:2308.15022, 2023.\\n\\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,\\nE., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting\\nelicits reasoning in large language models. Advances in\\nneural information processing systems, 35:24824–24837,\\n2022.\\n\\nWingate, D., Shoeybi, M., and Sorensen, T. Prompt com-\\npression and contrastive conditioning for controllability\\nand toxicity reduction in language models. In Findings of\\nthe Association for Computational Linguistics: EMNLP\\n2022, pp. 5621–5634, Abu Dhabi, United Arab Emi-\\nrates, December 2022. Association for Computational\\nLinguistics. doi: 10.18653/v1/2022.findings-emnlp.\\n412. URL https://aclanthology.org/2022.\\nfindings-emnlp.412.\\n\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024. URL https:\\n//arxiv.org/abs/2405.10637.\\n\\nWu, H., Zhan, M., Tan, H., Hou, Z., Liang, D., and\\nSong, L. VCSUM: A versatile Chinese meeting sum-\\nmarization dataset. In Rogers, A., Boyd-Graber, J.,\\nand Okazaki, N. (eds.), Findings of the Association\\nfor Computational Linguistics: ACL 2023, pp. 6065–\\n6079, Toronto, Canada, 2023. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2023.findings-acl.\\n377. URL https://aclanthology.org/2023.\\nfindings-acl.377.\\n\\nXiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Ef-\\nficient streaming language models with attention sinks.\\nIn The Twelfth International Conference on Learning\\nRepresentations, 2024. URL https://openreview.\\nnet/forum?id=NG7sS51zVF.\\n\\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C.,\\nLi, C., Li, C., Liu, D., Huang, F., et al. Qwen2 technical\\nreport. ArXiv preprint, abs/2407.10671, 2024a. URL\\nhttps://arxiv.org/abs/2407.10671.\\n\\nYang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and\\nZhao, H. PyramidInfer: Pyramid KV cache com-\\npression for high-throughput LLM inference. In Ku,\\nL.-W., Martins, A., and Srikumar, V. (eds.), Find-\\nings of the Association for Computational Linguis-\\ntics ACL 2024, pp. 3258–3270, Bangkok, Thailand\\nand virtual meeting, 2024b. Association for Computa-\\ntional Linguistics. doi: 10.18653/v1/2024.findings-acl.\\n195. URL https://aclanthology.org/2024.\\nfindings-acl.195.\\n\\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhut-\\ndinov, R., and Manning, C. D. HotpotQA: A dataset\\nfor diverse, explainable multi-hop question answering.\\nIn Riloff, E., Chiang, D., Hockenmaier, J., and Tsujii,\\nJ. (eds.), Proceedings of the 2018 Conference on Em-\\npirical Methods in Natural Language Processing, pp.\\n2369–2380, Brussels, Belgium, 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/D18-1259.\\nURL https://aclanthology.org/D18-1259.\\n\\nYepes, A. J., You, Y., Milczek, J., Laverde, S., and Li, R. Fi-\\nnancial report chunking for effective retrieval augmented\\ngeneration, 2024. URL https://arxiv.org/abs/\\n2402.05131.\\n\\nYou, Y., Li, J., Reddi, S. J., Hseu, J., Kumar, S., Bho-\\njanapalli, S., Song, X., Demmel, J., Keutzer, K., and\\nHsieh, C. Large batch optimization for deep learning:\\nTraining BERT in 76 minutes. In 8th International Con-\\nference on Learning Representations, ICLR 2020, Addis\\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net,\\n2020. URL https://openreview.net/forum?\\nid=Syx4wnEtvH.\\n\\nYoung, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang,\\nG., Li, H., Zhu, J., Chen, J., Chang, J., et al. Yi:\\nOpen foundation models by 01. ai. ArXiv preprint,\\nabs/2403.04652, 2024. URL https://arxiv.org/\\nabs/2403.04652.\\n\\nZhang, X.', 'B., Sabato, S., and Scarlett, J. (eds.), Inter-\\nnational Conference on Machine Learning, ICML 2023,\\n23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of\\nProceedings of Machine Learning Research, pp. 12098–\\n12107. PMLR, 2023. URL https://proceedings.\\nmlr.press/v202/guo23j.html.\\n\\nGuo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R.,\\nZhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: In-\\ncentivizing reasoning capability in llms via reinforcement\\nlearning. arXiv preprint arXiv:2501.12948, 2025.\\n\\nHan, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H.,\\nand Wang, S. LM-infinite: Zero-shot extreme length gen-\\neralization for large language models. In Duh, K., Gomez,\\nH., and Bethard, S. (eds.), Proceedings of the 2024 Con-\\nference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Tech-\\nnologies (Volume 1: Long Papers), pp. 3991–4008, Mex-\\nico City, Mexico, 2024. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\n2024.naacl-long.222.\\n\\nHe, W., Liu, K., Liu, J., Lyu, Y., Zhao, S., Xiao, X., Liu, Y.,\\nWang, Y., Wu, H., She, Q., Liu, X., Wu, T., and Wang, H.\\nDuReader: a Chinese machine reading comprehension\\ndataset from real-world applications. In Choi, E., Seo, M.,\\nChen, D., Jia, R., and Berant, J. (eds.), Proceedings of the\\nWorkshop on Machine Reading for Question Answering,\\npp. 37–46, Melbourne, Australia, 2018. Association for\\nComputational Linguistics. doi: 10.18653/v1/W18-2605.\\nURL https://aclanthology.org/W18-2605.\\n\\nHo, X., Duong Nguyen, A.-K., Sugawara, S., and\\nAizawa, A. Constructing a multi-hop QA dataset\\nfor comprehensive evaluation of reasoning steps. In\\nScott, D., Bel, N., and Zong, C. (eds.), Proceed-\\nings of the 28th International Conference on Compu-\\ntational Linguistics, pp. 6609–6625, Barcelona, Spain\\n(Online), 2020. International Committee on Computa-\\ntional Linguistics. doi: 10.18653/v1/2020.coling-main.\\n580. URL https://aclanthology.org/2020.\\ncoling-main.580.\\n\\n10ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nHsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D.,\\nJia, F., Zhang, Y., and Ginsburg, B. Ruler: What’s the\\nreal context size of your long-context language models?\\nArXiv preprint, abs/2404.06654, 2024. URL https:\\n//arxiv.org/abs/2404.06654.\\n\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adapta-\\ntion of large language models. In The Tenth Interna-\\ntional Conference on Learning Representations, ICLR\\n2022, Virtual Event, April 25-29, 2022. OpenReview.net,\\n2022. URL https://openreview.net/forum?\\nid=nZeVKeeFYf9.\\n\\nHuang, L., Cao, S., Parulian, N., Ji, H., and Wang,\\nL. Efficient attentions for long document summariza-\\ntion. In Toutanova, K., Rumshisky, A., Zettlemoyer,\\nL., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell,\\nR., Chakraborty, T., and Zhou, Y. (eds.), Proceedings\\nof the 2021 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Hu-\\nman Language Technologies, pp. 1419–1436, Online,\\n2021. Association for Computational Linguistics. doi:\\n10.18653/v1/2021.naacl-main.112. URL https://\\naclanthology.org/2021.naacl-main.112.\\n\\nJacobs, S. A. et al. DeepSpeed Ulysses: System optimiza-\\ntions for enabling training of extreme long sequence\\nTransformer models. ArXiv preprint, abs/2309.14509,\\n2023. URL https://arxiv.org/abs/2309.\\n14509.\\n\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\\nT., and Sayed, W. E. Mistral 7b, 2023a. URL https:\\n//arxiv.org/abs/2310.06825.\\n\\nJiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.\\nLLMLingua: Compressing prompts for accelerated in-\\nference of large language models. In Bouamor, H.,\\nPino, J., and Bali, K. (eds.), Proceedings of the 2023\\nConference on Empirical Methods in Natural Language\\nProc', 'd comprehensive understanding benchmarks reflects the field’s commitment to\\nrigorously assessing the capabilities of long-context models across diverse linguistic challenges. Prompting Compression\\nIn the field of prompt compression, various designs effectively combine semantic information to compress natural language.\\nWingate et al. (2022) utilize soft prompts to encode more information with fewer tokens. Chevalier et al. (2023) present\\nAutoCompressor, which uses soft prompts to compress the input sequence and extend the original length of the base\\nmodel. Both Zhou et al. (2023) and Wang et al. (2023) recurrently apply LLMs to summarize input texts, maintaining long\\nshort-term memory for specific purposes such as story writing and dialogue generation. The LLMLingua series (Jiang et al.,\\n2023b; 2024; Fei et al., 2024) explores the potential of compressing LLM prompts in long-context, reasoning, and RAG\\nscenarios. Fei et al. (2024) use pre-trained language models to chunk the long context and summarize semantic information,\\ncompressing the original context.\\n\\n# E. Statistics of Models\\n\\nTable 18 provides configuration parameters for LLMs that we evaluated in our experiments.\\n\\n| Model Name | LLaMA-3-8B-Instruct | Mistral-7B-Instruct-v0.2 & 0.3 | Qwen2-7B-Instruct |\\n| --- | --- | --- | --- |\\n| L (Number of layers) | 32 | 32 | 28 |\\n| N (Number of attention heads) | 32 | 32 | 28 |\\n| D (Dimension of each head) | 128 | 128 | 128 |\\n\\n\\nTable 18: Models Configuration Parameters\\n\\n33ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# F. Statistics of Datasets\\n\\nLongBench is a meticulously designed benchmark suite that evaluates the capabilities of language models in handling\\nextended documents and complex information sequences. This benchmark was created for multi-task evaluation of long-\\ncontext inputs and includes 17 datasets covering tasks such as single-document QA (Koˇciský et al., 2018; Dasigi et al.,\\n2021), multi-document QA (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022; He et al., 2018), summarization (Huang\\net al., 2021; Zhong et al., 2021; Fabbri et al., 2019; Wu et al., 2023), few-shot learning (Li & Roth, 2002; Gliwa et al., 2019;\\nJoshi et al., 2017), synthetic tasks and code generation (Guo et al., 2023; Liu et al., 2024d). The datasets feature an average\\ninput length ranging from 1K to 18K tokens, requiring substantial memory for KV cache management.\\n\\nTable 19 shows the statistics of the datasets that we used in our experiments.\\n\\n| DATASET | # TRAIN | # TEST |\\n| --- | --- | --- |\\n| GSM8K (COBBE ET AL., 2021) | 7,473 | 1,319 |\\n| LONGBENCH (BAI ET AL., 2024) | - | 4,750 |\\n| NIAH* (KAMRADT, 2023) | - | 800 |\\n\\n\\nTable 19: Dataset Statistics. # TRAIN and # TEST represent the number of training and test samples, respectively. *: The\\nsize of the NIAH test set varies based on the context length and step size, typically around 800 samples per evaluation.\\n\\n# G. Prompt\\n\\nTable 20 shows the prompt for the Figure 1\\n\\nThe prompt for demonstration\\n\\n. . . . . .\\n\\n. . . . . .\\n\\nThe purple-crested turaco (Gallirex porphyreolophus) or, in South Africa, the purple-crested loerie, (Khurukhuru in the\\nLuvend. a (Vend. a) language) is a species of bird in the clade Turaco with an unresolved phylogenetic placement. Initial\\nanalyses placed the purple-crested turaco in the family Musophagidae, but studies have indicated that these birds do\\nnot belong to this family and have been placed in the clade of Turacos with an unresolved phylogeny. It is the National\\nBird of the Kingdom of Eswatini, and the crimson flight feathers of this and related turaco species are important in\\nthe ceremonial regalia of the Swazi royal family. This bird has a purple-coloured crest above a green head, a red ring\\naround their eyes, and a black bill. The neck and chest are green and brown. The rest of the body is purple, with red\\nflight feathers. Purple-crested turacos are often seen near water sources, where they can be observed dri', 'essing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\\nM., Petroni, F., and Liang, P. Lost in the middle: How\\nlanguage models use long contexts. Transactions of the\\nAssociation for Computational Linguistics, 12:157–173,\\n2024c. doi: 10.1162/tacl_a_00638. URL https://\\naclanthology.org/2024.tacl-1.9.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\\nrepository-level code auto-completion systems. In The\\nTwelfth International Conference on Learning Represen-\\ntations, 2024d. URL https://openreview.net/\\nforum?id=pPjZIOuQuF.\\n\\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyril-\\nlidis, A., and Shrivastava, A. Scissorhands: Exploiting\\nthe persistence of importance hypothesis for llm kv cache\\ncompression at test time. Advances in Neural Information\\nProcessing Systems, 36, 2024e.\\n\\nLuo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv:\\nA benchmark for assessing the robustness of multimodal\\nlarge language models against jailbreak attacks. In First\\nConference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=GC4mXVfquq.\\n\\nMeta. Introducing meta llama 3: The most capable openly\\navailable llm to date. https://ai.meta.com/\\nblog/meta-llama-3/, 2024. Accessed: 2024-06-\\n07.\\n\\nMohtas', ' LLM Inference\\n\\nTable 13: Reusing Indexing Performance Comparison on GSM8K\\n\\n| Model | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers | Number of Index Reuse Layers |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Model | 1 | 2 | 3 | 5 | 8 | 10 | 20 | 28/32 |\\n| LLaMA-3-8B-Instruct | 74.5 | 74.6 | 65.9 | 44.1 | 15.3 | 2.20 | 1.60 | 1.80 |\\n| Qwen2-7B-Instruct | 71.2 | 71.2 | 73.0 | 69.4 | 67.4 | 71.1 | 54.0 | 49.4 |\\n\\n\\nB.2. LongBench\\n\\nThe Table 14 shows the average performance of KV cache compression methods in the LongBench English subtask\\ncategories. The ChunkKV achieves the best performance on the overall average, and the Multi-Document QA category,\\nwhich supports that chunk method is more effective for semantic preservation.\\n\\nTable 14: Comprehensive performance comparison of KV cache compression methods across LongBench English subtasks.\\nResults are shown for various models and tasks, highlighting the effectiveness of different compression techniques.\\n\\n| Method | Single-Document QA | Single-Document QA | Single-Document QA | Multi-Document QA A | Multi-Document QA A | Multi-Document QA A | Summarization | Summarization | Summarization | Few-shot Learning | Few-shot Learning | Few-shot Learning | Synthetic Code m n t L c c B P R e R u | Synthetic Code m n t L c c B P R e R u | Synthetic Code m n t L c c B P R e R u | P - Avg. ↑ |\\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\\n| Method | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | v Q A e r e n t Q A Q e p o rt m e w s A u M F - T R E C a Q Q a s p M u si q u N rt P Q M S u T ri v i M u lti N H o t p o S A M S G o v R e 2 W i k i M | P - Avg. ↑ |\\n| ', 'ity on the following tokens, i.e. requiring larger {KLu(0*||�)}u>j.\\n\\nOn the other hand, the token-level sparsity enlarges the requirement on the distinguishability uniformly for each example Oi\\n(the case in Figure 1), which uniformly loses the bound of L0-1 (fn) as in Equation 9.\\n\\nChunk-level Importance Measure. Different from token-level importance measure, ChunkKV regards tokens in a\\ncontinuous window as a basic unit that should be left or discarded as a whole. The preserved window can be regarded\\nas saving the complete Oi = [xi, yi] without noise. Thus, ChunkKV reduces the noise §⌀(r) for the preserved Oi, which\\nlowers the bound of L0-1 (fn).\\n\\nMore intuitively, ChunkKV focus on reducing the noise on some complete training examples, but some other examples\\noverall with low importance will be discarded. Then, the model identifies the Xtest from those clean and more related training\\nexamples Oi and neglect those Oi with less importance.\\n\\nNote that here, we do not provide the rigorous proof on how KV cache sparsity enhances the requirement of the distin-\\nguishability and how different KLj (�*||�) on Oi = [xi, yi] influences the bound L0-1 (fn). We left this as the future work to\\nanalyze how KV cache sparsity influences the in-context learning.\\n\\n2Here, training means prompt learning (Fang & Xie, 2022).\\n\\n32ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# D. Additional Related Work\\n\\nKV cache sharing Recent work has explored various strategies for sharing KV caches across transformer layers. Layer-\\nCondensed KV Cache (LCKV) (Wu & Tu, 2024) computes KVs only for the top layer and pairs them with queries from all\\nlayers, while optionally retaining standard attention for a few top and bottom layers to mitigate performance degradation.\\nSimilarly, You Only Cache Once (YOCO) (Sun et al., 2024) computes KVs exclusively for the top layer but pairs them with\\nqueries from only the top half of layers, employing efficient attention in the bottom layers to maintain a constant cache\\nsize. In contrast, Cross-Layer Attention (CLA) (Brandon et al., 2024) divides layers into groups, pairing queries from all\\nlayers in each group with KVs from that group’s bottom layer. MiniCache (Liu et al., 2024a) introduces a novel method\\nthat merges layer-wise KV caches while enabling recovery during compute-in-place operations, optimizing KV cache size.\\nThese methods illustrate various trade-offs between computation, memory usage, and model performance when sharing KV\\ncaches across transformer layers.\\n\\nLong-Context Benchmarks The landscape of long-context model benchmarks has evolved to encompass a wide range\\nof tasks, with particular emphasis on retrieval and comprehension capabilities. Benchmarks for understanding have made\\nsignificant strides, with ∞-Bench (Zhang et al., 2024) pushing the boundaries by presenting challenges that involve more\\nthan 100,000 tokens. LongBench (Bai et al., 2024) has introduced bilingual evaluations, addressing tasks such as long-\\ndocument question answering, summarization, and code completion. Complementing these efforts, ZeroSCROLLS (Shaham\\net al., 2023) and L-Eval (An et al., 2023) have broadened the scope to include a diverse array of practical natural language\\ntasks, including query-driven summarization.\\n\\nIn parallel, retrieval benchmarks have largely relied on synthetic datasets, offering researchers precise control over variables\\nsuch as the length of input tokens. This approach minimizes the impact of disparate parametric knowledge resulting from\\nvaried training methodologies. A significant body of recent work has concentrated on the development of synthetic tasks\\nspecifically for retrieval evaluation (Kamradt, 2023; Mohtashami & Jaggi, 2023; Li et al., 2023; Liu et al., 2024c; Hsieh\\net al., 2024). In addition, researchers have explored the potential of extended contexts in facilitating various forms of\\nreasoning (Tay et al., 2021).\\n\\nThis dual focus on synthetic retrieval tasks an', ') and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nreasoning (O1 and R1) LLMs, achieving state-of-the-art\\nperformance.\\n\\n# 2. Related Work\\n\\nKV Cache Compression. KV cache compression technol-\\nogy has developed rapidly in the era of LLM, with meth-\\nods mainly focused on evicting unimportant tokens. The\\ncompression process occurs before the attention blocks, op-\\ntimizing both the prefilling time and GPU memory. Xiao\\net al. (2024) and Han et al. (2024) propose that initial and re-\\ncent tokens consistently have high attention scores between\\ndifferent layers and attention heads. As a result, retaining\\nthese tokens in the KV cache is more likely to preserve im-\\nportant information. Furthermore, FastGen (Ge et al., 2023)\\nevicts tokens based on observed patterns. H2O (Zhang et al.,\\n2023) and SnapKV (Li et al., 2024) employ dynamic KV\\ncache compression methods, evaluating the importance of\\ntokens based on attention scores and then evicting the less\\nimportant ones. As inference scenarios become increas-\\ningly complex, dynamic KV cache compression methods\\ndemonstrate powerful performance. Recently, Yang et al.\\n(2024b) and Cai et al. (2024) have closely examined the dis-\\ntributions of attention scores during the pre-filling stage of\\nthe Retrieval-Augmented Generation (RAG) task, discover-\\ning a pyramidal KV cache compression pattern in different\\ntransformer layers.\\n\\nAlthough these KV cache compression methods have ex-\\nplored efficient GPU memory management while maintain-\\ning original performance, our study focuses more on the\\nsemantic information of the prompt. We find that chunks\\nof the original KV cache are more important than discrete\\ntokens.\\n\\nChunking Method. The chunking methodology is widely\\nused in the field of NLP due to its simplicity and effective-\\nness (Tjong Kim Sang & Veenstra, 1999). In the era of\\nLLMs, chunking is primarily applied in data pre-processing.\\nFor example, Shi et al. suggest grouping related train-\\ning data into chunks to achieve better convergence curves\\nto pre-train LLMs. Fei et al. (2024) apply a topic-based\\nchunking method to improve the semantic compression of\\nprompts. Furthermore, chunking plays an important role in\\nthe Retrieval-Augmented Generation (RAG) field (Yepes\\net al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It\\nserves to divide documents into units of information with\\nsemantic content suitable for embedding-based retrieval and\\nprocessing by LLMs.\\n\\nLayer-Wise Technique The layer-wise technique is widely\\nused in the training and inference of large language models\\n(LLMs). LISA (Pan et al., 2024a) is a layer-wise sampling\\nmethod based on observations of the training dynamics of\\nLow-Rank Adaptation (LoRA)(Hu et al., 2022) across lay-\\n\\ners. LAMB(You et al., 2020)', 'hami, A. and Jaggi, M. Landmark attention:\\nRandom-access infinite context length for transformers.\\nArXiv preprint, abs/2305.16300, 2023. URL https:\\n//arxiv.org/abs/2305.16300.\\n\\nOpenAI. Gpt-4o-mini: Advancing cost-efficient intelli-\\ngence, 2023. Accessed: 2023-12-14.\\n\\nPan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C.,\\nand Zhang, T. Lisa: Layerwise importance sampling\\nfor memory-efficient large language model fine-tuning.\\nArXiv preprint, abs/2403.17919, 2024a. URL https:\\n//arxiv.org/abs/2403.17919.\\n\\nPan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K.,\\nZhang, J., Pi, R., and Zhang, T. Plum: Prompt learning us-\\ning metaheuristics. In Ku, L.-W., Martins, A., and Sriku-\\nmar, V. (eds.), Findings of the Association for Computa-\\ntional Linguistics ACL 2024, pp. 2177–2197, Bangkok,\\nThailand and virtual meeting, August 2024b. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2024.\\nfindings-acl.129. URL https://aclanthology.\\norg/2024.findings-acl.129.\\n\\nPires, B. Á. and Szepesvári, C. Multiclass classification\\ncalibration functions. arXiv preprint arXiv:1609.06385,\\n2016.\\n\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lill-\\nicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Fi-\\nrat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of\\ncontext. ArXiv preprint, abs/2403.05530, 2024. URL\\nhttps://arxiv.org/abs/2403.05530.\\n\\nShaham, U., Ivgi, M., Efrat, A., Berant, J., and\\nLevy, O. ZeroSCROLLS: A zero-shot benchmark for\\nlong text understanding. In Bouamor, H., Pino, J.,\\nand Bali, K. (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023, pp. 7977–\\n7989, Singapore, 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.\\n536. URL https://aclanthology.org/2023.\\nfindings-emnlp.536.\\n\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V.,\\nSmith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis,\\nM. In-context pretraining: Language modeling beyond\\ndocument boundaries. In The Twelfth International Con-\\nference on Learning Representations.\\n\\nSmith, B. and Troynikov, A. Evaluating chunking\\nstrategies for retrieval. Technical report, Chroma,\\n2024. URL https://research.trychroma.\\ncom/evaluating-chunking.\\n\\nSteinwart, I. How to compare different loss functions and\\ntheir risks. Constructive Approximation, 26:225–287,\\n2007. URL https://api.semanticscholar.\\norg/CorpusID:16660598.\\n\\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma,\\nS., Zhang, Q., Wang, J., and Wei, F. You only cache\\nonce: Decoder-decoder architectures for language models.\\narXiv preprint arXiv:2405.05254, 2024.\\n\\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\\nS. Quest: Query-aware sparsity for efficient long-context\\nllm inference. ArXiv preprint, abs/2406.10774, 2024.\\nURL https://arxiv.org/abs/2406.10774.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,\\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\\nD. Long range arena : A benchmark for efficient trans-\\nformers. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=qVyeW-grC2k.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\\nUnifying language learning paradigms. ArXiv preprint,\\nabs/2205.05131, 2022. URL https://arxiv.org/\\nabs/2205.05131.\\n\\n12ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundatio']\n",
            "Based on the retrieved content, answer the user question.\n",
            "\n",
            "Assistant: The paper 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' presents a novel KV cache compression method called ChunkKV. This method focuses on preserving essential semantic information by selectively retaining chunks of the original KV cache. The authors evaluate ChunkKV on various long-context benchmarks and demonstrate its superior performance in both efficiency and accuracy compared to existing KV cache compression methods. The key contributions of this paper include identifying the phenomenon of discrete KV cache compression methods inadvertently pruning necessary semantic information, proposing ChunkKV as a simple yet effective approach to KV cache compression, and achieving state-of-the-art performance on cutting-edge long-context benchmarks.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            if value[\"messages\"][-1].content:\n",
        "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "                print()\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHUaI7-uM_mn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNws3v19x6yrKxJ0POnLedu",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
