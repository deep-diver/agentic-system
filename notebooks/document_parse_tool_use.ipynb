{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Parse as a Tool w/ LLMs\n",
    "\n",
    "In this tutorial we are going to explore how to implement a triage agentic system using Tool(Function) Calls with LLMs from scratch. To proceed with this tutorial, you'll need API keys for OpenAI, Serper, and Upstage. We'll be building an intelligent document parsing system that can triage and route documents, extract relevant information from PDFs, and use that information to answer questions appropriately.\n",
    " \n",
    "We'll cover:\n",
    "1. Setting up the necessary tools and APIs\n",
    "2. Creating function schemas for tool calling and triage routing\n",
    "3. Implementing document parsing and classification functionality\n",
    "4. Building the triage agent architecture\n",
    "5. Integrating these tools with LLMs for intelligent routing\n",
    " \n",
    "This approach demonstrates how LLMs can be enhanced with specialized tools and agentic capabilities to handle complex document workflow tasks. The triage system will intelligently analyze documents, determine their type and priority, extract key information, and route them to appropriate downstream processes. By the end, you'll understand how to create your own tool-augmented LLM systems with intelligent triage capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies\n",
    "\n",
    "Let's highlight some key dependencies with why we need them:\n",
    "\n",
    "- PyPDF2: For reading and parsing PDF documents\n",
    "- OpenAI: This is a standard SDK for any LLM API calls. In this tutorial, we are going to use it to access GPT models from OpenAI and Solar models from Upstage.\n",
    "- Pydantic: For data validation and schema generation\n",
    "- dotenv: For loading environment variables containing API keys\n",
    "\n",
    "> ❓ Why do we include GPT models in this tutorial?\n",
    "\n",
    "While our main goal is to integrate Document Parse in Solar model, it is quite convenient to test things first with GPT models. GPT models are known to be reliable and performant on various tasks, so they serve as a good upperbound for evaluating our document parsing system. Then we can safely move to replace GPT model with Solar model. This is a common strategy for every engineering project to not being lost in the middle of developing the entire system.\n",
    "\n",
    "> ❓ What is the role of dotenv?\n",
    "\n",
    "dotenv is a Python library that helps manage environment variables by loading them from a .env file into your application's environment. This is particularly useful for handling sensitive information like API keys that shouldn't be hardcoded in your source code. To use dotenv, first create a .env file in your project's root directory and add your environment variables in KEY=VALUE format. Then, in your Python code, import load_dotenv from python-dotenv and call load_dotenv() at the start of your program. After that, you can access the environment variables using os.getenv(\"KEY_NAME\"). This keeps your sensitive credentials secure and makes it easier to manage different configurations across development and production environments.\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import PyPDF2\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the API Keys\n",
    "\n",
    "Once `load_dotenv()` is called successfully, you will see `True` is returned and printed out. At this point, all the variables from `.env` file is loaded up as environment variable. Hence, you can access them with `os.getenv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we are going to need the following three API keys:\n",
    "- `SERPER_API_KEY`: API key for Google Search API service from [Serper.dev](https://serper.dev/)\n",
    "- `OPENAI_API_KEY`: API key for accessing [OpenAI](https://openai.com/)'s GPT models. \n",
    "- `UPSTAGE_API_KEY`: API key for accessing [Upstage](https://www.upstage.ai/)'s Solar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPER_API_KEY = os.getenv(\"SERPER_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "UPSTAGE_API_KEY = os.getenv(\"UPSTAGE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Helper Functions\n",
    "\n",
    "Here, we are defining two helper functions necessary for tool(function) calling:\n",
    "- `function_to_schema()`: Converts a Python function into a JSON schema format that can be used for function calling with LLMs. This can be thought as just structured string that is going to be injected into LLM as context so that LLM can understand what kind of functions are available to call\n",
    "- `execute_tool_call()`: Executes the function call based on the LLM's response and returns the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_to_schema(func) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a Python function into a JSON schema format for LLM function calling.\n",
    "    \n",
    "    Args:\n",
    "        func: The Python function to convert to schema\n",
    "        \n",
    "    Returns:\n",
    "        dict: JSON schema describing the function's interface\n",
    "        \n",
    "    The schema includes:\n",
    "    - Function name\n",
    "    - Description from docstring\n",
    "    - Parameters with their types\n",
    "    - Required parameters list\n",
    "    \"\"\"\n",
    "    # Map Python types to JSON schema types\n",
    "    type_map = {\n",
    "        str: \"string\",\n",
    "        int: \"integer\", \n",
    "        float: \"number\",\n",
    "        bool: \"boolean\",\n",
    "        list: \"array\",\n",
    "        dict: \"object\",\n",
    "        type(None): \"null\",\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Get function signature using inspect\n",
    "        signature = inspect.signature(func)\n",
    "    except ValueError as e:\n",
    "        raise ValueError(\n",
    "            f\"Failed to get signature for function {func.__name__}: {str(e)}\"\n",
    "        )\n",
    "\n",
    "    # Build parameters dictionary\n",
    "    parameters = {}\n",
    "    for param in signature.parameters.values():\n",
    "        try:\n",
    "            # Get JSON type for parameter, default to string if type not found\n",
    "            param_type = type_map.get(param.annotation, \"string\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(\n",
    "                f\"Unknown type annotation {param.annotation} for parameter {param.name}: {str(e)}\"\n",
    "            )\n",
    "        parameters[param.name] = {\"type\": param_type}\n",
    "\n",
    "    # Get list of required parameters (those without default values)\n",
    "    required = [\n",
    "        param.name\n",
    "        for param in signature.parameters.values()\n",
    "        if param.default == inspect._empty\n",
    "    ]\n",
    "\n",
    "    # Return complete schema\n",
    "    return {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": (func.__doc__ or \"\").strip(),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": parameters,\n",
    "                \"required\": required,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    \n",
    "def execute_tool_call(tool_call, tools, agent_name):\n",
    "    \"\"\"\n",
    "    Executes a function call based on the LLM's response.\n",
    "    \n",
    "    Args:\n",
    "        tool_call: Object containing function call details from LLM\n",
    "        tools: Dictionary mapping function names to actual functions\n",
    "        agent_name: Name of the agent making the call, for logging\n",
    "        \n",
    "    Returns:\n",
    "        The result of executing the specified function with given arguments\n",
    "        \n",
    "    This function:\n",
    "    1. Extracts function name and arguments from tool_call\n",
    "    2. Logs the function call\n",
    "    3. Executes the function with provided arguments\n",
    "    \"\"\"\n",
    "    # Extract function name and parse arguments from JSON\n",
    "    name = tool_call.function.name\n",
    "    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "    # Log the function call\n",
    "    print(f\"{agent_name}:\", f\"{name}({args})\")\n",
    "\n",
    "    # Execute the function with unpacked arguments\n",
    "    return tools[name](**args)  # call corresponding function with provided arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Tool Calling\n",
    "\n",
    "Tool(function) calling is going to happen essentially in real environment, hence it would actually cost some resources (money, time, etc.,). That means we should be able to test if LLM can really understand well about the functions that we are going to provide without actually executing them. \n",
    "\n",
    "\n",
    "We can validate the LLM's understanding of the tools by defining functions with only signatures and docstrings, without actual implementations. Tool calling is essentially prompt engineering, so we should validate the LLM's understanding before making real API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a simple wrapper class and initial user prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement Triage Agentic system with dummy functions\n",
    "\n",
    "Let's define triage agentic system with the following three components:\n",
    "- `to_paper_search_agent()`: A dummy function that does nothing but with appropriate docstring for LLMs to understand this function is for searching paper information. This dummy function will be implemented in detail with Serper.dev API.\n",
    "- `to_download_and_parse_paper_agent()`: A dummy function that does nothing but with appropriate docstring for LLMs to understand this function is for downloading and parsing paper PDF. This dummy function will be implemented in detail with Upstage's Document Parse API.\n",
    "- `supervisor_agent`: The center of triage agentic system. While it could looks too fancy, this is just LLM with designated tools(functions) with appropriate system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent():\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "def to_download_and_parse_paper_agent():\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    return \"\"\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]\n",
    ")\n",
    "\n",
    "tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "tools = {tool.__name__: tool for tool in supervisor_agent.tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call LLM with Function schemas (1st conversation)\n",
    "\n",
    "Let's make a call to LLM with the pre-defined functions. The JSON schema of the pre-defined functions are already prepared in `tool_schemas` with `function_to_schema()`. \n",
    "\n",
    "> ❓ Why making a manual chat complemetion call?\n",
    "\n",
    "While we can make chat completion calls iteratively in a while loop, this would make us hard to analyze any potential issues. This is especially true when you are dealing with LLMs since LLM itself is already very high-level entity. We don't know what is going on inside of them, hence we have to make sure if everything works as expected one by one before moving on to the fully autonomous system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_7zbELPSWR4JXMbjR5OQoB4p2', function=Function(arguments='{}', name='to_paper_search_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Initial trial\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the outputs\n",
    "- `response.choices[0].message.content`: when LLM makes a decision to make tool(function) calls, `response.choices[0].message.content` should contain nothing (hence, `None`).\n",
    "- `response.choices[0].message.tool_calls`: instead, when LLM makes a decision to make tool(function) calls, `response.choices[0].message.tool_calls` should contain `ChatCompletionMessageToolCall` object instance. Inside `ChatCompletionMessageToolCall`, there are all the information what we need to do next such as:\n",
    "  - `.function.name`: what tool(function) has to be called\n",
    "  - `.function.arguments`: what arguments should be passed to the tool(function) when called\n",
    "\n",
    "We see the LLM correctly made a decision to call `to_paper_search_agent` function. This is correct because we don't have the paper information yet, so we are going to have to go through searching process before analyzing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call LLM with Function schemas (2nd conversation)\n",
    "\n",
    "Let's make the 2nd call to LLM. At this time, we are going to inject a dummy message with the tool(function) calling output. It has to follow the below format:\n",
    "\n",
    "```python\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n",
    "        \"content\": \"Paper URL: https://arxiv.org/abs/2502.00299\"\n",
    "    }\n",
    "```\n",
    "\n",
    "> ❓ here, you have to assign the ID of the tool(function) that LLM has previoulsy decided to call. That ID can be found in the previous message. If these IDs do not match, it will cause an error. \n",
    "\n",
    "We are pretending that we have successfully obtained the URL of the paper as the result of the `to_paper_search_agent` function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[ChatCompletionMessageToolCall(id='call_TX7Y8E6mwqjri6tDVbvbRZY3', function=Function(arguments='{}', name='to_download_and_parse_paper_agent'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Second)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id,\n",
    "        \"content\": \"Paper URL: https://arxiv.org/abs/2502.00299\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we see the LLM correctly made a decision to call `to_download_and_parse_paper_agent` function. This is correct because we have the target URL of the paper, so it is time to download and pare the content of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call LLM with Function schemas (3rd conversation)\n",
    "\n",
    "Let's make the 3rd call to LLM. At this time, we are going to inject a dummy message with the tool(function) calling output again. It has to follow the below format:\n",
    "\n",
    "```python\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id, \n",
    "        \"content\": \"Retrieved Paper Content\\n\"\n",
    "        \"--------------------------------\\n\"\n",
    "        \"Title: ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\"\n",
    "        \"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods.\"\n",
    "    }\n",
    "```\n",
    "\n",
    "To simulate function callings and their results, I simply grasped the actual content of the abtract of the paper from arXiv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" addresses the challenge of reducing memory costs in long-context inference using Large Language Models (LLMs). It identifies limitations in previous key-value (KV) cache compression methods which treat token importance in isolation, overlooking inter-token dependencies inherent in language.\n",
      "\n",
      "To counter this, ChunkKV introduces a novel approach where tokens are grouped into chunks, serving as the fundamental compression unit. This method prioritizes retaining chunks that hold significant semantic information and discards less important ones. An additional innovation is the proposal of layer-wise index reuse, leveraging the observed similarity in preserved indices across different layers to reduce computational demands further.\n",
      "\n",
      "The effectiveness of ChunkKV is demonstrated through evaluations on advanced long-context benchmarks like LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV, along with testing on instruction tuning and multi-step reasoning models. The approach yields up to a 10% performance improvement over existing methods, even under stringent compression settings.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Subsequent trial (Third)\n",
    "messages.append(response.choices[0].message)\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\", \n",
    "        \"tool_call_id\": response.choices[0].message.tool_calls[0].id, \n",
    "        \"content\": \"Retrieved Paper Content\\n\"\n",
    "        \"--------------------------------\\n\"\n",
    "        \"Title: ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\"\n",
    "        \"To reduce memory costs in long-context inference with Large Language Models (LLMs), many recent works focus on compressing the key-value (KV) cache of different tokens. However, we identify that the previous KV cache compression methods measure token importance individually, neglecting the dependency between different tokens in the real-world language characterics. In light of this, we introduce ChunkKV, grouping the tokens in a chunk as a basic compressing unit, and retaining the most informative semantic chunks while discarding the less important ones. Furthermore, observing that ChunkKV exhibits higher similarity in the preserved indices across different layers, we propose layer-wise index reuse to further reduce computational overhead. We evaluated ChunkKV on cutting-edge long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K and JailbreakV in-context learning benchmark. Our experiments with instruction tuning and multi-step reasoning (O1 and R1) LLMs, achieve up to 10\\% performance improvement under aggressive compression ratios compared to existing methods.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=supervisor_agent.model,\n",
    "    messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "    tools=tool_schemas or None,\n",
    "    tool_choice=\"auto\",\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "print(response.choices[0].message.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, we see the LLM correctly made a decision not to call any function. This is correct because we already have the actual content of the paper. Hence, instead of making an another tool(function) calling, the LLM decided to answer the question from the initial user prompt: `\"Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv.\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in the Dummy Tools (Functions)\n",
    "\n",
    "Now, it is time to fill in the dummy functions with actual behaviour! We are starting off with the same setup as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseModel):\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"gpt-4o\"\n",
    "    instructions: str = \"You are a helpful Agent\"\n",
    "    tools: list = []\n",
    "    \n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define triage agentic system with the following three components:\n",
    "- `to_paper_search_agent(paper_title: str)`: It has the same docstring as we had in the dummy function along with additional argument of `paper_title`. The information of this argument will be passed to the LLM after applying `function_to_schema()` function to it.\n",
    "  - what this function does is very simple. It makes an API call to Serper.dev for google search, then return the first searched item whose link is starting with `https://arxiv.org`.\n",
    "  - how the value of `paper_title` argument is assigned even though we gave a mission in natural language? This is totally up to the LLM itself. The LLM decides what value to assign to the argument.\n",
    "\n",
    "- `to_download_and_parse_paper_agent(paper_url: str)`: It has the same docstring as we had in the dummy function along with additiona argument of `paper_url`. The information of this argument will be passed to the LLM after applying `function_to_schema()` function to it.\n",
    "  - this function simply download the PDF of the paper with the given URL, then it returns the first 10,000 characters of it.\n",
    "  - as this function expects paper_url as argument (required), LLM will make a decision to call it only when URL info. is provided. \n",
    "\n",
    "- `supervisor_agent`: The center of triage agentic system. This is exactly the same as in our dummy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_paper_search_agent(paper_title: str):\n",
    "    \"\"\"Use this to search for paper URL on arXiv only when paper URL is not found yet.\"\"\"\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "\n",
    "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
    "    headers = {\n",
    "        'X-API-KEY': SERPER_API_KEY,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "    search_results = response.json()['organic']\n",
    "    \n",
    "    if len(search_results) == 0:\n",
    "        return \"Count not find the URL to download the paper\"\n",
    "    \n",
    "    first_result = search_results[0]\n",
    "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
    "        return \"Could not find the URL to download the paper\"\n",
    "    \n",
    "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    response = requests.get(paper_url)\n",
    "    pdf_reader = PyPDF2.PdfReader(BytesIO(response.content))\n",
    "    text = \"Retrieved Paper Content\\n-----------------------------------\\n\"\n",
    "    for page in pdf_reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text.strip()[:10000]\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]#, to_paper_analysis_agent]#, to_triage, to_end_agent],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of making a sequential LLM API calls manually, let's create a simple `run` function that does the same thing with `While` loop. The loop is escaped when there is no need for tool(function) call anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(client, messages, supervisor_agent):\n",
    "    # Loop through the conversation steps\n",
    "    while True:\n",
    "        # Prepare tools for the current step\n",
    "        tool_schemas = [function_to_schema(tool) for tool in supervisor_agent.tools]\n",
    "        tools = {tool.__name__: tool for tool in supervisor_agent.tools}\n",
    "        \n",
    "        # Get model response\n",
    "        response = client.chat.completions.create(\n",
    "            model=supervisor_agent.model,\n",
    "            messages=[{\"role\": \"system\", \"content\": supervisor_agent.instructions}] + messages,\n",
    "            tools=tool_schemas or None,\n",
    "            tool_choice=\"auto\",\n",
    "        )\n",
    "        \n",
    "        if response.choices[0].message.tool_calls:\n",
    "            print(response.choices[0].message.tool_calls)\n",
    "        else:\n",
    "            print(\"--------------------------------\")\n",
    "            print(response.choices[0].message.content)\n",
    "            print(\"--------------------------------\")\n",
    "            break # escape the loop when there is no need for tool(function) call anymore\n",
    "        \n",
    "        # Add model response to messages\n",
    "        messages.append(response.choices[0].message)\n",
    "        \n",
    "        # Add tool response to messages\n",
    "        if response.choices[0].message.tool_calls:\n",
    "            for tool_call in response.choices[0].message.tool_calls:\n",
    "                tool_response = execute_tool_call(tool_call, tools, supervisor_agent.name)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\", \n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"content\": tool_response\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_td4d3gSzmIjXLckS2b5m0D8z', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='call_zg8EX15mTG0LBnGf4Op3EcrA', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper titled \"ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\" presents a new method for reducing memory costs in long-context inference with Large Language Models (LLMs). Here’s a comprehensive summary of the paper:\n",
      "\n",
      "### Introduction\n",
      "- Large Language Models (LLMs) are critical for processing natural language tasks that require interpreting long contexts. These tasks sometimes involve tens of thousands of tokens.\n",
      "- A significant challenge in using LLMs for long-context processing is the GPU memory requirements, primarily due to the key-value (KV) cache.\n",
      "- Existing approaches to address memory consumption in KV caching focus on compressing the cache by pruning non-important parts of the prompt tokens.\n",
      "\n",
      "### Main Contributions\n",
      "1. **ChunkKV Method**: \n",
      "   - Introduces a novel compression method by grouping tokens into chunks instead of measuring token importance individually, which traditional methods do.\n",
      "   - This approach retains the most informative semantic chunks while discarding less important ones, effectively reducing memory usage without losing crucial semantic information.\n",
      "\n",
      "2. **Layer-Wise Index Reuse**:\n",
      "   - Observes that ChunkKV exhibits higher similarity in the preserved indices across different layers, leading to the development of the layer-wise index reuse technique.\n",
      "   - This technique minimizes the additional computational time typically introduced by KV cache compression.\n",
      "\n",
      "3. **Evaluation and Results**:\n",
      "   - Evaluated ChunkKV on several benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV.\n",
      "   - Demonstrated up to 10% performance improvement under aggressive compression ratios compared to existing methods.\n",
      "\n",
      "### Methodology\n",
      "- Instead of focusing on discrete tokens, ChunkKV takes into account the continuity and dependencies in language, maintaining complete chunks to preserve semantic integrity.\n",
      "- The paper highlights the constraints of prior methods which often miss out on essential semantic information due to isolated token importance measurement.\n",
      "\n",
      "### Experiments and Findings\n",
      "- Conducted detailed experiments using various benchmarks and different models.\n",
      "- Found that ChunkKV outperforms existing methods by effectively preserving semantic information while efficiently reducing computational overhead.\n",
      "\n",
      "### Conclusion\n",
      "- ChunkKV provides a straightforward solution to the KV cache compression problem by focusing on semantic preservation.\n",
      "- It successfully reduces memory use while maintaining, or even improving, model performance.\n",
      "\n",
      "Overall, the paper proposes a simple yet effective approach to KV cache compression, emphasizing the semantic importance of contiguous token chunks, which enhances the efficiency of long-context LLM inference.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, everything now works as expected with the actual function implementations.\n",
    "\n",
    "Even though this tutorial is going super smooth, it is worth noting that this tutorial was written on top of lots of tials and errors of prompt engineering on system prompt, docstrings, etc,. Hence, keep in mind to start with dummy implementation to secure your budget. After you are confident with your dummy example, you can safely move on to the actual implementation. This will significantly reduce the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the same with Upstage's Solar-Pro \n",
    "\n",
    "We tried out the Triage Agentic system with OpenAI's GPT-4o. We can easily switch to Upstage's Solar model. The only thing you should make changes is the API endpoint and the API key. Other than that, all remains the same. Check out the below code snippet and the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='3e45e34a-a053-4b8a-9425-2fb99aa4ff95', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='82640c4a-0263-4b0f-b646-2d52a50080ec', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n",
      "--------------------------------\n",
      "The paper 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' presents a novel approach to KV cache compression that aims to preserve semantic information in the cache. The authors propose a method called ChunkKV, which groups tokens into chunks and retains the most informative semantic chunks while discarding the less important ones. They also introduce a layer-wise index reuse technique to further reduce computational overhead. The paper evaluates ChunkKV on various benchmarks and demonstrates its superior performance over existing methods. The key contributions of the paper include the identification of the phenomenon in which discrete KV cache compression methods inadvertently prune necessary semantic information, the proposal of ChunkKV as a simple yet effective KV cache compression method, and the development of the layer-wise index reuse technique to reduce additional computational time.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent.model = \"solar-pro\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]\n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Document Parse as a Tool\n",
    "\n",
    "Previously, the `to_download_and_parse_paper_agent()` function simply truncated the content of PDF by 10,000 characters. Since PDF has some meta information to keep the document as PDF format, this would have some impact on quality and token usage. \n",
    "\n",
    "Instead, we are going to modify the `to_download_and_parse_paper_agent()` to use Upstage's Document Parse. Document Parse is known to be the best document analyzer, and we can extract information of any kind of documents into various formats (HTML, Markdown, etc,.). We can hope to lower the token usage and to increase accuracy of the document analysis with Document Parse.\n",
    "\n",
    "Additionally, in this section, we are going to have `truncate_tokens_if_needed()` helpful function. Since some LLMs have limited context length support (Solar offers 32K context length), a full length of a single PDF can't be fit. If we pass more tokens than what model could handle, that would crash the program. Hence, we have to manually fill up the full context length by calculating the number of tokens in a given prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you are using macos\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/opt/homebrew/bin/:\" + os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what `truncate_tokens_if_needed()` function does:\n",
    "- It applies the `apply_chat_template()` function of the given tokenizer to the messages. `apply_chat_template()` function correctly translates(encodes) the whole messages into tokens with special tokens. Upstage provides the tokenizer as open source on Hugging Face, hence we can simply load it up with `transformers` library as below:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
    "```\n",
    "\n",
    "- It truncates the encoded tokens up to the allowed `max_token_limit` which is 32,000 for Solar model.\n",
    "- It decodes the tokens back to string and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_tokens_if_needed(tokenizer, messages, content, max_token_limit=32000):\n",
    "    \"\"\"\n",
    "    Truncate the markdown content if the total tokens exceed the maximum limit.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding/decoding\n",
    "        messages: List of message dictionaries for the conversation\n",
    "        content: The markdown content to potentially truncate\n",
    "        max_token_limit: Maximum token limit (default: 32000)\n",
    "        \n",
    "    Returns:\n",
    "        truncated_markdown: The potentially truncated markdown\n",
    "        base_token_numbers: Number of tokens in the base conversation\n",
    "        paper_token_numbers: Number of tokens in the paper after potential truncation\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": supervisor_agent.instructions}\n",
    "        ] + messages\n",
    "    )\n",
    "    base_token_numbers = len(inputs)\n",
    "    encoded_content = tokenizer.encode(content)\n",
    "    paper_token_numbers = len(encoded_content)\n",
    "\n",
    "    print(f\"Base token numbers: {base_token_numbers}\")\n",
    "    print(f\"Paper token numbers: {paper_token_numbers}\")\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "\n",
    "    total_token_numbers = base_token_numbers + paper_token_numbers\n",
    "\n",
    "    if total_token_numbers > max_token_limit:\n",
    "        # Calculate how many tokens we need to truncate\n",
    "        tokens_to_keep = max_token_limit - base_token_numbers\n",
    "        # Truncate the encoded markdown\n",
    "        encoded_content = encoded_content[:tokens_to_keep]\n",
    "        # Update the paper token count\n",
    "        paper_token_numbers = len(encoded_content)\n",
    "        # Update the markdown string by decoding the truncated tokens\n",
    "        truncated_content = tokenizer.decode(encoded_content)\n",
    "        print(f\"Truncated paper tokens to: {paper_token_numbers}\")\n",
    "    else:\n",
    "        print(\"No truncation needed\")\n",
    "        truncated_content = content\n",
    "\n",
    "    print(f\"Total token numbers: {base_token_numbers + paper_token_numbers}\")\n",
    "    return truncated_content, base_token_numbers, paper_token_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Upstage's Document Parse, we need to deal with some edge cases. That is, sometime Document parse fails at parsing a given document, and it does not return any intermediate results but just failure message.\n",
    "\n",
    "Imagine that you are makeing an API call to Document Parse with 30 pages long document. If that process fails, you are not only going to lose $0.01 dollars per page but also lose your valuable time. To prevent that akward moment happening, we can simply split the whole document into separate files which contain a single page each. `split_pdf_by_pages()` is a helper function for that purpose.\n",
    "\n",
    "Additionally, we have defined `get_document_parse_response()` wrapper function to make an API call to Document Parse easily. This is to keep the logic as simple as possible.\n",
    "\n",
    "Finally, `to_download_and_parse_paper_agent()` is modified to call `split_pdf_by_pages()` to split whole document into series of multiple files, `get_document_parse_response()` to parse the document each file by file, then `truncate_tokens_if_needed()` to truncate the contents up to the context length that Solar model supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import requests\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"upstage/solar-pro-preview-instruct\")\n",
    "message_template = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper below, \\n\"\n",
    "    },\n",
    "]\n",
    "\n",
    "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
    "    # Open the PDF\n",
    "    pdf = PdfReader(input_pdf_path)\n",
    "    total_pages = len(pdf.pages)\n",
    "    \n",
    "    # Calculate number of output PDFs needed\n",
    "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
    "    \n",
    "    output_paths = []\n",
    "    \n",
    "    # Create output directory using input filename\n",
    "    base_name = os.path.splitext(input_pdf_path)[0]\n",
    "    os.makedirs(base_name, exist_ok=True)\n",
    "    \n",
    "    # Split into multiple PDFs\n",
    "    for i in range(num_pdfs):\n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Calculate start and end pages for this split\n",
    "        start_page = i * pages_per_pdf\n",
    "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
    "        \n",
    "        # Add pages to writer\n",
    "        for page_num in range(start_page, end_page):\n",
    "            writer.add_page(pdf.pages[page_num])\n",
    "            \n",
    "        # Save the split PDF\n",
    "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
    "        with open(output_path, \"wb\") as output_file:\n",
    "            writer.write(output_file)\n",
    "        output_paths.append(output_path)\n",
    "        \n",
    "    return output_paths\n",
    "\n",
    "def get_document_parse_response(filename, api_key):\n",
    "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "    files = {\"document\": open(filename, \"rb\")}\n",
    "    data = {\"output_formats\": \"['markdown']\"}\n",
    "\n",
    "    response = requests.post(url, headers=headers, files=files, data=data)\n",
    "    upstage_response = json.loads(response.text)\n",
    "    return upstage_response\n",
    "\n",
    "def to_download_and_parse_paper_agent(paper_url: str):\n",
    "    \"\"\"Use this to download and parse paper only when paper URL is found.\"\"\"\n",
    "    response = requests.get(paper_url)\n",
    "    # Save the PDF to a temporary file\n",
    "    root_path = \"tmp\"\n",
    "    temp_pdf_path = \"temp_paper.pdf\"\n",
    "    with open(temp_pdf_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    shutil.rmtree(root_path, ignore_errors=True)\n",
    "    os.makedirs(root_path, exist_ok=True)\n",
    "\n",
    "    split_factor = 1\n",
    "    split_pdfs = split_pdf_by_pages(temp_pdf_path, root_path, split_factor) # by 10\n",
    "\n",
    "    markdown = \"\"\n",
    "    total_responses = []\n",
    "    for i, split_pdf in enumerate(split_pdfs):\n",
    "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
    "        \n",
    "        # Append the response to the total_responses list\n",
    "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})        \n",
    "        # Also write the response to a JSON file for persistence\n",
    "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
    "        with open(json_output_path, \"w\") as json_file:\n",
    "            json.dump(upstage_response, json_file, indent=2)\n",
    "\n",
    "        try:\n",
    "            markdown += upstage_response['content']['markdown']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    markdown = \"Retrieved Paper Content\\n-----------------------------------\\n\" + markdown\n",
    "    markdown, _, _ = truncate_tokens_if_needed(tokenizer, message_template, markdown)\n",
    "    return markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the whole process in action with Upstage's Solar and Document Parse!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=(\n",
    "        \"You are a academic paper analyzer. \"\n",
    "        \"- Basiclly, you don't have knowledge of the requested paper.\"\n",
    "        \"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
    "        \"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
    "        \"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
    "    ),\n",
    "    tools=[to_paper_search_agent, to_download_and_parse_paper_agent]#, to_paper_analysis_agent]#, to_triage, to_end_agent],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='9521a151-6c08-463b-a377-2e2c0df1563d', function=Function(arguments='{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', name='to_paper_search_agent'), type='function')]\n",
      "Supervisor Agent: to_paper_search_agent({'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'})\n",
      "[ChatCompletionMessageToolCall(id='f85864f5-f429-46dd-a900-cc97f7773ed8', function=Function(arguments='{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', name='to_download_and_parse_paper_agent'), type='function')]\n",
      "Supervisor Agent: to_download_and_parse_paper_agent({'paper_url': 'https://arxiv.org/pdf/2502.00299'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (53672 > 4096). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base token numbers: 115\n",
      "Paper token numbers: 53672\n",
      "Total token numbers: 53787\n",
      "Truncated paper tokens to: 31885\n",
      "Total token numbers: 32000\n",
      "--------------------------------\n",
      "The assistant summarizes the key findings from the experiments and analysis conducted on the ChunkKV method, which is a novel KV cache compression technique for efficient long-context inference in large language models (LLMs). The method retains the most informative semantic chunks from the original KV cache, leading to improved performance compared to existing methods. The experiments were conducted on various LLMs and benchmarks, demonstrating the effectiveness of ChunkKV in preserving essential contextual information for complex reasoning tasks, long-context understanding, and safety evaluations. The method’s chunk-based approach maintains crucial contextual information, leading to superior performance in challenging scenarios and benchmarks. The proposed layer-wise index reuse technique provides significant computational efficiency gains with minimal performance impact, achieving up to a 20.7% reduction in latency and\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "supervisor_agent.model = \"solar-pro\"\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.upstage.ai/v1\",\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Provide a comprehensive summary of the paper, \"\n",
    "                   \"'ChunkKV - Semantic-Preserving KV Cache Compression \"\n",
    "                   \"for Efficient Long-Context LLM Inference' on arXiv. \"\n",
    "    },\n",
    "]\n",
    "\n",
    "run(client, messages, supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentic-system",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
