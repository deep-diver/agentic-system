{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNT511P72L850aM3568Oxlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/agentic-system/blob/main/notebooks/document_parse_w_lang_graph.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vbA6tJoU5Hu"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install -qU langchain-core langchain-upstage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "Ayr6Ncp445Zc",
        "outputId": "f7883f24-25bf-49bf-ee62-62453b1f963c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"UPSTAGE_API_KEY\"] = \"up_ANcdA1bxF651vbxwBuDFReIen32J2\"\n",
        "UPSTAGE_API_KEY = os.environ[\"UPSTAGE_API_KEY\"]\n",
        "SERPER_API_KEY = \"aabe90976f29be746021f818451820c3a9652ec1\""
      ],
      "metadata": {
        "id": "Y0F7q1TcW1M3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "embedding_context_length = 4000\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.upstage.ai/v1\",\n",
        "    api_key=UPSTAGE_API_KEY\n",
        ")\n",
        "\n",
        "class UpstageEmbeddingFunction(EmbeddingFunction[Documents]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        client,\n",
        "        model_name: str = \"embedding-query\",\n",
        "    ):\n",
        "        self.client = client\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if not all(isinstance(item, str) for item in input):\n",
        "            raise ValueError(\"Solar embedding only supports text documents, not images\")\n",
        "\n",
        "        batch_process_result = self.client.embeddings.create(model=self.model_name, input=input).data\n",
        "        passage_embedding_list = [i.embedding for i in batch_process_result]\n",
        "        return np.array(passage_embedding_list, dtype=np.float32)\n",
        "\n",
        "embedding_fn = UpstageEmbeddingFunction(client)"
      ],
      "metadata": {
        "id": "wB1KdL1qxbhv"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf 2502.00299"
      ],
      "metadata": {
        "id": "655THXfo4fzk"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PyPDF2 import PdfReader, PdfWriter\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def to_paper_search_agent(paper_title: str):\n",
        "    \"\"\"Use this tool to search for paper's arXiv URL on the internet\"\"\"\n",
        "    url = \"https://google.serper.dev/search\"\n",
        "\n",
        "    payload = json.dumps({\"q\": f\"{paper_title} on arXiv\"})\n",
        "    headers = {\n",
        "        'X-API-KEY': SERPER_API_KEY,\n",
        "        'Content-Type': 'application/json'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    search_results = response.json()['organic']\n",
        "\n",
        "    if len(search_results) == 0:\n",
        "        return \"Count not find the URL to download the paper\"\n",
        "\n",
        "    first_result = search_results[0]\n",
        "    if not first_result['link'].startswith(\"https://arxiv.org\"):\n",
        "        return \"Could not find the URL to download the paper\"\n",
        "\n",
        "    return f\"URL to download '{paper_title}': {first_result['link'].replace('abs', 'pdf')}\"\n",
        "\n",
        "def split_pdf_by_pages(input_pdf_path, root_path, pages_per_pdf=10):\n",
        "    # Open the PDF\n",
        "    pdf = PdfReader(input_pdf_path)\n",
        "    total_pages = len(pdf.pages)\n",
        "\n",
        "    # Calculate number of output PDFs needed\n",
        "    num_pdfs = (total_pages + pages_per_pdf - 1) // pages_per_pdf\n",
        "\n",
        "    output_paths = []\n",
        "\n",
        "    # Split into multiple PDFs\n",
        "    for i in range(num_pdfs):\n",
        "        writer = PdfWriter()\n",
        "\n",
        "        # Calculate start and end pages for this split\n",
        "        start_page = i * pages_per_pdf\n",
        "        end_page = min((i + 1) * pages_per_pdf, total_pages)\n",
        "\n",
        "        # Add pages to writer\n",
        "        for page_num in range(start_page, end_page):\n",
        "            writer.add_page(pdf.pages[page_num])\n",
        "\n",
        "        # Save the split PDF\n",
        "        output_path = f\"{root_path}/{i+1}.pdf\"\n",
        "        with open(output_path, \"wb\") as output_file:\n",
        "            writer.write(output_file)\n",
        "        output_paths.append(output_path)\n",
        "\n",
        "    return output_paths\n",
        "\n",
        "def get_document_parse_response(filename, api_key):\n",
        "    url = \"https://api.upstage.ai/v1/document-ai/document-parse\"\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "    files = {\"document\": open(filename, \"rb\")}\n",
        "    data = {\"output_formats\": \"['markdown']\"}\n",
        "\n",
        "    response = requests.post(url, headers=headers, files=files, data=data)\n",
        "    upstage_response = json.loads(response.text)\n",
        "    return upstage_response\n",
        "\n",
        "def get_md_with_document_parse(root_path, paper_url, paper_id):\n",
        "    print(1)\n",
        "    response = requests.get(paper_url)\n",
        "    # Save the PDF to a temporary file\n",
        "\n",
        "    print(2)\n",
        "    pdf_path = f\"{root_path}/paper.pdf\"\n",
        "    with open(pdf_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(3)\n",
        "    split_factor = 1\n",
        "    split_pdfs = split_pdf_by_pages(pdf_path, root_path, split_factor) # by 10\n",
        "\n",
        "    print(4)\n",
        "    markdown = \"\"\n",
        "    total_responses = []\n",
        "    for i, split_pdf in enumerate(split_pdfs):\n",
        "        upstage_response = get_document_parse_response(split_pdf, UPSTAGE_API_KEY)\n",
        "\n",
        "        # Append the response to the total_responses list\n",
        "        total_responses.append({f\"page_{i+1 * split_factor}\": upstage_response})\n",
        "        # Also write the response to a JSON file for persistence\n",
        "        json_output_path = f\"{root_path}/response_{i+1}.json\"\n",
        "        with open(json_output_path, \"w\") as json_file:\n",
        "            json.dump(upstage_response, json_file, indent=2)\n",
        "\n",
        "        try:\n",
        "            markdown += upstage_response['content']['markdown']\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    print(5)\n",
        "    collection = chroma_client.create_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "\n",
        "    processed_input = []\n",
        "    if len(markdown) > embedding_context_length:\n",
        "        chunks = [markdown[i:i+embedding_context_length] for i in range(0, len(markdown), embedding_context_length)]\n",
        "        processed_input.extend(chunks)\n",
        "    else:\n",
        "        processed_input.append(markdown)\n",
        "\n",
        "    ids = []\n",
        "    for i in range(len(processed_input)):\n",
        "        ids.append(f\"{paper_id}_{i}\")\n",
        "\n",
        "    collection.add(documents=processed_input, ids=ids)\n",
        "    return collection\n",
        "\n",
        "@tool\n",
        "def to_download_and_parse_paper_agent(paper_url: str):\n",
        "    \"\"\"Use this tool to download and parse paper. Use this tool when paper URL is already found.\"\"\"\n",
        "    paper_id = paper_url.split(\"/\")[-1]\n",
        "    root_path = paper_id\n",
        "\n",
        "    if os.path.exists(root_path):\n",
        "        print(f\"Found cached markdown for {paper_id}\")\n",
        "        return f\"we already have the paper content stored in our database in the id of {paper_id}\"\n",
        "        # chunks = get_md_from_fs(paper_id)\n",
        "    else:\n",
        "        print(f\"No cached markdown found for {paper_id}, parsing from URL\")\n",
        "        os.makedirs(root_path, exist_ok=True)\n",
        "        _ = get_md_with_document_parse(root_path, paper_url, paper_id)\n",
        "        return f\"we have parsed the paper content and stored in our database in the id of {paper_id}\"\n",
        "\n",
        "@tool\n",
        "def to_retrive_paper_content_to_answer_question_agent(question: str, paper_id: str):\n",
        "    \"\"\"Use this tool to answer question about the paper.\"\"\"\n",
        "    collection = chroma_client.get_collection(name=paper_id, embedding_function=embedding_fn)\n",
        "    results = collection.query(query_texts=[question], n_results=10)\n",
        "    results_str = [\"Below is the retrieved content of the Paper.\\n-----------------------------------\\n\"]\n",
        "    for i in range(len(results['documents'])):\n",
        "        results_str.append(f\"{i}: {results['documents'][i]}\")\n",
        "    results_str.append(\"Based on the retrieved content, answer the user query.\")\n",
        "    return \"\\n\".join(results_str)"
      ],
      "metadata": {
        "id": "fzHCPTkcpTV6"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_upstage import ChatUpstage\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import ToolMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    found: bool\n",
        "\n",
        "tools = [\n",
        "    to_paper_search_agent,\n",
        "    to_download_and_parse_paper_agent,\n",
        "    to_retrive_paper_content_to_answer_question_agent\n",
        "]\n",
        "\n",
        "llm = ChatUpstage()\n",
        "llm = llm.bind_tools(tools)\n",
        "\n",
        "system_prompt = \"You are a academic paper analyzer. \"\n",
        "\"- Basiclly, you don't have knowledge of the requested paper.\"\n",
        "\"- Hence, you need to use the provided tools to get the paper information from the internet. \"\n",
        "\"- Your job is to find appropriate tool to transfer to based on the user's request and results of tool calls. \"\n",
        "\"- If enough information is collected to complete the user request, you should say directly answer to the user request. \"\n",
        "\n",
        "# Define the node that will call the LLM\n",
        "def chatbot(state: State):\n",
        "    messages = state['messages']\n",
        "\n",
        "    # --- Specify the System Prompt Here ---\n",
        "    system_prompt = \"You are a helpful assistant that translates English to French.\"\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_prompt), # Static system message\n",
        "        MessagesPlaceholder(variable_name=\"messages\"), # Placeholder for history/user input\n",
        "    ])\n",
        "\n",
        "    # Chain the prompt and model\n",
        "    chain = prompt | llm\n",
        "\n",
        "    # Invoke the chain with the current messages from the state\n",
        "    # Note: We pass the messages directly to the placeholder\n",
        "    response = chain.invoke({\"messages\": messages})\n",
        "    print(response)\n",
        "\n",
        "    # Return the AI's response to be added to the state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "class BasicToolNode:\n",
        "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1]\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls:\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append(\n",
        "                ToolMessage(\n",
        "                    content=tool_result,\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "    \"\"\"\n",
        "    Use in the conditional_edge to route to the ToolNode if the last message\n",
        "    has tool calls. Otherwise, route to the end.\n",
        "    \"\"\"\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    return END"
      ],
      "metadata": {
        "id": "DHr7nluEW5g7"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = BasicToolNode(tools=tools)\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "nAGnRL6JptGc"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "2R3gFLSjp1fD",
        "outputId": "16481338-6ded-4de3-f386-3202f473447a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n",
        "        for value in event.values():\n",
        "            if value[\"messages\"][-1].content:\n",
        "                print(\"Assistant:\", value[\"messages\"][-1].content)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        break"
      ],
      "metadata": {
        "id": "r3LBl1Pbo84R",
        "outputId": "4b615fb7-fc39-4920-ec15-02329a338f3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv. \n",
            "content='' additional_kwargs={'tool_calls': [{'id': '9822f649-8023-4790-93bb-d4c960443ffc', 'function': {'arguments': '{\"paper_title\":\"ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\"}', 'name': 'to_paper_search_agent'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 476, 'total_tokens': 622, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-250123', 'system_fingerprint': None, 'id': 'de158e6d-65a1-4157-9804-64544e4db7e4', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-4ee06396-35f4-49d8-be30-f0e371751bae-0' tool_calls=[{'name': 'to_paper_search_agent', 'args': {'paper_title': 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference'}, 'id': '9822f649-8023-4790-93bb-d4c960443ffc', 'type': 'tool_call'}] usage_metadata={'input_tokens': 476, 'output_tokens': 146, 'total_tokens': 622, 'input_token_details': {}, 'output_token_details': {}}\n",
            "Assistant: URL to download 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference': https://arxiv.org/pdf/2502.00299\n",
            "content='' additional_kwargs={'tool_calls': [{'id': '3936a203-7ee7-4f51-99fb-de32ccd5f7e2', 'function': {'arguments': '{\"paper_url\":\"https://arxiv.org/pdf/2502.00299\"}', 'name': 'to_download_and_parse_paper_agent'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 620, 'total_tokens': 777, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-250123', 'system_fingerprint': None, 'id': '1c2683fd-e315-43b5-ad48-2860b125ee80', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-236b1197-e3dd-4022-bdfd-f43df3657a36-0' tool_calls=[{'name': 'to_download_and_parse_paper_agent', 'args': {'paper_url': 'https://arxiv.org/pdf/2502.00299'}, 'id': '3936a203-7ee7-4f51-99fb-de32ccd5f7e2', 'type': 'tool_call'}] usage_metadata={'input_tokens': 620, 'output_tokens': 157, 'total_tokens': 777, 'input_token_details': {}, 'output_token_details': {}}\n",
            "Found cached markdown for 2502.00299\n",
            "Assistant: we already have the paper content stored in our database in the id of 2502.00299\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'a01336f1-09a3-44da-b559-463431d6de06', 'function': {'arguments': '{\"paper_id\":\"2502.00299\",\"question\":\"Provide a comprehensive summary of the paper, \\'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\' on arXiv.\"}', 'name': 'to_retrive_paper_content_to_answer_question_agent'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 738, 'total_tokens': 855, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-250123', 'system_fingerprint': None, 'id': '7cf9f124-7535-4823-8168-0d9d1d5127e7', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-0940e795-710a-4a78-bab1-9cbfc8a45af0-0' tool_calls=[{'name': 'to_retrive_paper_content_to_answer_question_agent', 'args': {'paper_id': '2502.00299', 'question': \"Provide a comprehensive summary of the paper, 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' on arXiv.\"}, 'id': 'a01336f1-09a3-44da-b559-463431d6de06', 'type': 'tool_call'}] usage_metadata={'input_tokens': 738, 'output_tokens': 117, 'total_tokens': 855, 'input_token_details': {}, 'output_token_details': {}}\n",
            "Assistant: Below is the retrieved content of the Paper.\n",
            "-----------------------------------\n",
            "\n",
            "0: ['\\nsame as in LongBench in Section 4.2. The chunk size is\\nset from the range {1, 3, 5, 10, 20, 30}. Figure 5 shows the\\nperformance of the ChunkKV with different chunk size on\\nthe LongBench and NIAH benchmarks. The three colorful\\ncurves represent three LLMs with different chunk sizes, and\\nthe colorful dashed line is the corresponding FullKV perfor-\\nmance. For more experiments on the size of the chunks with\\ndifferent compression ratios, refer to the Appendix B.4.\\n\\nTable 9: LongBench Performance with Different Chunk\\nSizes and Compression Ratios for LLaMA-3-8B-Instruct\\n\\n| Compression | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Rate | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 10% | 37.32 | 40.49 | 40.47 | 40.51 | 40.21 | 40.05 | 39.57 |\\n| 20% | 38.80 | 40.66 | 40.57 | 40.74 | 40.53 | 40.46 | 40.04 |\\n| 30% | 39.23 | 41.02 | 41.29 | 41.59 | 41.38 | 41.33 | 41.02 |\\n\\n\\nFrom Figure 5, we can observe that the LongBench per-\\nformance of ChunkKV is not significantly affected by the\\nchunk size, with performance variations less than 1%. The\\nthree curves are closely aligned, indicating that chunk sizes\\nin the range of {10, 20} exhibit better performance.\\n\\nTable 9 and 10 show the performance of ChunkKV with\\ndifferent comperession ratios and different chunk sizes on\\nthe LongBench and NIAH. We conducted extensive exper-\\niments across different compression ratios and KV cache\\nsizes to shows the effectiveness of ChunkKV and the chunk\\nsize is robust.\\n\\nTable 10: NIAH Performance with Different Chunk Sizes\\nand KV Cache Sizes for LLaMA-3-8B-Instruct\\n\\n| KV Cache | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| Size | 1 | 3 | 5 | 10 | 15 | 20 | 30 |\\n| 96 | 41.0 | 63.2 | 65.2 | 70.3 | 67.2 | 65.3 | 53.1 |\\n| 128 | 47.9 | 65.6 | 69.1 | 73.8 | 72.3 | 72.0 | 71.2 |\\n| 256 | 61.7 | 70.3 | 71.2 | 74.1 | 73.2 | 72.3 | 71.1 |\\n| 512 | 68.6 | 72.6 | 72.5 | 74.5 | 74.3 | 74.0 | 72.6 |\\n\\n\\nFrom the chunk size ablation study, we can observe that\\nacross different tasks (LongBench and NIAH) and vari-\\nous compression settings, a chunk size of 10 consistently\\ndelivers optimal or near-optimal performance. This em-\\npirical finding suggests that a chunk size of 10 strikes a\\ngood balance between preserving semantic information and\\ncompression efficiency, making it a robust default choice\\nfor ChunkKV. Therefore, we adopt this chunk size setting\\nthroughout our experiments.\\n\\n# 6. Conclusion\\n\\nWe introduced ChunkKV, a novel KV cache compression\\nmethod that preserves semantic information by retaining\\nmore informative chunks. Through extensive experiments\\nacross multiple state-of-the-art LLMs (including DeepSeek-\\nR1, LLaMA-3, Qwen2, and Mistral) and diverse bench-\\nmarks (GSM8K, LongBench, NIAH, and JailbreakV), we\\ndemonstrate that ChunkKV consistently outperforms ex-\\nisting methods while using only a fraction of the memory.\\nOur comprehensive analysis shows that ChunkKV’s chunk-\\nbased approach maintains crucial contextual information,\\nleading to superior performance in complex reasoning tasks,\\nlong-context understanding, and safety evaluations. The\\nmethod’s effectiveness is particularly evident in challenging\\nscenarios like many-shot GSM8K and multi-document QA\\ntasks, where semantic coherence is crucial. Furthermore,\\nour proposed layer-wise index reuse technique provides\\nsignificant computational efficiency gains with minimal per-\\nformance impact, achieving up to 20.7% latency reduction\\nand 26.5% throughput improvement. These findings, sup-\\nported by detailed quantitative analysis and ablation stud-\\nies, establish ChunkKV as a significant advancement in KV\\ncache compression technology, offering an effective solution\\nfor deploying LLMs in resource-constrained environments\\nwhile maintaining high-quality outputs.\\n\\n8ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n', 'hami, A. and Jaggi, M. Landmark attention:\\nRandom-access infinite context length for transformers.\\nArXiv preprint, abs/2305.16300, 2023. URL https:\\n//arxiv.org/abs/2305.16300.\\n\\nOpenAI. Gpt-4o-mini: Advancing cost-efficient intelli-\\ngence, 2023. Accessed: 2023-12-14.\\n\\nPan, R., Liu, X., Diao, S., Pi, R., Zhang, J., Han, C.,\\nand Zhang, T. Lisa: Layerwise importance sampling\\nfor memory-efficient large language model fine-tuning.\\nArXiv preprint, abs/2403.17919, 2024a. URL https:\\n//arxiv.org/abs/2403.17919.\\n\\nPan, R., Xing, S., Diao, S., Sun, W., Liu, X., Shum, K.,\\nZhang, J., Pi, R., and Zhang, T. Plum: Prompt learning us-\\ning metaheuristics. In Ku, L.-W., Martins, A., and Sriku-\\nmar, V. (eds.), Findings of the Association for Computa-\\ntional Linguistics ACL 2024, pp. 2177–2197, Bangkok,\\nThailand and virtual meeting, August 2024b. Association\\nfor Computational Linguistics. doi: 10.18653/v1/2024.\\nfindings-acl.129. URL https://aclanthology.\\norg/2024.findings-acl.129.\\n\\nPires, B. Á. and Szepesvári, C. Multiclass classification\\ncalibration functions. arXiv preprint arXiv:1609.06385,\\n2016.\\n\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lill-\\nicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Fi-\\nrat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking\\nmultimodal understanding across millions of tokens of\\ncontext. ArXiv preprint, abs/2403.05530, 2024. URL\\nhttps://arxiv.org/abs/2403.05530.\\n\\nShaham, U., Ivgi, M., Efrat, A., Berant, J., and\\nLevy, O. ZeroSCROLLS: A zero-shot benchmark for\\nlong text understanding. In Bouamor, H., Pino, J.,\\nand Bali, K. (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023, pp. 7977–\\n7989, Singapore, 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-emnlp.\\n536. URL https://aclanthology.org/2023.\\nfindings-emnlp.536.\\n\\nShi, W., Min, S., Lomeli, M., Zhou, C., Li, M., Lin, X. V.,\\nSmith, N. A., Zettlemoyer, L., Yih, W.-t., and Lewis,\\nM. In-context pretraining: Language modeling beyond\\ndocument boundaries. In The Twelfth International Con-\\nference on Learning Representations.\\n\\nSmith, B. and Troynikov, A. Evaluating chunking\\nstrategies for retrieval. Technical report, Chroma,\\n2024. URL https://research.trychroma.\\ncom/evaluating-chunking.\\n\\nSteinwart, I. How to compare different loss functions and\\ntheir risks. Constructive Approximation, 26:225–287,\\n2007. URL https://api.semanticscholar.\\norg/CorpusID:16660598.\\n\\nSun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma,\\nS., Zhang, Q., Wang, J., and Wei, F. You only cache\\nonce: Decoder-decoder architectures for language models.\\narXiv preprint arXiv:2405.05254, 2024.\\n\\nTang, J., Zhao, Y., Zhu, K., Xiao, G., Kasikci, B., and Han,\\nS. Quest: Query-aware sparsity for efficient long-context\\nllm inference. ArXiv preprint, abs/2406.10774, 2024.\\nURL https://arxiv.org/abs/2406.10774.\\n\\nTay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D.,\\nPham, P., Rao, J., Yang, L., Ruder, S., and Metzler,\\nD. Long range arena : A benchmark for efficient trans-\\nformers. In 9th International Conference on Learn-\\ning Representations, ICLR 2021, Virtual Event, Austria,\\nMay 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=qVyeW-grC2k.\\n\\nTay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Bahri, D.,\\nSchuster, T., Zheng, H. S., Houlsby, N., and Metzler, D.\\nUnifying language learning paradigms. ArXiv preprint,\\nabs/2205.05131, 2022. URL https://arxiv.org/\\nabs/2205.05131.\\n\\n12ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nTjong Kim Sang, E. F. and Veenstra, J. Representing text\\nchunks. In Thompson, H. S. and Lascarides, A. (eds.),\\nNinth Conference of the European Chapter of the As-\\nsociation for Computational Linguistics, pp. 173–179,\\nBergen, Norway, 1999. Association for Computational\\nLinguistics. URL https://aclanthology.org/\\nE99-1023.\\n\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., et al. Llama 2: Open foundatio', ') and JailbreakV (Luo et al.,\\n2024). And also different models including DeepSeek-\\nR1-Distill-Llama-8B (Guo et al., 2025),LLaMA-3-8B-\\nInstruct (Meta, 2024), Mistral-7B-Instruct (Jiang et al.,\\n2023a), and Qwen2-7B-Instruct (Yang et al., 2024a). Our\\nexperimental results demonstrate that ChunkKV surpasses\\nexisting KV cache compression methods in both efficiency\\n\\nand accuracy, primarily due to its ability to preserve essen-\\ntial information through selective chunk retention. These\\nfindings establish ChunkKV as a simple yet effective ap-\\nproach to KV cache compression.\\n\\nWe summarize our key contributions as follows:\\n\\n- • We identify the phenomenon in which discrete KV cache\\n- compression methods inadvertently prune the necessary\\n- semantic information.\\n\\n\\n- • We propose ChunkKV, a simple KV cache compression\\n- method that uses the fragmentation method that keeps the\\n- semantic information, and propose the layer-wise index\\n- reuse technique to reduce the additional computational\\n- time.\\n\\n\\n- • We evaluate ChunkKV on cutting-edge long-context\\n- benchmarks including LongBench and Needle-In-A-\\n- HayStack, as well as the GSM8K, many-shot GSM8K and\\n- JailbreakV in-context learning benchmark, and multi-step\\n\\n\\n2ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nreasoning (O1 and R1) LLMs, achieving state-of-the-art\\nperformance.\\n\\n# 2. Related Work\\n\\nKV Cache Compression. KV cache compression technol-\\nogy has developed rapidly in the era of LLM, with meth-\\nods mainly focused on evicting unimportant tokens. The\\ncompression process occurs before the attention blocks, op-\\ntimizing both the prefilling time and GPU memory. Xiao\\net al. (2024) and Han et al. (2024) propose that initial and re-\\ncent tokens consistently have high attention scores between\\ndifferent layers and attention heads. As a result, retaining\\nthese tokens in the KV cache is more likely to preserve im-\\nportant information. Furthermore, FastGen (Ge et al., 2023)\\nevicts tokens based on observed patterns. H2O (Zhang et al.,\\n2023) and SnapKV (Li et al., 2024) employ dynamic KV\\ncache compression methods, evaluating the importance of\\ntokens based on attention scores and then evicting the less\\nimportant ones. As inference scenarios become increas-\\ningly complex, dynamic KV cache compression methods\\ndemonstrate powerful performance. Recently, Yang et al.\\n(2024b) and Cai et al. (2024) have closely examined the dis-\\ntributions of attention scores during the pre-filling stage of\\nthe Retrieval-Augmented Generation (RAG) task, discover-\\ning a pyramidal KV cache compression pattern in different\\ntransformer layers.\\n\\nAlthough these KV cache compression methods have ex-\\nplored efficient GPU memory management while maintain-\\ning original performance, our study focuses more on the\\nsemantic information of the prompt. We find that chunks\\nof the original KV cache are more important than discrete\\ntokens.\\n\\nChunking Method. The chunking methodology is widely\\nused in the field of NLP due to its simplicity and effective-\\nness (Tjong Kim Sang & Veenstra, 1999). In the era of\\nLLMs, chunking is primarily applied in data pre-processing.\\nFor example, Shi et al. suggest grouping related train-\\ning data into chunks to achieve better convergence curves\\nto pre-train LLMs. Fei et al. (2024) apply a topic-based\\nchunking method to improve the semantic compression of\\nprompts. Furthermore, chunking plays an important role in\\nthe Retrieval-Augmented Generation (RAG) field (Yepes\\net al., 2024; Smith & Troynikov, 2024; Anthropic, 2024). It\\nserves to divide documents into units of information with\\nsemantic content suitable for embedding-based retrieval and\\nprocessing by LLMs.\\n\\nLayer-Wise Technique The layer-wise technique is widely\\nused in the training and inference of large language models\\n(LLMs). LISA (Pan et al., 2024a) is a layer-wise sampling\\nmethod based on observations of the training dynamics of\\nLow-Rank Adaptation (LoRA)(Hu et al., 2022) across lay-\\n\\ners. LAMB(You et al., 2020)', 'in-\\nformation usually appear in a continuous sequence (Fang &\\nXie, 2022). Thus, we introduce a straightforward yet effec-\\ntive ChunkKV, grouping the tokens in a chunk as a basic\\ncompressing unit, which should be preserved or discarded\\nas a whole. Thus, it retains the most informative semantic\\nchunks from the original KV cache. As shown in Figure 1,\\npreserving a chunk helps to catch the subject, predicate,\\nand object. Furthermore, we investigate that the preserved\\n\\n1ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nQuestion: purple-crested turaco eats what food?\\n\\n| Discrete KV methods: 𝑆\" = 𝑓(𝑡\") | ChunkKV: 𝑆! = \\t ∑ #\" $% 𝑓 𝑡\" , \\twhere\\t𝑐\\t = \\t {𝑡%, … , 𝑡#} |\\n| --- | --- |\\n| Discrete KV methods with a low sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… Bird …… …… eat …… …… …… turacos, …… …… …… …… pulp …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… water …… …… …… …… …… …… …… leaves …… …… …… …… …… …… …… …… nuts …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a low sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… similar turacos, the purple-crested turaco have faster minimum transit times when consuming smaller seed diets than larger seed diets, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… |\\n| Discrete KV methods with a high sparsity …… …… purple-crested turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… Purple-crested turacos …… …… …… …… …… …… eat …… …… …… turacos, …… …… …… …… …… …… …… …… …… …… Turaco …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… coexist with various other animals, including those that might enjoy strawberries …… …… …… …… …… …… …… …… …… …… …… ……. | ChunkKV with a high sparsity …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… After fruit consumption, they …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… porphyreolophus primarily consumes fruits …… …… …… …… ……, …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… …… ………… …… …… …… |\\n\\n\\nFigure 1: Illustration of the impact of the token discrete method and the chunk method on semantic preservation. The\\ndiscrete method preserves words related to the question but often omits the subject. In contrast, the chunk method retains the\\nsubject of the words, maintaining more accurate semantic information. For the equation: S is the score function, and c is a\\nchunk of tokens.\\n\\nTable 1: Comparison of Methods on KV Cache Compression.\\n\\n| Method | KV Cache Compression | Dynamic Policy | Layer-Wise Policy | Semantic Information | Efficient Index Reuse |\\n| --- | --- | --- | --- | --- | --- |\\n| StreamingLLM (Xiao et al., 2024) | ✓ |  |  |  |  |\\n| H2O (Zhang et al., 2023) | ✓ | ✓ |  |  |  |\\n| SnapKV (Li et al., 2024) | ✓ | ✓ |  |  |  |\\n| PyramidInfer (Yang et al., 2024b) | ✓ | ✓ | ✓ |  |  |\\n| PyramidKV (Cai et al., 2024) | ✓ | ✓ | ✓ |  |  |\\n| ChunkKV(Ours) | ✓ | ✓ | ✓ | ✓ | ✓ |\\n\\n\\nKV cache indices by ChunkKV exhibit a higher similarity\\ncompared to previous methods. Consequently, we develop a\\ntechnique called layer-wise index reuse, which reduces the\\nadditional computational time introduced by the KV cache\\ncompression method. As outlined in Table 1, recent highly\\nrelevant KV cache compression methods lack the ability to\\nretain semantic information and efficiently reuse indices.\\n\\nTo evaluate ChunkKV’s performance, we conduct com-\\nprehensive experiments across multiple cutting-edge long-\\ncontext benchmarks: long-context tasks including Long-\\nBench (Bai et al., 2024) and Needle-In-A-HayStack\\n(NIAH) (Kamradt, 2023), in-context learning tasks such as\\nGSM8K (Cobbe et al., 2021', ', Chen, Y., Hu, S., Xu, Z., Chen, J., Hao, M. K.,\\nHan, X., Thai, Z. L., Wang, S., Liu, Z., et al. ∞-bench:\\nExtending long context evaluation beyond 100k tokens.\\nArXiv preprint, abs/2402.13718, 2024. URL https:\\n//arxiv.org/abs/2402.13718.\\n\\n13ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nZhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,\\nR., Song, Z., Tian, Y., Ré, C., Barrett, C., et al. H2o:\\nHeavy-hitter oracle for efficient generative inference of\\nlarge language models. Advances in Neural Information\\nProcessing Systems, 36:34661–34710, 2023.\\n\\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\\nllm-as-a-judge with mt-bench and chatbot arena. Ad-\\nvances in Neural Information Processing Systems, 36:\\n46595–46623, 2023.\\n\\nZhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R.,\\nAwadallah, A. H., Celikyilmaz, A., Liu, Y., Qiu, X., and\\nRadev, D. QMSum: A new benchmark for query-based\\nmulti-domain meeting summarization. In Toutanova, K.,\\nRumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Belt-\\nagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and\\nZhou, Y. (eds.), Proceedings of the 2021 Conference of\\nthe North American Chapter of the Association for Com-\\nputational Linguistics: Human Language Technologies,\\npp. 5905–5921, Online, 2021. Association for Compu-\\ntational Linguistics. doi: 10.18653/v1/2021.naacl-main.\\n472. URL https://aclanthology.org/2021.\\nnaacl-main.472.\\n\\nZhou, W., Jiang, Y. E., Cui, P., Wang, T., Xiao, Z., Hou, Y.,\\nCotterell, R., and Sachan, M. Recurrentgpt: Interactive\\ngeneration of (arbitrarily) long text, 2023.\\n\\nZhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., and Han, B.\\nCan language models perform robust reasoning in chain-\\nof-thought prompting with noisy rationales? In The\\nThirty-eighth Annual Conference on Neural Information\\nProcessing Systems, 2024.\\n\\n14ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nAppendix\\nA In-depth Analysis of ChunkKV vs. Discrete Token Methods 16\\nA.1 Quantitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\nA.2 Hypothetical Scenario . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.3 Comparative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\nA.4 Implications for Model Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB Additional Experiments 18\\nB.1 Layer-Wise Index Reuse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\nB.2 LongBench . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nB.3 Needle-In-A-Haystack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nB.4 Chunk Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nB.5 Multi-Lingual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nC Theoretical Understanding 30\\nD Additional Related Work 33\\nE Statistics of Models 33\\nF Statistics of Datasets 34\\nG Prompt 34\\nH Limitations 35\\nI Licenses 35\\n\\n15ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n# A. In-depth Analysis of ChunkKV vs. Discrete Token Methods\\n\\nA.1. Quantitative Analysis\\n\\nTo rigorously evaluate the effectiveness of ChunkKV compared to discrete token-based methods, we conducted systematic\\nexperiments using a LLaMA-3-8B-Instruct model. We randomly selected 100 sequences from the each sub-category of\\nLongBench dataset and analyzed two key metrics across different model layers: KV cache L1 loss and attention cosine\\nsimilarity. For each sequence, we: 1. Computed the full KV cache and attention patterns without compression as ground\\ntruth. 2. Applied ChunkKV,', \" is a layer-wise adaptive learn-\\ning rate method that speeds up LLM training by stabilizing\\ntraining convergence with large batch sizes. DoLa (Chuang\\net al., 2023) employs layer-wise contrasting to reduce hallu-\\ncinations during LLM inference.\\n\\n# 3. ChunkKV\\n\\n3.1. Preliminary Study of KV Cache Compression\\n\\nWith the increasing long-context capabilities of LLMs, the\\nKV cache has become crucial for improving inference effi-\\nciency. However, it can consume significant GPU memory\\nwhen handling long input contexts. The GPU memory cost\\nof the KV cache for the decoding stage can be calculated as\\nfollows:\\n\\n$$M_{K V}=2\\\\times B\\\\times S\\\\times L\\\\times N\\\\times D\\\\times2\\\\qquad\\\\qquad(1)$$\\n\\nwhere B is the batch size, S is the sequence length of prompt\\nand decoded length, L is the number of layers, N is the num-\\nber of attention heads, D is the dimension of each attention\\nhead, and the first 2 accounts for the KV matrices, while\\nthe last 2 accounts for the precision when using float16.\\nTable E shows the configuration parameters for LLaMA-\\n3-8B-Instruct (Meta, 2024). With a batch size B = 1 and\\na sequence length of prompt S = 2048, the GPU mem-\\nory cost of the KV cache is nearly 1 GB. If the batch size\\nexceeds 24, the GPU memory cost of the KV cache will\\nexceed the capacity of an RTX 4090 GPU. To address this\\nissue, KV cache compression methods have been proposed,\\nwith the aim of retaining only a minimal amount of KV\\ncache while preserving as much information as possible.\\nFor more details on the LLM configuration parameters, re-\\nfer to Appendix E.\\n\\n3.2. Chunk Based KV Compression\\n\\nTo address the limitations of existing KV cache compression\\nmethods, we propose ChunkKV, a novel KV cache com-\\npression method that retains the most informative semantic\\nchunks. The key idea behind ChunkKV is to group tokens\\nin the KV cache into chunks that preserve more semantic\\ninformation, such as a chunk containing a subject, verb and\\nobject. As illustrated in Figure 1, ChunkKV preserves the\\nchunks of the KV cache that contain more semantic infor-\\nmation. First, we define a chunk as a group of tokens that\\ncontain related semantic information. By retaining the most\\ninformative chunks from the original KV cache, ChunkKV\\ncan effectively reduce the memory usage of the KV cache\\nwhile preserving essential information.\\n\\nThe Algorithm 1 shows the pseudocode outline of ChunkKV.\\nFirst, following H2O (Zhang et al., 2023) and SnapKV (Li\\net al., 2024), we set the observe window by computing the\\nobservation scores A ← QTq−w:Tq KT , where QTq−w:Tq\\n\\n3ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n| Algorithm 1 ChunkKV |\\n| --- |\\n| Input: Q E RTqxd K E RTkxd v E RTvxd observe , , window size w, chunk size c, compressed KV cache max length Lmax |\\n| Output: Compressed KV cache K', V' |\\n| Observe Window Calculation: |\\n| A ← QTq-w:Tq KT {Attention scores for the observe window} |\\n| C ← [ ] I {Calculate the number of chunks} |\\n| Chunk Attention Score Calculation: |\\n| for 2 = 1 to C do |\\n| Ai ← �j=(i-1)c+1 A:⌀ {Sum of observation scores for each chunk} |\\n| end for |\\n| Top-K Chunk Selection: |\\n| k ← Lmax c |\\n| Top K_ Indices ← indices of Top-k chunks based on Ai Compression: |\\n| K', V' ← index_select(K, V, Top_K_Indices) |\\n| Concatenation: |\\n| K' ← concat(Ko:Lmax -w' KTk-w:Tk) |\\n| v Tv-w:Tv ) V' ← concat(Vo:Lmax |\\n| -w' K', V' |\\n\\n\\nis the observe window, K is the Key matrix and the win-\\ndow size w is usually set to {4,8, 16, 32}. Next, the num-\\nber of chunks C is calculated as C = [ Te 1, where Tk is\\nthe length of the Key matrix and c is the chunk size. The\\nobservation scores for each chunk are then computed as\\nAi = �j=(i-1)c+1 A:⌀ for 2 = 1, 2, · · · , C. Referring to\\nprevious works (Zhang et al., 2023; Li et al., 2024; Yang\\net al., 2024b; Cai et al., 2024), we still use the top-k algo-\\nrithm as ChunkKV's sampling policy. For the top-k chunk\\nselection, the top-k chunks are selected based on their obser-\\nvation scores, where k = Lmax I, and Lmax is\", '# ChunkKV: Semantic-Preserving KV Cache Compression for\\nEfficient Long-Context LLM Inference\\n\\nXiang Liu 1 Zhenheng Tang 2 Peijie Dong 1 Zeyu Li 1 Bo Li 2 Xuming Hu 1 Xiaowen Chu 1\\n\\n# Abstract\\n\\nTo reduce memory costs in long-context inference\\nwith Large Language Models (LLMs), many re-\\ncent works focus on compressing the key-value\\n(KV) cache of different tokens. However, we\\nidentify that the previous KV cache compression\\nmethods measure token importance individually,\\nneglecting the dependency between different to-\\nkens in the real-world language characterics. In\\nlight of this, we introduce ChunkKV, grouping the\\ntokens in a chunk as a basic compressing unit, and\\nretaining the most informative semantic chunks\\nwhile discarding the less important ones. Further-\\nmore, observing that ChunkKV exhibits higher\\nsimilarity in the preserved indices across differ-\\nent layers, we propose layer-wise index reuse\\nto further reduce computational overhead. We\\nevaluated ChunkKV on cutting-edge long-context\\nbenchmarks including LongBench and Needle-\\nIn-A-HayStack, as well as the GSM8K and Jail-\\nbreakV in-context learning benchmark. Our ex-\\nperiments with instruction tuning and multi-step\\nreasoning (O1 and R1) LLMs, achieve up to 10%\\nperformance improvement under aggressive com-\\npression ratios compared to existing methods.\\n\\n# 1. Introduction\\n\\n2025\\nFeb\\n1\\n[cs.CL]\\narXiv:2502.00299v1\\n\\nLarge Language Models (LLMs) have become essential for\\naddressing various downstream tasks of natural language\\nprocessing (NLP), including summarization and question\\nanswering, which require the interpretation of a long con-\\ntext from sources such as books, reports, and documents,\\noften encompassing tens of thousands of tokens (Brown\\net al., 2020; Tay et al., 2022; Touvron et al., 2023). Re-\\ncent advances in long-context technology within the field of\\n\\n1The Hong Kong University of Science and Technol-\\nogy(Guangzhou), Guangzhou, China 2The Hong Kong University\\nof Science and Technology, Hong Kong, China. Correspondence\\nto: Xuming Hu <xuminghu@hkust-gz.edu.cn>, Xiaowen Chu\\n<xwchu@hkust-gz.edu.cn>.\\n\\nmachine learning (ML) systems (Dao, 2024; Jacobs et al.,\\n2023; Xiao et al., 2024) have significantly enhanced com-\\nputational throughputs and reduced latency of LLMs to\\nprocess increasingly large input context lengths (Liu et al.,\\n2024b; Young et al., 2024) with saving historical KV cache\\n(key value attentions). However, the memory requirement\\nof the KV cache in serving super-long contexts becomes a\\nnew bottlneck (Zhang et al., 2023; Reid et al., 2024). For\\ninstance, the KV cache for a single token in a 7B-parameter\\nmodel requires approximately 0.5 MB of GPU memory, re-\\nsulting in a 10,000-token prompt consuming around 5 GB\\nof GPU memory.\\n\\nTo address the substantial GPU memory consumption\\ncaused by KV caching, recent studies consider compressing\\nthe KV cache by pruning non-important discrete parts from\\nthe prompt tokens (Zhang et al., 2023; Li et al., 2024; Ge\\net al., 2023; Cai et al., 2024; Fu et al., 2024a; Yang et al.,\\n2024b; Liu et al., 2024e; Tang et al., 2024). H2O (Zhang\\net al., 2023) and SnapKV (Li et al., 2024) have shown that\\nretaining less than 50% of the discrete KV cache can signif-\\nicantly reduce GPU memory usage with minimal impact on\\nperformance. However, we identify that the previous KV\\ncache compression methods (Zhang et al., 2023; Cai et al.,\\n2024) measure token importance isolatedly, neglecting the\\ndependency between different tokens in the real-world lan-\\nguage characterics. For example, as shown in Figure 1,\\nfocusing on token-level importance might excessively fo-\\ncus on words about subjects “turaco” in the question while\\nomitting crucial information about the objects (foods) in the\\ndocuments, resulting the loss of essential semantic informa-\\ntion. This motivates us to rethink the following question:\\n\\nHow to avoid isolated token importance measurement and\\npreserve the semantic information in KV cache?\\n\\nIn light of this, we observe that the complete semantic ', 'essing, pp. 13358–13376, Singapore, December\\n2023b. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.emnlp-main.825. URL https://\\naclanthology.org/2023.emnlp-main.825.\\n\\nJiang, H., Wu, Q., , Luo, X., Li, D., Lin, C.-Y., Yang, Y., and\\nQiu, L. LongLLMLingua: Accelerating and enhancing\\nLLMs in long context scenarios via prompt compression.\\nIn Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Pro-\\nceedings of the 62nd Annual Meeting of the Association\\nfor Computational Linguistics (Volume 1: Long Papers),\\n\\npp. 1658–1677, Bangkok, Thailand, August 2024. As-\\nsociation for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.91.\\n\\nJoshi, M., Choi, E., Weld, D., and Zettlemoyer, L. Trivi-\\naQA: A large scale distantly supervised challenge dataset\\nfor reading comprehension. In Barzilay, R. and Kan,\\nM.-Y. (eds.), Proceedings of the 55th Annual Meet-\\ning of the Association for Computational Linguistics\\n(Volume 1: Long Papers), pp. 1601–1611, Vancouver,\\nCanada, 2017. Association for Computational Linguis-\\ntics. doi: 10.18653/v1/P17-1147. URL https://\\naclanthology.org/P17-1147.\\n\\nKamradt, G. Needle In A Haystack - pres-\\nsure testing LLMs. Github, 2023. URL\\nhttps://github.com/gkamradt/LLMTest_\\nNeedleInAHaystack/tree/main.\\n\\nKleijn and der Vaart, V. The bernstein-von-mises the-\\norem under misspecification. Electronic Journal of\\nStatistics, 6:354–381, 2012. URL https://api.\\nsemanticscholar.org/CorpusID:85548207.\\n\\nKoˇciský, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\\nK. M., Melis, G., and Grefenstette, E. The NarrativeQA\\nreading comprehension challenge. Transactions of the\\nAssociation for Computational Linguistics, 6:317–328,\\n2018. doi: 10.1162/tacl_a_00023. URL https://\\naclanthology.org/Q18-1023.\\n\\nLi, D., Shao, R., et al. How long can open-source LLMs\\ntruly promise on context length?, 2023. URL https:\\n//lmsys.org/blog/2023-06-29-longchat.\\n\\nLi, X. and Roth, D. Learning question classifiers. In\\nCOLING 2002: The 19th International Conference on\\nComputational Linguistics, 2002. URL https://\\naclanthology.org/C02-1150.\\n\\nLi, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli,\\nA., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv:\\nLlm knows what you are looking for before genera-\\ntion. ArXiv preprint, abs/2404.14469, 2024. URL\\nhttps://arxiv.org/abs/2404.14469.\\n\\nLiu, A., Liu, J., Pan, Z., He, Y., Haffari, G., and Zhuang, B.\\nMinicache: Kv cache compression in depth dimension for\\nlarge language models. arXiv preprint arXiv:2405.14366,\\n2024a.\\n\\nLiu, H., Yan, W., Zaharia, M., and Abbeel, P. World model\\non million-length video and language with ringattention.\\nArXiv preprint, abs/2402.08268, 2024b. URL https:\\n//arxiv.org/abs/2402.08268.\\n\\n11ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\\nM., Petroni, F., and Liang, P. Lost in the middle: How\\nlanguage models use long contexts. Transactions of the\\nAssociation for Computational Linguistics, 12:157–173,\\n2024c. doi: 10.1162/tacl_a_00638. URL https://\\naclanthology.org/2024.tacl-1.9.\\n\\nLiu, T., Xu, C., and McAuley, J. Repobench: Benchmarking\\nrepository-level code auto-completion systems. In The\\nTwelfth International Conference on Learning Represen-\\ntations, 2024d. URL https://openreview.net/\\nforum?id=pPjZIOuQuF.\\n\\nLiu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyril-\\nlidis, A., and Shrivastava, A. Scissorhands: Exploiting\\nthe persistence of importance hypothesis for llm kv cache\\ncompression at test time. Advances in Neural Information\\nProcessing Systems, 36, 2024e.\\n\\nLuo, W., Ma, S., Liu, X., Guo, X., and Xiao, C. Jailbreakv:\\nA benchmark for assessing the robustness of multimodal\\nlarge language models against jailbreak attacks. In First\\nConference on Language Modeling, 2024. URL https:\\n//openreview.net/forum?id=GC4mXVfquq.\\n\\nMeta. Introducing meta llama 3: The most capable openly\\navailable llm to date. https://ai.meta.com/\\nblog/meta-llama-3/, 2024. Accessed: 2024-06-\\n07.\\n\\nMohtas', 'king, the continuously chunk-\\nlevel KV cache preserves the whole examples (semantic\\ninformation) in ICL, thus reducing the requirement on dis-\\ntinguishability, i.e lower bound of KL divergence between\\nthe example and the question (Equation 4 in Condition 2).\\nThe complete analysis is provided in Appendix C.\\n\\n# 4. Experiment Results\\n\\nIn this section, we conduct experiments to evaluate the ef-\\nfectiveness of ChunkKV on KV cache compression in two\\nbenchmark fields, with a chunk size set to 10 even for vari-\\nous model architectures. The first is the In-Context Learn-\\ning benchmark, for which we select GSM8K (Cobbe et al.,\\n2021) and Jailbreakv (Luo et al., 2024) to evaluate the perfor-\\nmance of ChunkKV, furthermore we also include multi-step\\nreasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al.,\\n2025) to evaluate the performance of ChunkKV. The In-\\nContext Learning scenario is a crucial capability for LLMs\\nand has been adapted in many powerful technologies such as\\nChain-of-Thought (Wei et al., 2022; Diao et al., 2024; Pan\\net al., 2024b). The second is the Long-Context benchmark,\\nwhich includes LongBench (Bai et al., 2024) and Needle-In-\\nA-HayStack (NIAH) (Kamradt, 2023), both widely used for\\nassessing KV cache compression methods. All experiments\\nwere conducted three times, using the mean score to ensure\\nrobustness.\\n\\n4.1. In-Context Learning\\n\\nThe In-Context Learning (ICL) ability significantly en-\\nhances the impact of prompts on large language models\\n(LLMs). For example, the Chain-of-Thought approach (Wei\\n\\net al., 2022) increases the accuracy of the GSM8K of the\\nPaLM model (Chowdhery et al., 2022) from 18% to 57%\\nwithout additional training. In this section, we evaluate\\nthe performance of ChunkKV on the GSM8K, Many-Shot\\nGSM8K (Agarwal et al., 2024), and JailbreakV (Luo et al.,\\n2024) benchmarks.\\n\\nTable 3: GSM8K Performance Comparison.\\n\\n| Ratio | StreamingLLM | H2O | SnapKV | PyramidKV | ChunkKV (Ours) |\\n| --- | --- | --- | --- | --- | --- |\\n| DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ | DeepSeek-R1-Distill-Llama-8B FullKV: 69.4% ↑ |\\n| 10% | 51.6% | 55.6% | 57.6% | 62.6% | 65.7% |\\n| LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ | LlaMa-3.1-8B-Instruct FullKV: 79.5% ↑ |\\n| 30% | 70.5% | 72.2% | 76.1% | 77.1% | 77.3% |\\n| 20% | 63.8% | 64.0% | 68.8% | 71.4% | 77.6% |\\n| 10% | 47.8% | 45.0% | 50.3% | 48.2% | 65.7% |\\n| LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ | LlaMa-3-8B-Instruct FullKV: 76.8% ↑ |\\n| 30% | 70.6% | 73.6% | 70.2% | 68.2% | 74.6% |\\n| Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ | Qwen2-7B-Instruct FullKV: 71.1% ↑ |\\n| 30% | 70.8% | 61.2% | 70.8% | 64.7% | 73.5% |\\n\\n\\nGSM8K In the in-context learning scenario, we\\nevaluated multiple KV cache compression methods\\nfor GSM8K (Cobbe et al., 2021), which contains\\nmore than 1,000 arithmetic questions on LLaMA-3-8B-\\nInstruct, LLaMA-3.1-8B-Instruct (Meta, 2024), Qwen2-\\n7B-Instruct (Yang et al., 2024a) and DeepSeek-R1-Distill-\\nLlama-8B (Guo et al., 2025). Follow the Agarwal et al.\\n(2024), we consider many-shot GSM8K as a long-context\\nreasoning scenario, which is a more challenging task than\\nLongBench (Bai et al., 2024). The CoT prompt settings\\nfor this experiment are the same as those used by Wei et al.\\n(2022), for many-shot GSM8K we set the number of shots\\nto 50, which the prompt length is more than 4k tokens.\\n\\n5ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-', 'suring Efficiency. We evaluated the latency and\\nthroughput of ChunkKV compared to FullKV using\\nLLaMA3-8B-Instruct on an A40 GPU. All experiments\\nwere conducted with reuse layer is 2, batch size set to 1\\nand inference was performed using Flash Attention 2, each\\nexperiment was repeated 10 times and the average latency\\nand throughput were reported.\\n\\nTable 8: Latency and throughput comparison between\\nChunkKV and FullKV under different input-output configu-\\nrations. Percentages in parentheses indicate improvements\\nover FullKV baseline.\\n\\n| Method | Sequence Length | Sequence Length | Performance Metrics | Performance Metrics |\\n| --- | --- | --- | --- | --- |\\n| Method | Input | Output | Latency(s) ↓ | Throughput(T/S) ↑ |\\n| FullKV | 4096 | 1024 | 43.60 | 105.92 |\\n| ChunkKV | 4096 | 1024 | 37.52 (13.9%) | 118.85 (12.2%) |\\n| ChunkKV_reuse | 4096 | 1024 | 37.35 (14.3%) | 124.09 (17.2%) |\\n| FullKV | 4096 | 4096 | 175.50 | 37.73 |\\n| ChunkKV | 4096 | 4096 | 164.55 (6.2%) | 40.58 (7.6%) |\\n| ChunkKV_reuse | 4096 | 4096 | 162.85 (7.2%) | 41.12 (9.0%) |\\n| FullKV | 8192 | 1024 | 46.48 | 184.08 |\\n| ChunkKV | 8192 | 1024 | 37.83 (18.6%) | 228.96 (24.4%) |\\n| ChunkKV_reuse | 8192 | 1024 | 36.85 (20.7%) | 232.99 (26.5%) |\\n| FullKV | 8192 | 4096 | 183.42 | 55.93 |\\n| ChunkKV | 8192 | 4096 | 164.78 (10.2%) | 65.14 (16.5%) |\\n| ChunkKV_reuse | 8192 | 4096 | 162.15 (11.6%) | 66.05 (18.1%) |\\n\\n\\nThe results in Table 8 shows that the layer-wise index reuse\\nstrategy (ChunkKV_reuse) further boosts performance,\\nachieving up to a 20.7% reduction in latency, and through-\\nput improvements are particularly notable for longer input\\nsequences, with ChunkKV_reuse delivering up to a 26.5%\\nimprovement over FullKV.\\n\\nIndex Reuse Performance on LongBench\\n\\n![image](/image/placeholder)\\n- Chart Title: Index Reuse Performance on LongBench\\n- X-Axis: Number of Index Reuse Layers\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | LLaMA-3-8B-Inst | LLaMA-7B-Inst | MIstral-7B-Inst | Qwen2-7B-Inst |\\n| --- | --- | --- | --- | --- |\\n| item_01 | 45Not explicitly visible | 45Not explicitly visible | 40Not explicitly visible | 35Not explicitly visible |\\n\\n\\nFigure 4: Comparison with different index reuse layers on\\nLongBench.\\n\\nMeasuring Task Performance. This experiment evaluates\\nthe performance of the layer-wise index reuse approach by\\nmeasuring the performance of the LongBench (Bai et al.,\\n2024), the experiment settings are the same as LongBench\\nin 4.2. And the number of index reuse layers is set from 1\\nto the number of layers in the model, where an index reuse\\nlayer of 1 corresponds to the normal ChunkKV without\\nindex reuse, and our method set reuse layer to 2.\\n\\nFigure 4 illustrates the performance of ChunkKV with vary-\\ning index reuse layers on the LongBench benchmark. Gen-\\nerally, reuse layer set to 2 can achieve the minimal perfor-\\nmance degradation across all models. For more experiments\\n\\n7ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\\n\\n![image](/image/placeholder)\\n- Chart Title: LongBench Performance vs Chunk Size\\n- X-Axis: Chunk Size\\n- Y-Axis: LongBench Score\\n- Chart Type: line\\n|  | ILaMA-3-BB | Mistral-7B | Owen2-7B | LLaMA-3-BB Full kV | Mistral-7B Full kV | Qwen2-7B Full KV |\\n| --- | --- | --- | --- | --- | --- | --- |\\n| item_01 | 38 | 46 | 46 | 46 | 46 | 40 |\\n\\n\\nFigure 5: LongBench Performance Comparison with differ-\\nent chunk size under 10% compression rate.\\n\\non index reuse, please refer to the APPENDIX B.1.3.\\n\\nOverall, these findings on efficiency and performance sug-\\ngest that layer-wise index reuse can be an effective technique\\nfor optimizing the efficiency-performance trade-off in KV\\ncache compression, with the potential for model-specific\\ntuning to maximize benefits.\\n\\n# 5. Ablation study\\n\\n# 5.1. Chunk Size\\n\\nThis section aims to investigate the impact of chunk size on\\nthe performance of ChunkKV. Different chunk sizes will\\nlead to varying degrees of compression on the semantic\\ninformation of the data. We set the experiemnt setting the']\n",
            "Based on the retrieved content, answer the user query.\n",
            "content=\"The paper 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' introduces a novel KV cache compression method called ChunkKV. This method aims to preserve semantic information by retaining more informative chunks. The authors conducted extensive experiments across multiple state-of-the-art LLMs and diverse benchmarks, demonstrating that ChunkKV consistently outperforms existing methods while using only a fraction of the memory. The method's effectiveness is particularly evident in challenging scenarios like many-shot GSM8K and multi-document QA tasks, where semantic coherence is crucial. Furthermore, the proposed layer-wise index reuse technique provides significant computational efficiency gains with minimal performance impact, achieving up to 20.7% latency reduction and 26.5% throughput improvement. These findings establish ChunkKV as a significant advancement in KV cache compression technology, offering an effective solution for deploying LLMs in resource-constrained environments while maintaining high-quality outputs.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 16359, 'total_tokens': 16586, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'solar-mini-250123', 'system_fingerprint': None, 'id': 'ac4f5f88-dddd-4441-a97e-cb9f9305f872', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-20833758-68c8-4258-89ba-816acb7f3d16-0' usage_metadata={'input_tokens': 16359, 'output_tokens': 227, 'total_tokens': 16586, 'input_token_details': {}, 'output_token_details': {}}\n",
            "Assistant: The paper 'ChunkKV - Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference' introduces a novel KV cache compression method called ChunkKV. This method aims to preserve semantic information by retaining more informative chunks. The authors conducted extensive experiments across multiple state-of-the-art LLMs and diverse benchmarks, demonstrating that ChunkKV consistently outperforms existing methods while using only a fraction of the memory. The method's effectiveness is particularly evident in challenging scenarios like many-shot GSM8K and multi-document QA tasks, where semantic coherence is crucial. Furthermore, the proposed layer-wise index reuse technique provides significant computational efficiency gains with minimal performance impact, achieving up to 20.7% latency reduction and 26.5% throughput improvement. These findings establish ChunkKV as a significant advancement in KV cache compression technology, offering an effective solution for deploying LLMs in resource-constrained environments while maintaining high-quality outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TNbNvxL4_eID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}