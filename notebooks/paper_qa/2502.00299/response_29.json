{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference\n\n# B.4. Chunk Size\n\nTable 15 shows the performance of ChunkKV with different chunk size on the LongBench benchmark.\n\nTable 15: LongBench Performance Comparison with different chunk sizes\n\n| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\n| --- | --- | --- | --- | --- | --- | --- |\n| Model | 3 | 5 | 10 | 20 | 30 | Full KV |\n| LLaMA-3-8B-Instruct | 40.49 | 40.47 | 40.51 | 40.05 | 39.57 | 41.46 |\n| Mistral-7B-Instruct | 46.45 | 46.51 | 46.71 | 46.42 | 45.98 | 48.08 |\n| Qwen2-7B-Instruct | 40.38 | 40.33 | 40.66 | 40.88 | 40.73 | 40.71 |\n\n\nTable 16 shows the performance of ChunkKV with different chunk size on the GSM8K benchmark. Figure 19 shows that\nthe ChunkKV with different chunk sizes on GSM8K displays the same curve pattern as LongBench. The CoT prompt length\nfor GSM8K is only 1K tokens, so the optimal chunk size range is smaller.\n\n![image](/image/placeholder)\n- Chart Title: GSM8K Performance vs Chunk Size\n- X-Axis: Chunk Size\n- Y-Axis: GSM8K Score\n- Chart Type: line\n|  | 3 | 5 | 10 | 20 |\n| --- | --- | --- | --- | --- |\n| item_01 | 75.0 | 72.5 | 72.5 | 72.0 |\n\n\nFigure 19: GSM8K Performance Comparison with different chunk size\n\nTable 16: GSM8K Performance Comparison with different chunk sizes\n\n| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\n| --- | --- | --- | --- | --- | --- |\n| Model | 3 | 5 | 10 | 20 | Full KV |\n| LLaMA-3-8B-Instruct | 74.6 | 74.5 | 73.9 | 63.2 | 76.8 |\n| Qwen2-7B-Instruct | 73.5 | 71.2 | 71.8 | 71.7 | 71.1 |\n\n\n# B.5. Multi-Lingual\n\nTable 17 is the Chinese support model Qwen2-7B-Instruct evaluated on the LongBench Chinese subtask, where ChunkKV\nachieves better performance than other compression methods and the full KV cache performance. Both the English and\nChinese results indicate that ChunkKV is a promising approach for maintaining crucial information in the KV cache.\n\n29",
    "text": ""
  },
  "elements": [
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1731,
          "y": 0.0574
        },
        {
          "x": 0.8026,
          "y": 0.0574
        },
        {
          "x": 0.8026,
          "y": 0.0721
        },
        {
          "x": 0.1731,
          "y": 0.0721
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# B.4. Chunk Size",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0875,
          "y": 0.0857
        },
        {
          "x": 0.2044,
          "y": 0.0857
        },
        {
          "x": 0.2044,
          "y": 0.1002
        },
        {
          "x": 0.0875,
          "y": 0.1002
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 15 shows the performance of ChunkKV with different chunk size on the LongBench benchmark.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0867,
          "y": 0.1093
        },
        {
          "x": 0.7629,
          "y": 0.1093
        },
        {
          "x": 0.7629,
          "y": 0.1252
        },
        {
          "x": 0.0867,
          "y": 0.1252
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 15: LongBench Performance Comparison with different chunk sizes",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2407,
          "y": 0.1477
        },
        {
          "x": 0.734,
          "y": 0.1477
        },
        {
          "x": 0.734,
          "y": 0.1641
        },
        {
          "x": 0.2407,
          "y": 0.1641
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\n| --- | --- | --- | --- | --- | --- | --- |\n| Model | 3 | 5 | 10 | 20 | 30 | Full KV |\n| LLaMA-3-8B-Instruct | 40.49 | 40.47 | 40.51 | 40.05 | 39.57 | 41.46 |\n| Mistral-7B-Instruct | 46.45 | 46.51 | 46.71 | 46.42 | 45.98 | 48.08 |\n| Qwen2-7B-Instruct | 40.38 | 40.33 | 40.66 | 40.88 | 40.73 | 40.71 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2264,
          "y": 0.1741
        },
        {
          "x": 0.7507,
          "y": 0.1741
        },
        {
          "x": 0.7507,
          "y": 0.2732
        },
        {
          "x": 0.2264,
          "y": 0.2732
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 16 shows the performance of ChunkKV with different chunk size on the GSM8K benchmark. Figure 19 shows that\nthe ChunkKV with different chunk sizes on GSM8K displays the same curve pattern as LongBench. The CoT prompt length\nfor GSM8K is only 1K tokens, so the optimal chunk size range is smaller.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.086,
          "y": 0.2982
        },
        {
          "x": 0.8905,
          "y": 0.2982
        },
        {
          "x": 0.8905,
          "y": 0.3454
        },
        {
          "x": 0.086,
          "y": 0.3454
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "chart",
      "content": {
        "html": "",
        "markdown": "![image](/image/placeholder)\n- Chart Title: GSM8K Performance vs Chunk Size\n- X-Axis: Chunk Size\n- Y-Axis: GSM8K Score\n- Chart Type: line\n|  | 3 | 5 | 10 | 20 |\n| --- | --- | --- | --- | --- |\n| item_01 | 75.0 | 72.5 | 72.5 | 72.0 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0904,
          "y": 0.3655
        },
        {
          "x": 0.8894,
          "y": 0.3655
        },
        {
          "x": 0.8894,
          "y": 0.626
        },
        {
          "x": 0.0904,
          "y": 0.626
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "Figure 19: GSM8K Performance Comparison with different chunk size",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2511,
          "y": 0.635
        },
        {
          "x": 0.7246,
          "y": 0.635
        },
        {
          "x": 0.7246,
          "y": 0.651
        },
        {
          "x": 0.2511,
          "y": 0.651
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "caption",
      "content": {
        "html": "",
        "markdown": "Table 16: GSM8K Performance Comparison with different chunk sizes",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2501,
          "y": 0.6925
        },
        {
          "x": 0.7231,
          "y": 0.6925
        },
        {
          "x": 0.7231,
          "y": 0.7091
        },
        {
          "x": 0.2501,
          "y": 0.7091
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "table",
      "content": {
        "html": "",
        "markdown": "| Model | Chunk Size | Chunk Size | Chunk Size | Chunk Size | Full KV |\n| --- | --- | --- | --- | --- | --- |\n| Model | 3 | 5 | 10 | 20 | Full KV |\n| LLaMA-3-8B-Instruct | 74.6 | 74.5 | 73.9 | 63.2 | 76.8 |\n| Qwen2-7B-Instruct | 73.5 | 71.2 | 71.8 | 71.7 | 71.1 |\n",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.269,
          "y": 0.7198
        },
        {
          "x": 0.7083,
          "y": 0.7198
        },
        {
          "x": 0.7083,
          "y": 0.8032
        },
        {
          "x": 0.269,
          "y": 0.8032
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# B.5. Multi-Lingual",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.087,
          "y": 0.8377
        },
        {
          "x": 0.2239,
          "y": 0.8377
        },
        {
          "x": 0.2239,
          "y": 0.8524
        },
        {
          "x": 0.087,
          "y": 0.8524
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Table 17 is the Chinese support model Qwen2-7B-Instruct evaluated on the LongBench Chinese subtask, where ChunkKV\nachieves better performance than other compression methods and the full KV cache performance. Both the English and\nChinese results indicate that ChunkKV is a promising approach for maintaining crucial information in the KV cache.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0875,
          "y": 0.8615
        },
        {
          "x": 0.8905,
          "y": 0.8615
        },
        {
          "x": 0.8905,
          "y": 0.9077
        },
        {
          "x": 0.0875,
          "y": 0.9077
        }
      ],
      "id": 11,
      "page": 1
    },
    {
      "category": "footer",
      "content": {
        "html": "",
        "markdown": "29",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.4766,
          "y": 0.9238
        },
        {
          "x": 0.4985,
          "y": 0.9238
        },
        {
          "x": 0.4985,
          "y": 0.9372
        },
        {
          "x": 0.4766,
          "y": 0.9372
        }
      ],
      "id": 12,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250116",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}