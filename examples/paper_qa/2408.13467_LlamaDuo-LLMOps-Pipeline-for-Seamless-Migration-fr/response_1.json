{
  "api": "2.0",
  "content": {
    "html": "",
    "markdown": "# LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs\nto Small-Scale Local LLMs\n\nChansung Park*\u2662, Juyong Jiang*\u2661, Fan Wang*\u2661, Sayak Paul\u2660, Jing Tang\u2020\u2661\u2663\n\u2662Electronics and Telecommunications Research Institute\n\u2661The Hong Kong University of Science and Technology (Guangzhou)\n\u2663The Hong Kong University of Science and Technology\n\u2660Hugging Face\n{deep.diver.csp,csjuyongjiang,csfanwang,spsayakpaul}@gmail.com\njingtang@ust.hk\n\nAbstract\n\nThe widespread adoption of cloud-based pro-\nprietary large language models (LLMs) has in-\ntroduced significant challenges, including op-\nerational dependencies, privacy concerns, and\nthe necessity of continuous internet connectiv-\nity. In this work, we introduce an LLMOps\npipeline, \u201cLlamaDuo\u201d, for the seamless migra-\ntion of knowledge and abilities from service-\noriented LLMs to smaller, locally manageable\nmodels. This pipeline is crucial for ensuring\nservice continuity in the presence of opera-\ntional failures, strict privacy policies, or of-\nfline requirements. Our LlamaDuo involves\nfine-tuning a small language model against the\nservice LLM using a synthetic dataset gener-\nated by the latter. If the performance of the\nfine-tuned model falls short of expectations, it\nis automatically improved through additional\nfine-tuning using extra similar data generated\nby the service LLM. This multi-turn process\nguarantees that the smaller model can eventu-\nally match or even surpass the service LLM\u2019s\ncapabilities in specific downstream tasks, offer-\ning a practical and scalable solution for man-\naging AI deployments in constrained environ-\nments. Extensive experiments with leading-\nedge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of\nLlamaDuo across various downstream tasks.\nOur pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.\n\n# 1 Introduction\n\nThe emergence of LLMs has significantly trans-\nformed a myriad of tasks and domains (Chowdhery\net al., 2023; Gemini Team, 2023; Achiam et al.,\n2023; Touvron et al., 2023; Zhao et al., 2023; Jiang\net al., 2024a,b). In particular, cloud-based propri-\netary LLMs, referred to as service models, such as\nGPT-4 (Achiam et al., 2023), Gemini 1.5 (Gemini\n\n*Equal contributors: Chansung Park, Juyong Jiang, and\nFan Wang. Listing order is random.\n\nTeam, 2023), and Claude 3 (Anthropic, 2024), have\nexhibited exceptional capabilities when compared\nto their smaller, open-source counterparts (Chang\net al., 2024). A notable survey involving 70 AI in-\ndustry leaders from diverse enterprises reveals that\napproximately 80% of the enterprise market share\nis dominated by closed-source platforms, with a\nsignificant portion of this share attributed to Ope-\nnAI (Wang and Xu, 2024).\n\nHowever, the increasing reliance on cloud-based\nservice models presents significant challenges in\nterms of operational dependencies (Achiam et al.,\n2023), privacy concerns (Wu et al., 2024), and\naccessibility challenges (Ray, 2023). These chal-\nlenges manifest in various ways, including poten-\ntial service disruptions, heightened risks to data\nprivacy due to the transmission of sensitive infor-\nmation to external providers, mandatory internet\nconnectivity for utilization, and inconsistencies\nstemming from updates to service providers\u2019 LLMs\n(Hadi et al., 2023; Zhao et al., 2023). Additionally,\nthe transition from proof-of-concept (PoC) devel-\nopment utilizing service LLMs to deployment with\nlocal models frequently leads to diminished prompt\neffectiveness owing to differences between models,\nsubsequently resulting in a suboptimal experience\nfor end-users (Naveed et al., 2023; Lyu et al., 2024).\nTo address these concerns and ensure consistent ser-\nvice delivery, it is imperative to develop smaller,\nlocally manageable LLMs that can operate inde-\npendently of cloud-based infrastructures.\n\nRecent studies have demonstrated that the strate-\ngic fine-tuning of smaller and open-source LLMs\nwith high-quality synthetic data (Wang et al.,\n2023b; Xu et al., 2023) generated by service LLMs\ncan achieve performances that are on par with, or\neven surpass, those of proprietary LLMs in specific\ndownstream tasks (Chiang et al., 2023; Taori et al.,\n2023a; Luo et al., 2023; Abdin et al., 2024; Zhou\net al., 2024). Motivated by these findings, we in-\ntroduce an LLMOps pipeline namely LlamaDuo\n\n# \u2020Corresponding author: Jing Tang.\n\n2025\nMay\n30\n[cs.LG]\narXiv:2408.13467v3",
    "text": ""
  },
  "elements": [
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs\nto Small-Scale Local LLMs",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1136,
          "y": 0.081
        },
        {
          "x": 0.8835,
          "y": 0.081
        },
        {
          "x": 0.8835,
          "y": 0.1177
        },
        {
          "x": 0.1136,
          "y": 0.1177
        }
      ],
      "id": 0,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Chansung Park*\u2662, Juyong Jiang*\u2661, Fan Wang*\u2661, Sayak Paul\u2660, Jing Tang\u2020\u2661\u2663\n\u2662Electronics and Telecommunications Research Institute\n\u2661The Hong Kong University of Science and Technology (Guangzhou)\n\u2663The Hong Kong University of Science and Technology\n\u2660Hugging Face\n{deep.diver.csp,csjuyongjiang,csfanwang,spsayakpaul}@gmail.com\njingtang@ust.hk",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1683,
          "y": 0.1274
        },
        {
          "x": 0.8362,
          "y": 0.1274
        },
        {
          "x": 0.8362,
          "y": 0.2464
        },
        {
          "x": 0.1683,
          "y": 0.2464
        }
      ],
      "id": 1,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Abstract",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.2621,
          "y": 0.2617
        },
        {
          "x": 0.3427,
          "y": 0.2617
        },
        {
          "x": 0.3427,
          "y": 0.2777
        },
        {
          "x": 0.2621,
          "y": 0.2777
        }
      ],
      "id": 2,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The widespread adoption of cloud-based pro-\nprietary large language models (LLMs) has in-\ntroduced significant challenges, including op-\nerational dependencies, privacy concerns, and\nthe necessity of continuous internet connectiv-\nity. In this work, we introduce an LLMOps\npipeline, \u201cLlamaDuo\u201d, for the seamless migra-\ntion of knowledge and abilities from service-\noriented LLMs to smaller, locally manageable\nmodels. This pipeline is crucial for ensuring\nservice continuity in the presence of opera-\ntional failures, strict privacy policies, or of-\nfline requirements. Our LlamaDuo involves\nfine-tuning a small language model against the\nservice LLM using a synthetic dataset gener-\nated by the latter. If the performance of the\nfine-tuned model falls short of expectations, it\nis automatically improved through additional\nfine-tuning using extra similar data generated\nby the service LLM. This multi-turn process\nguarantees that the smaller model can eventu-\nally match or even surpass the service LLM\u2019s\ncapabilities in specific downstream tasks, offer-\ning a practical and scalable solution for man-\naging AI deployments in constrained environ-\nments. Extensive experiments with leading-\nedge LLMs are conducted to demonstrate the\neffectiveness, adaptability, and affordability of\nLlamaDuo across various downstream tasks.\nOur pipeline implementation is available at\nhttps://github.com/deep-diver/llamaduo.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1447,
          "y": 0.2864
        },
        {
          "x": 0.4628,
          "y": 0.2864
        },
        {
          "x": 0.4628,
          "y": 0.7308
        },
        {
          "x": 0.1447,
          "y": 0.7308
        }
      ],
      "id": 3,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# 1 Introduction",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1167,
          "y": 0.7401
        },
        {
          "x": 0.2609,
          "y": 0.7401
        },
        {
          "x": 0.2609,
          "y": 0.7558
        },
        {
          "x": 0.1167,
          "y": 0.7558
        }
      ],
      "id": 4,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "The emergence of LLMs has significantly trans-\nformed a myriad of tasks and domains (Chowdhery\net al., 2023; Gemini Team, 2023; Achiam et al.,\n2023; Touvron et al., 2023; Zhao et al., 2023; Jiang\net al., 2024a,b). In particular, cloud-based propri-\netary LLMs, referred to as service models, such as\nGPT-4 (Achiam et al., 2023), Gemini 1.5 (Gemini",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1159,
          "y": 0.7653
        },
        {
          "x": 0.4906,
          "y": 0.7653
        },
        {
          "x": 0.4906,
          "y": 0.8768
        },
        {
          "x": 0.1159,
          "y": 0.8768
        }
      ],
      "id": 5,
      "page": 1
    },
    {
      "category": "footnote",
      "content": {
        "html": "",
        "markdown": "*Equal contributors: Chansung Park, Juyong Jiang, and\nFan Wang. Listing order is random.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1168,
          "y": 0.8845
        },
        {
          "x": 0.4902,
          "y": 0.8845
        },
        {
          "x": 0.4902,
          "y": 0.9086
        },
        {
          "x": 0.1168,
          "y": 0.9086
        }
      ],
      "id": 6,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Team, 2023), and Claude 3 (Anthropic, 2024), have\nexhibited exceptional capabilities when compared\nto their smaller, open-source counterparts (Chang\net al., 2024). A notable survey involving 70 AI in-\ndustry leaders from diverse enterprises reveals that\napproximately 80% of the enterprise market share\nis dominated by closed-source platforms, with a\nsignificant portion of this share attributed to Ope-\nnAI (Wang and Xu, 2024).",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5106,
          "y": 0.263
        },
        {
          "x": 0.8845,
          "y": 0.263
        },
        {
          "x": 0.8845,
          "y": 0.4063
        },
        {
          "x": 0.5106,
          "y": 0.4063
        }
      ],
      "id": 7,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "However, the increasing reliance on cloud-based\nservice models presents significant challenges in\nterms of operational dependencies (Achiam et al.,\n2023), privacy concerns (Wu et al., 2024), and\naccessibility challenges (Ray, 2023). These chal-\nlenges manifest in various ways, including poten-\ntial service disruptions, heightened risks to data\nprivacy due to the transmission of sensitive infor-\nmation to external providers, mandatory internet\nconnectivity for utilization, and inconsistencies\nstemming from updates to service providers\u2019 LLMs\n(Hadi et al., 2023; Zhao et al., 2023). Additionally,\nthe transition from proof-of-concept (PoC) devel-\nopment utilizing service LLMs to deployment with\nlocal models frequently leads to diminished prompt\neffectiveness owing to differences between models,\nsubsequently resulting in a suboptimal experience\nfor end-users (Naveed et al., 2023; Lyu et al., 2024).\nTo address these concerns and ensure consistent ser-\nvice delivery, it is imperative to develop smaller,\nlocally manageable LLMs that can operate inde-\npendently of cloud-based infrastructures.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5112,
          "y": 0.4075
        },
        {
          "x": 0.8846,
          "y": 0.4075
        },
        {
          "x": 0.8846,
          "y": 0.7599
        },
        {
          "x": 0.5112,
          "y": 0.7599
        }
      ],
      "id": 8,
      "page": 1
    },
    {
      "category": "paragraph",
      "content": {
        "html": "",
        "markdown": "Recent studies have demonstrated that the strate-\ngic fine-tuning of smaller and open-source LLMs\nwith high-quality synthetic data (Wang et al.,\n2023b; Xu et al., 2023) generated by service LLMs\ncan achieve performances that are on par with, or\neven surpass, those of proprietary LLMs in specific\ndownstream tasks (Chiang et al., 2023; Taori et al.,\n2023a; Luo et al., 2023; Abdin et al., 2024; Zhou\net al., 2024). Motivated by these findings, we in-\ntroduce an LLMOps pipeline namely LlamaDuo",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.5112,
          "y": 0.7621
        },
        {
          "x": 0.8845,
          "y": 0.7621
        },
        {
          "x": 0.8845,
          "y": 0.9227
        },
        {
          "x": 0.5112,
          "y": 0.9227
        }
      ],
      "id": 9,
      "page": 1
    },
    {
      "category": "heading1",
      "content": {
        "html": "",
        "markdown": "# \u2020Corresponding author: Jing Tang.",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.1391,
          "y": 0.9091
        },
        {
          "x": 0.3489,
          "y": 0.9091
        },
        {
          "x": 0.3489,
          "y": 0.9222
        },
        {
          "x": 0.1391,
          "y": 0.9222
        }
      ],
      "id": 10,
      "page": 1
    },
    {
      "category": "header",
      "content": {
        "html": "",
        "markdown": "2025\nMay\n30\n[cs.LG]\narXiv:2408.13467v3",
        "text": ""
      },
      "coordinates": [
        {
          "x": 0.0251,
          "y": 0.291
        },
        {
          "x": 0.0636,
          "y": 0.291
        },
        {
          "x": 0.0636,
          "y": 0.7127
        },
        {
          "x": 0.0251,
          "y": 0.7127
        }
      ],
      "id": 11,
      "page": 1
    }
  ],
  "merged_elements": [],
  "model": "document-parse-250404",
  "ocr": false,
  "usage": {
    "pages": 1
  }
}